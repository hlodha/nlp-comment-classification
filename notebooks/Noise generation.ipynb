{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"data\") / \"jigsaw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = [pd.read_csv(DATA_ROOT / fname) for fname in [\"train_new_large_without_leaks.csv\", \"test_proced.csv\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train.toxic = train.target>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def tok(s): return [tok.text for tok in nlp.tokenizer(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MostIndicativeN(vectorizer, clf, N):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    topClass1 = coefs_with_fns[:N]\n",
    "    topClass2 = coefs_with_fns[:-(N + 1):-1]\n",
    "    print(\"Class 1 best: \")\n",
    "    for feat in topClass1:\n",
    "        print(feat)\n",
    "    print(\"Class 2 best: \")\n",
    "    for feat in topClass2:\n",
    "        print(feat)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/harsh/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 best: \n",
      "(-0.6055146956335433, 'cuntnlu')\n",
      "(-0.5819076848671683, 'redirect')\n",
      "(-0.5781602364093442, 'thanks')\n",
      "(-0.44569571694290727, 'talk')\n",
      "(-0.37282909639777817, 'thank')\n",
      "(-0.35716070658040605, 'please')\n",
      "(-0.3002855035075513, 'scrotumcan')\n",
      "(-0.2947513122719908, 'scratch')\n",
      "(-0.27648761225028434, 'at')\n",
      "(-0.26095453811477454, 'may')\n",
      "Class 2 best: \n",
      "(2.0121141421705597, 'fuck')\n",
      "(1.8311746320367872, 'stupid')\n",
      "(1.6570192184075108, 'fucking')\n",
      "(1.4442572758859067, 'shit')\n",
      "(1.2636729928692303, 'idiot')\n",
      "(1.0693873315781715, 'ass')\n",
      "(1.0541216769006476, 'suck')\n",
      "(0.9631088047460871, 'penis')\n",
      "(0.8659579934653944, 'bitch')\n",
      "(0.846472173946417, 'asshole')\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=tok, ngram_range=(1,1))\n",
    "clf = LogisticRegression()\n",
    "pipe = Pipeline([('vectorizer', vectorizer), ('clf', clf)])\n",
    "\n",
    "train1 = train['comment_text'].tolist()\n",
    "test1 = test['comment_text'].tolist()\n",
    "labelsTest1 = test.toxic.tolist()\n",
    "\n",
    "labelsTrain1 = train.toxic.tolist()\n",
    "\n",
    "pipe.fit(train1, labelsTrain1)\n",
    "\n",
    "MostIndicativeN(vectorizer, clf, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(word):\n",
    "    transformations = 'insert'\n",
    "    \n",
    "    name = transformations\n",
    "    \n",
    "    if \"swap\" == name:\n",
    "        return swap(word)\n",
    "    elif \"insert\" == name:\n",
    "        return insert(word)\n",
    "    elif \"remove\" == name:\n",
    "        return remove(word)\n",
    "    elif \"homoglyph\" == name:\n",
    "        return homoglyph(word)\n",
    "    elif \"repeat_char\" == name:\n",
    "        return repeat_char(word)\n",
    "       elif \"distractor\" == name:\n",
    "        return distractor(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def swap(word):\n",
    "    cword = word\n",
    "    if len(word)>=4:\n",
    "        s = np.random.randint(1,len(word)-2)\n",
    "        cword = word[:s] + word[s+1] + word[s] + word[s+2:]\n",
    "    return (cword)\n",
    "\n",
    "def remove(word):\n",
    "    s = np.random.randint(0,len(word))\n",
    "    if len(word)>2:\n",
    "        cword = word[:s] + word[s+1:]\n",
    "    else:\n",
    "        cword = word\n",
    "    return cword\n",
    "\n",
    "def insert(word):\n",
    "    cword = word\n",
    "    s = np.random.randint(0,len(word)+1)\n",
    "    cword = word[:s] + chr(97+np.random.randint(0,26)) + word[s:]\n",
    "    \n",
    "    return (cword)\n",
    "\n",
    "\n",
    "def homoglyph(word):\n",
    "    s = np.random.randint(0,len(word))\n",
    "    homos = {'-':'Àó','9':'‡ß≠','8':'»¢','7':'ùüï','6':'–±','5':'∆º','4':'·èé','3':'∆∑','2':'·íø','1':'l','0':'O',\"'\":'`','a': '…ë', 'b': '–¨', 'c': 'œ≤', 'd': '‘Å', 'e': '–µ', 'f': 'ùöè', 'g': '…°', 'h': '’∞', 'i': '—ñ', 'j': 'œ≥', 'k': 'ùíå', 'l': '‚Öº', 'm': 'ÔΩç', 'n': '’∏', 'o':'–æ', 'p': '—Ä', 'q': '‘õ', 'r': '‚≤Ö', 's': '—ï', 't': 'ùöù', 'u': '’Ω', 'v': '—µ', 'w': '‘ù', 'x': '√ó', 'y': '—É', 'z': '·¥¢'}\n",
    "\n",
    "    if word[s] in homos: \n",
    "        rletter = homos[word[s]]\n",
    "    else:\n",
    "        rletter = word[s]\n",
    "    cword = word[:s] + rletter + word[s+1:]\n",
    "\n",
    "    return (cword)\n",
    "\n",
    "\n",
    "def repeat_char(word):\n",
    "    s = np.random.randint(0,len(word))\n",
    "    rletter = word[s]\n",
    "    cword = word[:s] + rletter + word[s:]\n",
    "\n",
    "    return (cword)\n",
    "\n",
    "def distractor(word):\n",
    "    s = np.random.randint(0,len(topClass1))\n",
    "    distractor_word = topClass1[s][1]\n",
    "    cword = word + ' ' + distractor_word\n",
    "\n",
    "    return (cword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to change only words indicative of toxicity\n",
    "\n",
    "def transform_sentence(text):\n",
    "    \n",
    "    text_2  =text\n",
    "    \n",
    "    #if (random.uniform(0,1) < prob):\n",
    "    for word in text.split():\n",
    "        if word.lower() in Top10000:\n",
    "            text_2 = text.replace(word, transform(word))\n",
    "    return text_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "def transform_sentence_2(text):\n",
    "    \n",
    "    text_2  =text\n",
    "    s = np.random.randint(0,len(text.split()))\n",
    "    word = text.split()[s]\n",
    "    prob = 1\n",
    "    if (random.uniform(0,1) < prob):\n",
    "        text_2 = text.replace(word, transform(word))\n",
    "    return text_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=10000\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "topClass1 = coefs_with_fns[:N]\n",
    "topClass2 = coefs_with_fns[:-(N + 1):-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top10000 = list(np.array(topClass2)[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train =pd.read_csv(DATA_ROOT/'train.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Old method of generating noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_insert = [transform_sentence(sent) for sent in test1]  #change transform(word) code accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_remove = [transform_sentence(sent) for sent in test1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_swap = [transform_sentence(sent) for sent in test1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_homoglyph = [transform_sentence(sent) for sent in test1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_repeat = [transform_sentence(sent) for sent in test1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = test\n",
    "test_2['comment_text'] = pd.Series(test2_repeat)\n",
    "test_2.to_csv(DATA_ROOT / 'new_large/test_noised_repeat.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top10000_insert = [insert(word) for word in Top10000]\n",
    "Top10000_remove = [remove(word) for word in Top10000]\n",
    "Top10000_swap = [swap(word) for word in Top10000]\n",
    "Top10000_homoglyph = [homoglyph(word) for word in Top10000]\n",
    "Top10000_repeat = [repeat_char(word) for word in Top10000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = [('original', Top10000),\n",
    "         ('insert', Top10000_insert),\n",
    "        ('remove', Top10000_remove),\n",
    "        ('swap', Top10000_swap),\n",
    "        ('homoglyph', Top10000_homoglyph),\n",
    "        ('repeat', Top10000_repeat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: from_items is deprecated. Please use DataFrame.from_dict(dict(items), ...) instead. DataFrame.from_dict(OrderedDict(items)) may be used to preserve the key order.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "noise_df= pd.DataFrame.from_items(noise)\n",
    "noise_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(word):\n",
    "    transformations = ['insert','remove','homoglyph','repeat_char']\n",
    "    s = np.random.randint(0,len(transformations))\n",
    "    \n",
    "    name = transformations\n",
    "    \n",
    "    if \"swap\" == name:\n",
    "        return swap(word)\n",
    "    elif \"insert\" == name:\n",
    "        return insert(word)\n",
    "    elif \"remove\" == name:\n",
    "        return remove(word)\n",
    "    elif \"homoglyph\" == name:\n",
    "        return homoglyph(word)\n",
    "    elif \"repeat_char\" == name:\n",
    "        return repeat_char(word)\n",
    "    elif \"distractor\" == name:\n",
    "        return distractor(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_transformed = [transform_sentence(sent) for sent in test1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = test\n",
    "test_2['comment_text'] = pd.Series(test2_repeat)\n",
    "test_2.to_csv(DATA_ROOT / 'new_large/test_noised_repeat.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
