{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import *\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, SequenceLabelField, LabelField\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs:\n",
    "\n",
    "- Make compatible with GPU\n",
    "- Try replicating SST results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    testing=True,\n",
    "    seed=1,\n",
    "    batch_size=64, # This is probably too large: need to handle effective v.s. machine batch size\n",
    "    embed_dim=256,\n",
    "    hidden_sz=768,\n",
    "    dataset=\"jigsaw\",\n",
    "    n_classes=2,\n",
    "    max_seq_len=128, # necessary to limit memory usage\n",
    "#     bert_model=None,\n",
    "    bert_model=\"bert-base-cased\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"../data\") / config.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed manually to replicate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x119157e70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.dataset_readers import DatasetReader, StanfordSentimentTreeBankDatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_registry = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register(name: str):\n",
    "    def dec(x: Callable):\n",
    "        reader_registry[name] = x\n",
    "        return x\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register(\"jigsaw\")\n",
    "class JigsawDatasetReader(DatasetReader):\n",
    "    def __init__(self, tokenizer: Callable[[str], List[str]]=lambda x: x.split(),\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None, # TODO: Handle mapping from BERT\n",
    "                 max_seq_len: Optional[int]=config.max_seq_len) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def text_to_instance(self, tokens: List[Token], label: str = None) -> Instance:\n",
    "        # TODO: Reimplement\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"tokens\": sentence_field}\n",
    "\n",
    "        label_field = LabelField(label=label, skip_indexing=True)\n",
    "        fields[\"label\"] = label_field\n",
    "\n",
    "        return Instance(fields)\n",
    "    \n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if config.testing: df = df.head(10000)\n",
    "        for i, row in df.iterrows():\n",
    "            yield self.text_to_instance(\n",
    "                [Token(x) for x in self.tokenizer(row[\"comment_text\"])],\n",
    "                row[\"toxic\"]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register(\"imdb\")\n",
    "class IMDBDatasetReader(DatasetReader):\n",
    "    def __init__(self, tokenizer=None, \n",
    "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                 max_seq_len=None) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def text_to_instance(self, tokens: List[Token], label: str = None) -> Instance:\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"tokens\": sentence_field}\n",
    "        \n",
    "        # TODO: Add statistical features?\n",
    "\n",
    "        label_field = LabelField(label=label)\n",
    "        fields[\"label\"] = label_field\n",
    "\n",
    "        return Instance(fields)\n",
    "\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        # TODO: Implement\n",
    "        for label in [\"pos\", \"neg\"]:\n",
    "            for file in (Path(file_path) / label).glob(\"*.txt\"):\n",
    "                text = file.open(\"rt\", encoding=\"utf-8\").read()\n",
    "                yield self.text_to_instance([Token(word) for word in self.tokenizer(text)], \n",
    "                                            label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare token handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/17/2019 17:11:41 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /Users/keitakurita/.pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.token_indexers import PretrainedBertIndexer, SingleIdTokenIndexer\n",
    "if config.bert_model is not None:\n",
    "    token_indexer = PretrainedBertIndexer(\n",
    "        pretrained_model=config.bert_model,\n",
    "        max_pieces=config.max_seq_len,\n",
    "        do_lowercase=\"uncased\" in config.bert_model,\n",
    "     )\n",
    "    # apparently we need to truncate the sequence here, which is a stupid design decision\n",
    "    def tokenizer(s: str):\n",
    "        return token_indexer.wordpiece_tokenizer(s)[:config.max_seq_len - 2]\n",
    "else:\n",
    "    token_indexer = SingleIdTokenIndexer(\n",
    "        lowercase_tokens=False,  # don't lowercase by default\n",
    "    )\n",
    "    tokenizer = lambda x: x.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_cls = reader_registry[config.dataset]\n",
    "reader = reader_cls(tokenizer=tokenizer,\n",
    "                    token_indexers={\"tokens\": token_indexer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:07, 1410.47it/s]\n",
      "10000it [00:06, 1529.95it/s]\n"
     ]
    }
   ],
   "source": [
    "if config.dataset == \"IMDB\":\n",
    "    data_dir = DATA_ROOT / \"imdb\" / \"aclImdb\"\n",
    "    train_ds, test_ds = (reader.read(data_dir / fname) for fname in [\"train\", \"test\"])\n",
    "    val_ds = None\n",
    "elif config.dataset == \"SST\":\n",
    "    pass # TODO: Implement\n",
    "else:\n",
    "    train_ds, test_ds = (reader.read(DATA_ROOT / fname) for fname in [\"train.csv\", \"test_proced.csv\"])\n",
    "    val_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.bert_model is not None: \n",
    "    vocab = Vocabulary()\n",
    "    token_indexer._add_encoding_to_vocabulary(vocab)\n",
    "else:\n",
    "    vocab = Vocabulary.from_instances(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.data.iterators import BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Allow for customization\n",
    "iterator = BucketIterator(batch_size=config.batch_size, \n",
    "                          biggest_batch_first=True,\n",
    "                          sorting_keys=[(\"tokens\", \"num_tokens\")],)\n",
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    batch = next(iter(iterator(train_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': {'tokens': tensor([[  101,  3320,  1116,  ..., 24834, 28137,   102],\n",
       "          [  101,   107,   156,  ...,  1474,  1122,   102],\n",
       "          [  101, 15112, 14494,  ...,  1559, 28131,   102],\n",
       "          ...,\n",
       "          [  101,  1345,  1386,  ...,  1103,  3371,   102],\n",
       "          [  101,   107,  6710,  ...,  1592,  1590,   102],\n",
       "          [  101,   107,  5046,  ...,  2543,   107,   102]]),\n",
       "  'tokens-offsets': tensor([[  1,   2,   3,  ..., 124, 125, 126],\n",
       "          [  1,   2,   3,  ..., 124, 125, 126],\n",
       "          [  1,   2,   3,  ..., 124, 125, 126],\n",
       "          ...,\n",
       "          [  1,   2,   3,  ..., 124, 125, 126],\n",
       "          [  1,   2,   3,  ..., 124, 125, 126],\n",
       "          [  1,   2,   3,  ..., 124, 125, 126]]),\n",
       "  'mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1]])},\n",
       " 'label': tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  3320,  1116,  ..., 24834, 28137,   102],\n",
       "        [  101,   107,   156,  ...,  1474,  1122,   102],\n",
       "        [  101, 15112, 14494,  ...,  1559, 28131,   102],\n",
       "        ...,\n",
       "        [  101,  1345,  1386,  ...,  1103,  3371,   102],\n",
       "        [  101,   107,  6710,  ...,  1592,  1590,   102],\n",
       "        [  101,   107,  5046,  ...,  2543,   107,   102]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.token_embedders.bert_token_embedder import BertEmbedder, PretrainedBertEmbedder\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.stacked_bidirectional_lstm import StackedBidirectionalLstm\n",
    "from allennlp.nn.util import get_text_field_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmEncoder(nn.Module):\n",
    "    def __init__(self, lstm):\n",
    "        super().__init__()\n",
    "        self.lstm = lstm\n",
    "        \n",
    "    def forward(self, x, mask): # TODO: replace with allennlp built in modules\n",
    "        _, (state, _) = self.lstm(x)\n",
    "        state = torch.cat([state[0, :, :], state[1, :, :]], dim=1)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    \"\"\"Source code copied\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(768, 768)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states, mask):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModel(Model):\n",
    "    def __init__(self, word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: StackedBidirectionalLstm,\n",
    "                 out_sz=config.n_classes):\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "#         self.projection = nn.Linear(encoder.get_output_dim(), out_sz)\n",
    "        self.projection = nn.Linear(config.hidden_sz, out_sz)\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        \n",
    "    def forward(self,\n",
    "                tokens: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor = None) -> torch.Tensor:\n",
    "        mask = get_text_field_mask(tokens)\n",
    "        embeddings = self.word_embeddings(tokens[\"tokens\"])\n",
    "        state = self.encoder(embeddings, mask)\n",
    "\n",
    "        class_logits = self.projection(state)\n",
    "        \n",
    "        output = {\"class_logits\": class_logits}\n",
    "        if label is not None:\n",
    "            self.accuracy(class_logits, label, None)\n",
    "            output[\"loss\"] = nn.CrossEntropyLoss()(class_logits, label)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/17/2019 17:11:59 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz from cache at /Users/keitakurita/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c\n",
      "01/17/2019 17:11:59 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /Users/keitakurita/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c to temp dir /var/folders/hy/1czs1y5j2d58zgkqx6w_wnpw0000gn/T/tmpucgh649e\n",
      "01/17/2019 17:12:02 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if config.bert_model is None:\n",
    "    token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                                embedding_dim=config.embed_dim)\n",
    "    word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "    # encoder = PytorchSeq2VecWrapper(nn.LSTM(config.embed_dim, config.hidden_sz, batch_first=True,\n",
    "    #                                         bidirectional=True))\n",
    "    encoder = LstmEncoder(\n",
    "        nn.LSTM(config.embed_dim, config.hidden_sz, batch_first=True, bidirectional=True)\n",
    "    )\n",
    "\n",
    "else:\n",
    "    word_embeddings = PretrainedBertEmbedder(\n",
    "        pretrained_model=config.bert_model,\n",
    "        top_layer_only=True, # conserve memory\n",
    "    )\n",
    "    encoder = BertPooler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentAnalysisModel(\n",
    "    word_embeddings, \n",
    "    encoder, \n",
    "    out_sz=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentAnalysisModel(\n",
       "  (word_embeddings): PretrainedBertEmbedder(\n",
       "    (bert_model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(28996, 768)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (projection): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(list(model.word_embeddings.parameters())[0].detach().numpy()).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.isnan(x.detach().numpy()).any() for x in list(model.encoder.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.isinf(x.detach().numpy()).any() for x in list(model.encoder.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0755,  0.2792, -0.2031,  ..., -0.5464, -0.2699, -0.1019],\n",
       "        [-0.2183,  0.3834, -0.3327,  ..., -0.4463, -0.1837,  0.0743],\n",
       "        [-0.1531,  0.2794, -0.2308,  ..., -0.5165, -0.0732, -0.2113],\n",
       "        ...,\n",
       "        [-0.0934,  0.3107, -0.0174,  ..., -0.4465, -0.2054, -0.1657],\n",
       "        [-0.1436,  0.4138, -0.1198,  ..., -0.4273,  0.0112, -0.2166],\n",
       "        [-0.1005,  0.4191, -0.0668,  ..., -0.5434, -0.1772, -0.1340]],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = batch[\"tokens\"]\n",
    "encoder(model.word_embeddings(tokens[\"tokens\"]), get_text_field_mask(tokens))\n",
    "# encoder(model.word_embeddings(tokens[\"tokens\"]))[1][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(**batch)[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7642, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.CrossEntropyLoss()(model(**batch)[\"class_logits\"][:10, :], batch[\"label\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7745, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 8.5304e-03,  3.5480e-04, -1.8205e-03,  ..., -8.0811e-03,\n",
       "           5.0827e-03,  1.0404e-03],\n",
       "         [-3.4866e-03, -2.8361e-05,  7.4339e-04,  ...,  3.3525e-03,\n",
       "          -1.9785e-03, -3.1576e-04],\n",
       "         [ 5.3810e-03,  2.0569e-04, -1.1151e-03,  ..., -5.1062e-03,\n",
       "           3.2566e-03,  6.4371e-04],\n",
       "         ...,\n",
       "         [-4.5126e-03, -1.9369e-04,  9.5409e-04,  ...,  4.3904e-03,\n",
       "          -2.7528e-03, -4.6456e-04],\n",
       "         [ 1.9360e-03,  6.3666e-05, -4.0991e-04,  ..., -1.8234e-03,\n",
       "           1.1646e-03,  2.4010e-04],\n",
       "         [ 5.1147e-03,  1.6980e-04, -1.0655e-03,  ..., -4.8573e-03,\n",
       "           3.0409e-03,  5.9746e-04]]),\n",
       " tensor([ 1.9886e-02, -8.1437e-03,  1.2440e-02, -7.9120e-03,  9.4516e-03,\n",
       "          1.6293e-02, -6.7221e-03,  1.3400e-02,  3.4142e-03,  9.0865e-03,\n",
       "          5.8568e-03,  9.3201e-03,  1.3623e-02, -9.1804e-03,  8.2231e-04,\n",
       "          7.2423e-03, -1.4093e-02,  3.5922e-03,  8.2995e-03,  2.2107e-02,\n",
       "         -1.1998e-02,  3.4935e-03,  1.4428e-02,  1.2406e-02,  9.9006e-03,\n",
       "         -1.1806e-03,  8.3171e-03,  8.4603e-03, -2.7376e-02, -8.9869e-03,\n",
       "          1.1443e-02, -1.0189e-02, -2.6449e-02,  1.8552e-03,  3.6768e-03,\n",
       "          1.3816e-02, -1.6096e-02,  4.1728e-03,  9.9355e-04, -7.4502e-03,\n",
       "         -1.4104e-02, -7.8427e-03, -1.5436e-03,  2.5165e-03, -4.3343e-03,\n",
       "         -7.8672e-03, -6.6168e-03, -1.4877e-03,  6.5369e-03, -6.6559e-03,\n",
       "          6.9124e-03, -1.6385e-02, -1.3358e-03,  7.2326e-04, -1.1366e-02,\n",
       "          1.3015e-02, -3.3013e-03,  4.0377e-03,  1.9634e-02,  3.9142e-03,\n",
       "          1.6996e-02, -1.7434e-03, -1.4207e-03,  1.6449e-03, -4.6920e-03,\n",
       "          1.0126e-02, -5.0880e-03, -7.2225e-03,  1.5690e-02,  7.4629e-03,\n",
       "          1.0632e-02,  1.9039e-02,  1.6168e-03, -6.8588e-03,  1.8137e-02,\n",
       "         -2.2437e-02, -1.1493e-02,  4.9379e-03,  3.7247e-04, -1.1542e-02,\n",
       "          1.9995e-03, -3.4957e-03,  1.1249e-02,  5.4930e-03,  6.1514e-04,\n",
       "         -1.8515e-02, -1.4007e-02, -1.8581e-02, -4.3932e-03,  1.6156e-02,\n",
       "         -8.1758e-03,  6.9876e-03, -5.1691e-03, -1.4968e-02, -2.2510e-02,\n",
       "          8.4508e-03, -1.5834e-02, -1.4140e-02,  8.8906e-03,  2.4448e-02,\n",
       "          3.6668e-03, -7.3529e-03,  9.4830e-03,  9.4702e-03, -4.6109e-03,\n",
       "          1.5713e-02,  1.0514e-02, -2.2020e-03, -2.7115e-02,  1.1832e-02,\n",
       "          2.0943e-02,  2.4983e-03, -7.1680e-03, -2.8239e-04,  1.0660e-02,\n",
       "          2.2811e-02, -6.7237e-03, -7.9501e-03, -9.3090e-03, -1.5236e-02,\n",
       "         -3.2768e-03, -9.7122e-03, -7.9107e-04, -8.0767e-03,  2.9443e-03,\n",
       "          1.6593e-03,  2.6091e-03,  5.7166e-04,  1.6795e-04, -1.1756e-02,\n",
       "          8.4013e-04, -3.7703e-04,  1.3565e-02,  1.0986e-02, -1.4191e-03,\n",
       "          4.7790e-03,  1.5934e-02,  8.2650e-03,  3.9735e-04, -2.0392e-02,\n",
       "          3.9454e-03,  2.1063e-03, -9.0934e-03, -1.6337e-02,  1.0030e-03,\n",
       "          5.9042e-04,  1.5459e-02,  1.0662e-02, -3.0399e-03,  1.6471e-02,\n",
       "          9.7347e-03, -5.9926e-03,  2.2302e-03, -6.9362e-03,  1.2224e-02,\n",
       "         -1.4895e-02, -1.9568e-03, -1.1467e-02, -6.0522e-03,  1.6944e-02,\n",
       "          1.6646e-02, -1.7459e-02,  2.7926e-03, -4.8074e-03, -2.1913e-02,\n",
       "         -2.2667e-02,  1.8404e-03, -2.2475e-03, -2.1689e-02,  1.1001e-02,\n",
       "          2.0603e-03, -2.1548e-03, -1.7075e-03,  8.7272e-03, -1.3221e-02,\n",
       "          2.1093e-02, -1.9390e-02, -9.2050e-03, -1.3807e-02, -6.2125e-03,\n",
       "          1.4035e-02, -7.9385e-03, -1.4197e-02, -8.3870e-03, -5.5370e-03,\n",
       "         -6.8773e-03, -6.0182e-03, -9.4700e-03, -1.1318e-03, -4.6404e-03,\n",
       "          1.3812e-02, -1.8848e-02, -7.5696e-04, -1.1916e-02, -1.4242e-03,\n",
       "          1.0188e-02,  7.7654e-03, -1.0944e-02, -4.9782e-03,  1.0344e-02,\n",
       "         -1.0005e-02,  2.1178e-03, -1.2418e-02, -3.7715e-03,  8.1036e-04,\n",
       "         -1.0162e-02,  1.3139e-03, -1.4091e-02,  5.2259e-03,  2.1020e-02,\n",
       "         -1.9481e-02, -6.1455e-03, -1.2792e-02, -2.3723e-02,  4.1086e-03,\n",
       "         -5.4736e-03, -1.6783e-03,  1.1251e-02,  7.1068e-03,  2.9725e-04,\n",
       "         -3.7489e-04, -3.8817e-03,  7.2593e-03,  1.0513e-02,  1.3782e-02,\n",
       "          1.5145e-02,  1.7275e-02,  1.7424e-02,  2.1021e-03,  3.8903e-03,\n",
       "          1.6407e-03,  4.1977e-03, -3.3637e-04,  1.0391e-02,  1.6809e-02,\n",
       "         -2.2690e-03, -4.1769e-03, -4.1699e-03,  5.2027e-03,  4.8242e-03,\n",
       "         -7.6327e-03, -1.0926e-02, -1.7477e-02, -1.9572e-02,  2.3057e-02,\n",
       "         -1.2824e-02, -8.5604e-03, -7.7049e-03,  1.2464e-03, -1.6305e-02,\n",
       "          1.5767e-02, -1.4632e-02, -8.8475e-03,  3.5748e-03, -1.0753e-05,\n",
       "          3.0039e-03,  1.2245e-02, -1.4613e-03, -6.2559e-03, -2.3533e-02,\n",
       "          4.6910e-03, -9.6169e-03, -2.3297e-03, -6.2977e-03,  5.4696e-03,\n",
       "         -2.1789e-02,  1.7159e-02,  2.3360e-03,  1.4222e-02, -1.2967e-02,\n",
       "         -7.1086e-03,  1.6025e-02, -1.5498e-02, -6.8069e-03, -4.4433e-03,\n",
       "         -6.5238e-03,  7.9669e-03, -8.2989e-03, -5.2797e-03,  5.1457e-04,\n",
       "          1.8038e-02, -1.2273e-02, -2.3473e-04,  2.0894e-04, -4.8719e-03,\n",
       "         -2.7704e-03, -1.4177e-02,  2.4699e-03, -6.3288e-03,  2.7843e-02,\n",
       "         -9.8799e-03,  8.7668e-03,  1.3532e-02, -2.8830e-03,  2.2329e-03,\n",
       "         -5.0205e-03, -2.1990e-02, -1.1985e-02, -8.2689e-03,  5.0435e-03,\n",
       "          2.2968e-02,  5.8376e-03,  3.2276e-03, -2.1473e-02,  4.4514e-03,\n",
       "          1.5275e-02,  3.1647e-03, -1.3133e-03, -1.6129e-02,  2.3918e-02,\n",
       "         -1.4856e-03, -1.6995e-03, -7.5709e-03, -1.1303e-02,  8.2846e-04,\n",
       "         -5.2283e-04,  2.8799e-04,  3.9039e-03, -3.7576e-03, -1.8676e-03,\n",
       "         -8.1469e-03,  5.4915e-03, -1.3575e-02,  1.0470e-02, -5.5820e-03,\n",
       "         -3.1936e-03,  8.1107e-03,  2.4013e-03, -7.9897e-03, -1.9538e-03,\n",
       "         -1.0027e-02, -1.3784e-02, -2.4930e-02,  1.6553e-02,  1.2514e-03,\n",
       "         -1.8467e-03, -1.1946e-02,  5.7718e-03,  2.0228e-02, -1.9977e-02,\n",
       "         -1.5277e-02, -9.7385e-03, -1.8290e-03,  8.3693e-04, -2.3142e-02,\n",
       "         -1.5495e-03, -1.2018e-02, -1.7475e-03, -1.4588e-03, -1.2080e-02,\n",
       "          1.0093e-02, -9.0134e-03,  8.0685e-03,  1.0118e-02, -1.5212e-02,\n",
       "         -9.3320e-04,  1.5281e-02, -2.5908e-02, -1.1897e-02,  1.5097e-02,\n",
       "          1.7593e-02,  7.1487e-03, -7.0348e-03,  1.4456e-02,  7.0984e-03,\n",
       "          1.3910e-02,  4.9618e-03, -2.0264e-03, -1.2617e-02, -1.3915e-02,\n",
       "         -2.2991e-03, -3.3186e-03, -6.1628e-03, -1.8683e-02,  5.7818e-03,\n",
       "         -1.9014e-03,  8.3510e-03,  1.5305e-02,  1.6161e-02, -9.7685e-04,\n",
       "         -2.3858e-02, -4.8124e-04,  2.2999e-03, -1.5275e-02, -1.3896e-02,\n",
       "          2.0840e-02, -1.5463e-02,  1.9840e-02, -7.0089e-04,  9.6544e-03,\n",
       "          2.2121e-02,  9.0127e-03, -7.1640e-03, -7.0355e-03,  4.1634e-03,\n",
       "          2.5876e-02, -1.6779e-02, -3.8744e-04,  6.3281e-03,  1.5096e-03,\n",
       "          5.1064e-03, -2.2514e-02,  1.0440e-02,  1.0448e-02, -9.8691e-03,\n",
       "          4.6395e-03, -9.0399e-03,  1.0247e-03, -2.5436e-02,  8.5594e-03,\n",
       "         -1.3159e-02, -4.1153e-03, -4.7034e-03, -3.8345e-03, -5.6333e-03,\n",
       "         -2.0183e-02,  3.8599e-03,  3.4000e-03, -3.4688e-04,  5.2748e-03,\n",
       "          2.3984e-02, -1.5374e-02,  3.2709e-03,  4.0432e-03,  1.2103e-02,\n",
       "         -2.1405e-03, -5.2513e-03, -3.2646e-03,  2.0165e-02,  3.6464e-03,\n",
       "          9.0538e-04, -2.4966e-03, -5.1496e-03,  2.5394e-02,  7.2437e-03,\n",
       "          2.7720e-03, -5.5326e-03, -1.1234e-02, -1.2922e-02, -1.6045e-02,\n",
       "          1.7348e-02, -1.5393e-02,  9.8033e-03, -8.0823e-03, -2.1628e-04,\n",
       "          2.6928e-04, -8.5479e-03, -4.1891e-03, -6.6914e-03, -2.8712e-03,\n",
       "          1.5857e-02, -1.5352e-02,  1.1372e-02, -1.6877e-03, -1.4756e-02,\n",
       "         -2.0447e-02,  6.8841e-03,  1.2851e-02,  6.9289e-03,  3.5252e-03,\n",
       "          2.5926e-03,  7.4883e-03, -8.9236e-03,  1.4034e-04, -1.6296e-02,\n",
       "          9.1791e-03, -1.0508e-02,  1.6370e-02,  2.0310e-03, -6.1669e-03,\n",
       "         -5.0627e-03,  1.1102e-02, -6.7888e-04,  1.5603e-03,  6.6865e-03,\n",
       "          1.0336e-02,  2.8388e-03, -1.2240e-02, -3.0882e-03,  7.1885e-03,\n",
       "         -1.9349e-02,  1.7010e-02, -9.6086e-03,  8.6631e-03,  4.3388e-04,\n",
       "          6.6624e-03,  1.2951e-03,  1.7355e-02, -1.4618e-02, -4.5841e-03,\n",
       "         -2.2293e-02,  2.4704e-03,  7.1145e-03,  2.2635e-04,  1.6803e-02,\n",
       "         -1.9044e-02, -5.3439e-03,  6.0818e-03,  3.4381e-03, -9.7435e-03,\n",
       "          2.4025e-03, -1.2514e-02, -1.1695e-02, -2.1869e-02,  7.1272e-03,\n",
       "         -4.2520e-03, -2.4618e-03,  9.7177e-03,  1.6748e-02,  1.3518e-02,\n",
       "          2.9439e-04,  9.2215e-03, -6.0143e-03, -1.2645e-03, -1.8648e-02,\n",
       "          1.1072e-02, -2.2654e-02,  1.6055e-02, -1.3235e-02, -1.2556e-02,\n",
       "          1.3314e-02,  2.4677e-03,  1.4474e-03, -1.6745e-02, -1.1156e-02,\n",
       "          1.3038e-02,  6.6297e-03,  4.5578e-04,  6.5759e-03, -7.7252e-03,\n",
       "         -2.0027e-03, -1.3984e-02, -6.1982e-03,  9.0905e-03,  7.9482e-03,\n",
       "         -9.7834e-03, -1.1215e-02,  1.4219e-02, -1.5792e-02, -9.2370e-03,\n",
       "          1.3328e-02, -1.0521e-02,  1.8868e-02, -1.6860e-02,  1.1350e-02,\n",
       "         -4.5305e-03, -4.3615e-03, -5.5282e-03,  1.9232e-03,  3.1106e-03,\n",
       "          6.8301e-03, -1.8279e-02,  8.2700e-03, -8.6914e-03,  2.3194e-02,\n",
       "          3.6723e-03, -2.2207e-02, -1.1867e-03,  1.5112e-02,  3.9826e-03,\n",
       "         -1.0906e-02,  2.0057e-02, -1.9010e-03,  6.7091e-04, -4.7201e-03,\n",
       "         -3.3280e-03, -8.2557e-03,  9.7319e-03,  1.0834e-02, -7.7460e-03,\n",
       "          1.4700e-02,  6.9692e-03,  1.2378e-02, -1.5567e-02,  1.3409e-02,\n",
       "         -1.1400e-02, -1.9216e-02,  4.8216e-03, -2.5543e-03,  2.4272e-02,\n",
       "          1.9935e-02,  9.9565e-03, -1.9794e-02, -5.3546e-03,  1.8038e-02,\n",
       "         -1.9649e-02, -9.2792e-03,  1.9634e-02,  7.7777e-03, -1.2116e-04,\n",
       "         -1.6072e-03,  1.4601e-02, -1.7675e-02, -1.7262e-02,  5.1151e-03,\n",
       "          9.2858e-05, -9.9098e-03, -1.8226e-02,  1.3617e-02,  2.1537e-03,\n",
       "         -2.3224e-03,  9.1229e-03,  9.5196e-03,  1.8690e-02,  3.7103e-03,\n",
       "          1.8118e-02, -2.4170e-02, -1.3227e-02, -1.3151e-02,  2.5017e-03,\n",
       "          5.5666e-03, -6.1785e-03,  6.0516e-03, -2.5363e-02,  2.0147e-03,\n",
       "         -6.1703e-03, -1.9701e-02, -3.2327e-03,  1.8595e-02,  6.3978e-03,\n",
       "          6.6954e-03, -7.4539e-04,  7.2891e-03,  5.6586e-03,  1.3542e-02,\n",
       "          1.4799e-02, -2.8735e-03,  2.7139e-03, -9.1053e-03,  1.1429e-02,\n",
       "          1.9891e-02,  1.4437e-02, -7.8779e-03,  3.9788e-03, -5.7469e-03,\n",
       "          1.1582e-02, -1.1639e-02, -4.7799e-03,  6.4881e-03,  1.2847e-02,\n",
       "          6.6435e-03, -1.9074e-03, -1.7342e-02,  1.3429e-02,  1.5658e-03,\n",
       "          1.3085e-02,  9.2683e-03,  1.3026e-03,  2.1848e-03,  1.8597e-02,\n",
       "         -3.8440e-03, -5.6701e-03,  1.4803e-02, -2.1852e-02,  2.9158e-03,\n",
       "         -4.2970e-03,  6.5892e-03,  1.1214e-02,  1.1614e-02, -1.5173e-02,\n",
       "          2.3284e-02, -2.2897e-02, -6.5871e-04,  2.4151e-02,  4.9812e-03,\n",
       "         -4.1850e-03, -3.4656e-03, -1.9028e-02, -2.6315e-02, -1.0261e-02,\n",
       "         -7.7496e-04,  2.2983e-02,  1.4282e-03, -1.2340e-02, -1.9332e-02,\n",
       "          1.2914e-02,  6.0962e-03,  2.2126e-03, -1.1103e-03, -6.6704e-03,\n",
       "          4.3420e-03,  1.0008e-02, -1.4382e-02, -1.9164e-02,  1.7134e-02,\n",
       "         -1.6219e-02, -5.6080e-03, -1.2149e-02, -1.7033e-02, -2.7021e-03,\n",
       "         -3.0743e-03, -2.2435e-02, -1.6463e-02,  1.2832e-02, -6.1630e-04,\n",
       "         -9.5840e-03, -2.0544e-04, -1.6898e-02, -1.1241e-02,  4.0987e-03,\n",
       "         -1.7546e-03,  1.8607e-02, -5.4119e-03,  1.2627e-02, -1.7905e-02,\n",
       "          1.8105e-03,  1.4171e-03,  2.3205e-02, -1.7410e-03,  2.0745e-03,\n",
       "          3.8521e-03, -3.2092e-03,  1.9075e-03,  1.0993e-02,  1.0294e-03,\n",
       "          3.0011e-03,  2.4401e-02,  2.2049e-02, -2.6993e-04,  6.8616e-03,\n",
       "          4.4201e-03,  1.1877e-02, -3.1957e-03,  3.5370e-03, -9.9759e-04,\n",
       "          1.0963e-02, -4.7262e-03, -1.5424e-03, -3.4642e-04,  1.2045e-02,\n",
       "         -1.1263e-02,  8.7025e-03, -1.6624e-02, -5.0122e-03, -4.7619e-03,\n",
       "          2.7721e-03,  6.5436e-03,  1.3155e-02,  5.8583e-03, -7.7498e-03,\n",
       "         -1.3585e-02,  6.5712e-04, -3.0557e-03,  2.0477e-03, -3.8188e-03,\n",
       "          1.3869e-02,  1.1793e-02, -1.3025e-02,  3.8415e-03, -1.3031e-02,\n",
       "         -5.3757e-03,  3.3364e-03,  4.1869e-03,  7.9248e-03,  6.1877e-03,\n",
       "          2.5654e-04,  1.4426e-02,  1.1353e-02,  9.0835e-03, -7.8865e-03,\n",
       "         -9.7632e-03,  5.0930e-03,  1.6908e-02,  1.6052e-03, -1.2295e-03,\n",
       "         -1.0550e-02,  4.4692e-03,  1.1905e-02])]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.grad for x in list(model.encoder.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_ds,\n",
    "                  validation_dataset=val_ds,\n",
    "                  serialization_dir=DATA_ROOT / \"ckpts\",\n",
    "                  patience=3,\n",
    "                  num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/17/2019 17:15:52 - INFO - allennlp.training.trainer -   Beginning training.\n",
      "01/17/2019 17:15:52 - INFO - allennlp.training.trainer -   Epoch 0/9\n",
      "01/17/2019 17:15:52 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 1792.45056\n",
      "01/17/2019 17:15:52 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9168, loss: 0.2234 ||: 100%|██████████| 157/157 [35:20<00:00, 12.87s/it]\n",
      "01/17/2019 17:51:13 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/17/2019 17:51:13 - INFO - allennlp.training.trainer -   loss          |     0.223  |       N/A\n",
      "01/17/2019 17:51:13 - INFO - allennlp.training.trainer -   cpu_memory_MB |  1792.451  |       N/A\n",
      "01/17/2019 17:51:13 - INFO - allennlp.training.trainer -   accuracy      |     0.917  |       N/A\n",
      "01/17/2019 17:51:13 - INFO - allennlp.training.trainer -   Epoch duration: 00:35:21\n",
      "01/17/2019 17:51:13 - INFO - allennlp.training.trainer -   Estimated training time remaining: 5:18:10\n",
      "01/17/2019 17:51:13 - INFO - allennlp.training.trainer -   Epoch 1/9\n",
      "01/17/2019 17:51:13 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 2293.202944\n",
      "01/17/2019 17:51:13 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9311, loss: 0.1800 ||: 100%|██████████| 157/157 [1:06:01<00:00, 11.34s/it] \n",
      "01/17/2019 18:57:15 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/17/2019 18:57:15 - INFO - allennlp.training.trainer -   loss          |     0.180  |       N/A\n",
      "01/17/2019 18:57:15 - INFO - allennlp.training.trainer -   cpu_memory_MB |  2293.203  |       N/A\n",
      "01/17/2019 18:57:15 - INFO - allennlp.training.trainer -   accuracy      |     0.931  |       N/A\n",
      "01/17/2019 18:57:15 - INFO - allennlp.training.trainer -   Epoch duration: 01:06:01\n",
      "01/17/2019 18:57:15 - INFO - allennlp.training.trainer -   Estimated training time remaining: 6:45:30\n",
      "01/17/2019 18:57:15 - INFO - allennlp.training.trainer -   Epoch 2/9\n",
      "01/17/2019 18:57:15 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 2293.202944\n",
      "01/17/2019 18:57:15 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9337, loss: 0.1684 ||: 100%|██████████| 157/157 [32:46<00:00,  7.92s/it]\n",
      "01/17/2019 19:30:02 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/17/2019 19:30:02 - INFO - allennlp.training.trainer -   loss          |     0.168  |       N/A\n",
      "01/17/2019 19:30:02 - INFO - allennlp.training.trainer -   cpu_memory_MB |  2293.203  |       N/A\n",
      "01/17/2019 19:30:02 - INFO - allennlp.training.trainer -   accuracy      |     0.934  |       N/A\n",
      "01/17/2019 19:30:02 - INFO - allennlp.training.trainer -   Epoch duration: 00:32:47\n",
      "01/17/2019 19:30:02 - INFO - allennlp.training.trainer -   Estimated training time remaining: 5:13:02\n",
      "01/17/2019 19:30:02 - INFO - allennlp.training.trainer -   Epoch 3/9\n",
      "01/17/2019 19:30:02 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 2293.202944\n",
      "01/17/2019 19:30:02 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9342, loss: 0.1705 ||: 100%|██████████| 157/157 [33:30<00:00, 10.82s/it]\n",
      "01/17/2019 20:03:32 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/17/2019 20:03:32 - INFO - allennlp.training.trainer -   loss          |     0.170  |       N/A\n",
      "01/17/2019 20:03:32 - INFO - allennlp.training.trainer -   cpu_memory_MB |  2293.203  |       N/A\n",
      "01/17/2019 20:03:32 - INFO - allennlp.training.trainer -   accuracy      |     0.934  |       N/A\n",
      "01/17/2019 20:03:32 - INFO - allennlp.training.trainer -   Epoch duration: 00:33:30\n",
      "01/17/2019 20:03:32 - INFO - allennlp.training.trainer -   Estimated training time remaining: 4:11:30\n",
      "01/17/2019 20:03:32 - INFO - allennlp.training.trainer -   Epoch 4/9\n",
      "01/17/2019 20:03:32 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 2293.202944\n",
      "01/17/2019 20:03:32 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9352, loss: 0.1714 ||: 100%|██████████| 157/157 [34:58<00:00, 13.29s/it]\n",
      "01/17/2019 20:38:31 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/17/2019 20:38:31 - INFO - allennlp.training.trainer -   loss          |     0.171  |       N/A\n",
      "01/17/2019 20:38:31 - INFO - allennlp.training.trainer -   cpu_memory_MB |  2293.203  |       N/A\n",
      "01/17/2019 20:38:31 - INFO - allennlp.training.trainer -   accuracy      |     0.935  |       N/A\n",
      "01/17/2019 20:38:31 - INFO - allennlp.training.trainer -   Epoch duration: 00:34:58\n",
      "01/17/2019 20:38:31 - INFO - allennlp.training.trainer -   Estimated training time remaining: 3:22:38\n",
      "01/17/2019 20:38:31 - INFO - allennlp.training.trainer -   Epoch 5/9\n",
      "01/17/2019 20:38:31 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 2293.202944\n",
      "01/17/2019 20:38:31 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9346, loss: 0.1671 ||: 100%|██████████| 157/157 [4:34:48<00:00, 2055.33s/it]  \n",
      "01/18/2019 01:13:20 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/18/2019 01:13:20 - INFO - allennlp.training.trainer -   loss          |     0.167  |       N/A\n",
      "01/18/2019 01:13:20 - INFO - allennlp.training.trainer -   cpu_memory_MB |  2293.203  |       N/A\n",
      "01/18/2019 01:13:20 - INFO - allennlp.training.trainer -   accuracy      |     0.935  |       N/A\n",
      "01/18/2019 01:13:20 - INFO - allennlp.training.trainer -   Epoch duration: 04:34:48\n",
      "01/18/2019 01:13:20 - INFO - allennlp.training.trainer -   Estimated training time remaining: 5:18:18\n",
      "01/18/2019 01:13:20 - INFO - allennlp.training.trainer -   Epoch 6/9\n",
      "01/18/2019 01:13:20 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 2293.202944\n",
      "01/18/2019 01:13:20 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9388, loss: 0.1602 ||: 100%|██████████| 157/157 [7:10:54<00:00, 12.39s/it]    \n",
      "01/18/2019 08:24:14 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/18/2019 08:24:14 - INFO - allennlp.training.trainer -   loss          |     0.160  |       N/A\n",
      "01/18/2019 08:24:14 - INFO - allennlp.training.trainer -   cpu_memory_MB |  2293.203  |       N/A\n",
      "01/18/2019 08:24:14 - INFO - allennlp.training.trainer -   accuracy      |     0.939  |       N/A\n",
      "01/18/2019 08:24:14 - INFO - allennlp.training.trainer -   Epoch duration: 07:10:54\n",
      "01/18/2019 08:24:14 - INFO - allennlp.training.trainer -   Estimated training time remaining: 6:29:18\n",
      "01/18/2019 08:24:14 - INFO - allennlp.training.trainer -   Epoch 7/9\n",
      "01/18/2019 08:24:14 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 2293.202944\n",
      "01/18/2019 08:24:14 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9389, loss: 0.1612 ||: 100%|██████████| 157/157 [33:04<00:00,  7.04s/it]\n",
      "01/18/2019 08:57:19 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/18/2019 08:57:19 - INFO - allennlp.training.trainer -   loss          |     0.161  |       N/A\n",
      "01/18/2019 08:57:19 - INFO - allennlp.training.trainer -   cpu_memory_MB |  2293.203  |       N/A\n",
      "01/18/2019 08:57:19 - INFO - allennlp.training.trainer -   accuracy      |     0.939  |       N/A\n",
      "01/18/2019 08:57:19 - INFO - allennlp.training.trainer -   Epoch duration: 00:33:04\n",
      "01/18/2019 08:57:19 - INFO - allennlp.training.trainer -   Estimated training time remaining: 3:55:21\n",
      "01/18/2019 08:57:19 - INFO - allennlp.training.trainer -   Epoch 8/9\n",
      "01/18/2019 08:57:19 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 2293.202944\n",
      "01/18/2019 08:57:19 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9375, loss: 0.1633 ||: 100%|██████████| 157/157 [33:23<00:00, 15.20s/it]\n",
      "01/18/2019 09:30:42 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/18/2019 09:30:42 - INFO - allennlp.training.trainer -   loss          |     0.163  |       N/A\n",
      "01/18/2019 09:30:42 - INFO - allennlp.training.trainer -   cpu_memory_MB |  2293.203  |       N/A\n",
      "01/18/2019 09:30:42 - INFO - allennlp.training.trainer -   accuracy      |     0.938  |       N/A\n",
      "01/18/2019 09:30:42 - INFO - allennlp.training.trainer -   Epoch duration: 00:33:23\n",
      "01/18/2019 09:30:42 - INFO - allennlp.training.trainer -   Estimated training time remaining: 1:48:18\n",
      "01/18/2019 09:30:42 - INFO - allennlp.training.trainer -   Epoch 9/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/18/2019 09:30:42 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 2293.202944\n",
      "01/18/2019 09:30:42 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9370, loss: 0.1591 ||: 100%|██████████| 157/157 [34:20<00:00, 11.01s/it]\n",
      "01/18/2019 10:05:03 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/18/2019 10:05:03 - INFO - allennlp.training.trainer -   loss          |     0.159  |       N/A\n",
      "01/18/2019 10:05:03 - INFO - allennlp.training.trainer -   cpu_memory_MB |  2293.203  |       N/A\n",
      "01/18/2019 10:05:03 - INFO - allennlp.training.trainer -   accuracy      |     0.937  |       N/A\n",
      "01/18/2019 10:05:03 - INFO - allennlp.training.trainer -   Epoch duration: 00:34:20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'peak_cpu_memory_MB': 2293.202944,\n",
       " 'training_duration': '16:49:10',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 9,\n",
       " 'epoch': 9,\n",
       " 'training_accuracy': 0.937,\n",
       " 'training_loss': 0.15912552930082485,\n",
       " 'training_cpu_memory_MB': 2293.202944,\n",
       " 'best_epoch': 9}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
