{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"../data\") / \"jigsaw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_ROOT / \"train.csv\")\n",
    "test = pd.read_csv(DATA_ROOT / \"test_proced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_trn = train[train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].sum(1) > 0]\n",
    "toxic_trn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_trn.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "ptrn = re.compile(\"(\\n|\\.|\\?|!)\")\n",
    "def split_sent(s: str):\n",
    "    splits = ptrn.split(s)\n",
    "    for i, (sts, nsts) in enumerate(zip(splits, splits[1:])):\n",
    "        if i % 2 == 0:\n",
    "            yield sts + nsts\n",
    "    if len(splits[-1]) > 0:\n",
    "        yield splits[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern 0: Use individual sentences as training data as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_sents = [(split_sent(row[\"comment_text\"]), row) for i, row in toxic_trn.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_words_in_sentence = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_extra = []\n",
    "for sents, row in extra_sents:\n",
    "    for s in sents:\n",
    "        if len(s.split()) > min_words_in_sentence:\n",
    "            train_extra.append({\n",
    "                \"id\": row[\"id\"],\n",
    "                \"comment_text\": s,\n",
    "                \"toxic\": row[\"toxic\"],\n",
    "                \"severe_toxic\": row[\"severe_toxic\"],\n",
    "                \"obscene\": row[\"obscene\"],\n",
    "                \"threat\": row[\"threat\"],\n",
    "                \"insult\": row[\"insult\"],\n",
    "                \"identity_hate\": row[\"identity_hate\"],\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_extra = pd.DataFrame(train_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_extra[train.columns].to_csv(DATA_ROOT / \"train_extra.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern 1: Only interpolate within toxic class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_extra = pd.read_csv(DATA_ROOT / \"train_extra.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_extra[\"lens\"] = train_extra[\"comment_text\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "p1, p2 = np.random.permutation(len(train_extra)), np.random.permutation(len(train_extra))\n",
    "new_data = []\n",
    "for i1, i2 in zip(p1, p2):\n",
    "    r1, r2 = train_extra.iloc[i1], train_extra.iloc[i2]\n",
    "    new_data.append({\n",
    "        \"id\": r1[\"id\"] + \"_\" + r2[\"id\"],\n",
    "        \"comment_text\": r1[\"comment_text\"] + \" \" + r2[\"comment_text\"],\n",
    "        \"toxic\": (r1[\"toxic\"] + r2[\"toxic\"]) / 2,\n",
    "        \"toxic\": (r1[\"toxic\"] + r2[\"toxic\"]) / 2,\n",
    "        \"severe_toxic\": (r1[\"severe_toxic\"] + r2[\"severe_toxic\"]) / 2,\n",
    "        \"obscene\": (r1[\"obscene\"] + r2[\"obscene\"]) / 2,\n",
    "        \"threat\": (r1[\"threat\"] + r2[\"threat\"]) / 2,\n",
    "        \"insult\": (r1[\"insult\"] + r2[\"insult\"]) / 2,\n",
    "        \"identity_hate\": (r1[\"identity_hate\"] + r2[\"identity_hate\"]) / 2,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.DataFrame(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data[train.columns].to_csv(DATA_ROOT / \"train_extra_interpolated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use augmented data provided by authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train = pd.read_csv(DATA_ROOT / \"train_aug_bt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "# aug_train_pos = aug_train[aug_train[cols].sum(1) > 0][train.columns]\n",
    "aug_train_pos = aug_train[train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([train, aug_train_pos], axis=0).to_csv(DATA_ROOT / \"train_with_bt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
