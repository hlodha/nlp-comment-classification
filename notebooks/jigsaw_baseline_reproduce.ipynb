{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "dependencies"
    ]
   },
   "outputs": [],
   "source": [
    "depends_on = [\n",
    "    \"preproc_jigsaw\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for seamlessly running on colab\n",
    "import os\n",
    "os.environ[\"IS_COLAB\"] = \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ[\"IS_COLAB\"] == \"True\":\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ \"$IS_COLAB\" = \"True\" ]; then\n",
    "    pip install git+https://github.com/facebookresearch/fastText.git\n",
    "    pip install torch\n",
    "    pip install torchvision\n",
    "    pip install allennlp\n",
    "    pip install dnspython\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import *\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.nn import util as nn_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "    \n",
    "import functools\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "def get_ref_free_exc_info():\n",
    "    \"Free traceback from references to locals/globals to avoid circular reference leading to gc.collect() unable to reclaim memory\"\n",
    "    type, val, tb = sys.exc_info()\n",
    "    traceback.clear_frames(tb)\n",
    "    return (type, val, tb)\n",
    "\n",
    "def gpu_mem_restore(func):\n",
    "    \"Reclaim GPU RAM if CUDA out of memory happened, or execution was interrupted\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except:\n",
    "            type, val, tb = get_ref_free_exc_info() # must!\n",
    "            raise type(val).with_traceback(tb) from None\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# for papermill\n",
    "testing = True\n",
    "debugging = False\n",
    "seed = 1\n",
    "computational_batch_size = 256\n",
    "batch_size = 256\n",
    "lr = 0.001\n",
    "epochs = 1\n",
    "hidden_sz = 64\n",
    "dataset = \"jigsaw\"\n",
    "n_classes = 6\n",
    "max_seq_len = 512\n",
    "download_data = False\n",
    "ft_model_path = \"../data/jigsaw/ft_model.bin\"\n",
    "max_vocab_size = 300000\n",
    "dropoute = 0.5\n",
    "val_ratio = 0.0\n",
    "use_augmented = False\n",
    "run_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Can we make this play better with papermill?\n",
    "config = Config(\n",
    "    testing=testing,\n",
    "    debugging=debugging,\n",
    "    seed=seed,\n",
    "    computational_batch_size=computational_batch_size,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    epochs=epochs,\n",
    "    hidden_sz=hidden_sz,\n",
    "    dataset=dataset,\n",
    "    n_classes=n_classes,\n",
    "    max_seq_len=max_seq_len, # necessary to limit memory usage\n",
    "    ft_model_path=ft_model_path,\n",
    "    max_vocab_size=max_vocab_size,\n",
    "    dropoute=dropoute,\n",
    "    val_ratio=val_ratio,\n",
    "    use_augmented=use_augmented,\n",
    "    run_id=run_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common.checks import ConfigurationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "RUN_ID = config.run_id if config.run_id is not None else now.strftime(\"%m_%d_%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ[\"IS_COLAB\"] == \"True\":\n",
    "    # Modify if your configuration is different\n",
    "    DATA_ROOT = Path(\"./gdrive\") / \"My Drive\" / \"Colab_Workspace\" / \"Colab Notebooks\" / \"data\" / config.dataset\n",
    "else:\n",
    "    DATA_ROOT = Path(\"../data\") / config.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {DATA_ROOT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "if download_data:\n",
    "    if config.val_ratio > 0.0:\n",
    "        fnames = [\"train_wo_val.csv\", \"test_proced.csv\", \"val.csv\", \"ft_model.bin\"]\n",
    "    else:\n",
    "        fnames = [\"train.csv\", \"test_proced.csv\", \"ft_model.bin\"]\n",
    "    for fname in fnames:\n",
    "        if not (DATA_ROOT / fname).exists():\n",
    "            print(subprocess.Popen([f\"aws s3 cp s3://nnfornlp/raw_data/jigsaw/{fname} {str(DATA_ROOT)}\"],\n",
    "                                   shell=True, stdout=subprocess.PIPE).stdout.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {DATA_ROOT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed manually to replicate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.dataset_readers import DatasetReader, StanfordSentimentTreeBankDatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_registry = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register(name: str):\n",
    "    def dec(x: Callable):\n",
    "        reader_registry[name] = x\n",
    "        return x\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = [\"toxic\", \"severe_toxic\", \"obscene\",\n",
    "              \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.fields import TextField, SequenceLabelField, LabelField, MetadataField, ArrayField\n",
    "\n",
    "@register(\"jigsaw\")\n",
    "class JigsawDatasetReader(DatasetReader):\n",
    "    def __init__(self, tokenizer: Callable[[str], List[str]]=lambda x: x.split(),\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None, # TODO: Handle mapping from BERT\n",
    "                 max_seq_len: Optional[int]=config.max_seq_len) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self, tokens: List[Token], id: str,\n",
    "                         labels: np.ndarray) -> Instance:\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"tokens\": sentence_field}\n",
    "        \n",
    "        id_field = MetadataField(id)\n",
    "        fields[\"id\"] = id_field\n",
    "        \n",
    "        label_field = ArrayField(array=labels)\n",
    "        fields[\"label\"] = label_field\n",
    "\n",
    "        return Instance(fields)\n",
    "    \n",
    "    @overrides\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if config.testing: df = df.head(1000)\n",
    "        for i, row in df.iterrows():\n",
    "            yield self.text_to_instance(\n",
    "                [Token(x) for x in self.tokenizer(row[\"comment_text\"])],\n",
    "                row[\"id\"], row[label_cols].values,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare token handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.data.token_indexers import WordpieceIndexer, SingleIdTokenIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "\n",
    "token_indexer = SingleIdTokenIndexer(\n",
    "    lowercase_tokens=False,  # don't lowercase by default\n",
    ")\n",
    "def tokenizer(x: str):\n",
    "    return [w.text for w in\n",
    "            SpacyWordSplitter(language='en_core_web_sm', \n",
    "                              pos_tags=False).split_words(x)[:config.max_seq_len]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = JigsawDatasetReader(\n",
    "    tokenizer=tokenizer,\n",
    "    token_indexers={\"tokens\": token_indexer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.val_ratio > 0.0:\n",
    "    train_ds, val_ds, test_ds = (reader.read(DATA_ROOT / fname) for fname in [\"train_wo_val.csv\",\n",
    "                                                                              \"val.csv\",\n",
    "                                                                              \"test_proced.csv\"])\n",
    "else:\n",
    "    train_ds, test_ds = (reader.read(DATA_ROOT / fname) for fname in [\"train.csv\",\n",
    "                                                                      \"test_proced.csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary.from_instances(train_ds, max_vocab_size=config.max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BucketIterator, DataIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Allow for customization\n",
    "iterator = BucketIterator(batch_size=config.batch_size, \n",
    "                          biggest_batch_first=True,\n",
    "                          sorting_keys=[(\"tokens\", \"num_tokens\")],\n",
    "                         )\n",
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(iterator(train_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"tokens\"][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"tokens\"][\"tokens\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.token_embedders.bert_token_embedder import BertEmbedder, PretrainedBertEmbedder\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.stacked_bidirectional_lstm import StackedBidirectionalLstm\n",
    "from allennlp.nn.util import get_text_field_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, inp_sz, dim=1, eps=1e-9):\n",
    "        super().__init__()\n",
    "        self.inp_sz, self.dim, self.eps = inp_sz, dim, eps\n",
    "        self.l1 = nn.Linear(inp_sz, inp_sz)\n",
    "        nn.init.xavier_uniform_(self.l1.weight.data)\n",
    "        nn.init.zeros_(self.l1.bias.data)\n",
    "        \n",
    "        vw = torch.zeros(inp_sz, 1)\n",
    "        nn.init.xavier_uniform_(vw)        \n",
    "        self.vw = nn.Parameter(vw)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        e = torch.tanh(self.l1(x))\n",
    "        e = torch.einsum(\"bij,jk->bi\", [e, self.vw])            \n",
    "        a = torch.exp(e)\n",
    "        \n",
    "        if mask is not None: a = a.masked_fill(mask, 0)\n",
    "\n",
    "        a = a / (torch.sum(a, dim=self.dim, keepdim=True) + self.eps)\n",
    "\n",
    "        weighted_input = x * a.unsqueeze(-1)\n",
    "        return torch.sum(weighted_input, dim=1), a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_gru_weights(gru: nn.GRU):\n",
    "    \"\"\"Applies orthogonal and xavier uniform initialization according to best practices\"\"\"\n",
    "    for nm, param in gru.named_parameters():\n",
    "        if \"weight_hh\" in nm:\n",
    "            nn.init.orthogonal_(param.data)\n",
    "        elif \"weight_ih\" in nm:\n",
    "            nn.init.xavier_uniform_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRUAttentionEncoder(Seq2VecEncoder):\n",
    "    def __init__(self, embed_sz: int, hidden_sz: int, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embed_sz = embed_sz\n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.gru = nn.GRU(self.embed_sz, self.hidden_sz,\n",
    "                          num_layers=num_layers, bidirectional=True)\n",
    "        init_gru_weights(self.gru)\n",
    "        self.attention = Attention(self.hidden_sz * 2, dim=1)\n",
    "        \n",
    "    @overrides\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self.embed_sz\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self.hidden_sz * 2\n",
    "    \n",
    "    @overrides\n",
    "    def forward(self, x: torch.tensor, \n",
    "                mask: Optional[torch.tensor]=None) -> torch.tensor:\n",
    "        x, _ = self.gru(x, None)\n",
    "        x, _ = self.attention(x, mask=mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training.metrics import CategoricalAccuracy, BooleanAccuracy, Metric\n",
    "\n",
    "def prod(x: Iterable):\n",
    "    acc = 1\n",
    "    for v in x: acc *= v\n",
    "    return acc\n",
    "\n",
    "class MultilabelAccuracy(Metric):\n",
    "    def __init__(self, thres=0.5):\n",
    "        self.thres = 0.5\n",
    "        self.correct_count = 0\n",
    "        self.total_count = 0\n",
    "    \n",
    "    def __call__(self, logits: torch.FloatTensor, \n",
    "                 t: torch.LongTensor) -> float:\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        t = t.detach().cpu().numpy()\n",
    "        cc = ((logits >= self.thres) == t).sum()\n",
    "        tc = prod(logits.shape)\n",
    "        self.correct_count += cc\n",
    "        self.total_count += tc\n",
    "        return cc / tc\n",
    "    \n",
    "    def get_metric(self, reset: bool=False):\n",
    "        acc = self.correct_count / self.total_count\n",
    "        if reset:\n",
    "            self.reset()\n",
    "        return acc\n",
    "    \n",
    "    @overrides\n",
    "    def reset(self):\n",
    "        self.correct_count = 0\n",
    "        self.total_count = 0\n",
    "    \n",
    "class MultilabelCrossEntropyLoss(nn.Module):\n",
    "    def forward(self, lgt, tgt: torch.LongTensor):\n",
    "        neg_abs = -lgt.abs()\n",
    "        loss = lgt.clamp(min=0) - lgt * tgt.float() + (1 + neg_abs.exp()).log()\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "\n",
    "class BaselineModel(Model):\n",
    "    def __init__(self, word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 out_sz: int=config.n_classes,\n",
    "                 multilabel: bool=True):\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.projection = nn.Linear(self.encoder.get_output_dim(), out_sz)\n",
    "        self.multilabel = multilabel\n",
    "        # TODO: Handle multiclass case\n",
    "        if self.multilabel:\n",
    "            self.accuracy = MultilabelAccuracy()\n",
    "            self.per_label_acc = {c: MultilabelAccuracy() for c in label_cols}\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        else:\n",
    "            self.loss = nn.CrossEntropyLoss()\n",
    "            self.accuracy = CategoricalAccuracy()\n",
    "        self.is_test_mode = False\n",
    "    \n",
    "    def test_mode(self):\n",
    "        self.is_test_mode = True\n",
    "\n",
    "    def forward(self, tokens: Dict[str, torch.Tensor],\n",
    "                id: Any, label: torch.Tensor) -> torch.Tensor:\n",
    "        if self.is_test_mode: tokens[\"tokens\"] *= 0\n",
    "        mask = get_text_field_mask(tokens) == 0\n",
    "        embeddings = self.word_embeddings(tokens)\n",
    "        state = self.encoder(embeddings, mask)\n",
    "        class_logits = self.projection(state)\n",
    "        \n",
    "        output = {\"class_logits\": class_logits}\n",
    "\n",
    "        # This is grossly inefficient...\n",
    "        output[\"accuracy\"] = self.accuracy(class_logits, label)\n",
    "        output[\"loss\"] = self.loss(class_logits, label)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText\n",
    "\n",
    "def get_fasttext_embeddings(model_path: str, vocab: Vocabulary):\n",
    "    vocab_size = min(vocab.get_vocab_size(), config.max_vocab_size)\n",
    "    ft_model = fastText.load_model(config.ft_model_path)\n",
    "    embedding_dim = ft_model.get_dimension()\n",
    "\n",
    "    # register parameters\n",
    "    config.set(\"vocab_size\", vocab_size)\n",
    "    config.set(\"embedding_dim\", embedding_dim)\n",
    "    \n",
    "    embeddings = np.zeros((vocab_size + 5, embedding_dim))\n",
    "    for idx, token in vocab.get_index_to_token_vocabulary().items():\n",
    "        embeddings[idx, :] = ft_model.get_word_vector(token)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"Loading embeddings\"):\n",
    "    embedding_weights = get_fasttext_embeddings(config.ft_model_path, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim,\n",
    "                 padding_index=None, max_norm=None,\n",
    "                 weight=None, dropout=0., scale=None):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.scale = scale\n",
    "        self.padding_idx = padding_index\n",
    "        self.embed = Embedding(num_embeddings, embedding_dim,\n",
    "                               padding_index=padding_index, max_norm=max_norm,\n",
    "                               weight=weight)\n",
    "    \n",
    "    def forward(self, words):\n",
    "        weight = self.embed.weight\n",
    "        if self.dropout > 0.0 and self.training:\n",
    "            mask = self.embed.weight.data.new().resize_((weight.size(0), 1)).bernoulli_(1 - self.dropout).expand_as(weight) / (1 - self.dropout)\n",
    "            masked_embed_weight = mask * weight\n",
    "        else:\n",
    "            masked_embed_weight = weight\n",
    "        if self.scale:\n",
    "            masked_embed_weight = scale.expand_as(masked_embed_weight) * masked_embed_weight\n",
    "\n",
    "        padding_idx = self.padding_idx\n",
    "        if padding_idx is None:\n",
    "            padding_idx = -1\n",
    "\n",
    "        X = torch.nn.functional.embedding(words, masked_embed_weight,\n",
    "            padding_idx, self.embed.max_norm, self.embed.norm_type,\n",
    "            self.embed.scale_grad_by_freq, self.embed.sparse\n",
    "          )\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = CustomEmbedding(num_embeddings=config.vocab_size + 5,\n",
    "                                  embedding_dim=config.embedding_dim,\n",
    "                                  weight=torch.tensor(embedding_weights, dtype=torch.float),\n",
    "                                  dropout=config.dropoute, padding_index=0)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "encoder = BiGRUAttentionEncoder(\n",
    "    config.embedding_dim, \n",
    "    config.hidden_sz,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineModel(\n",
    "    word_embeddings, \n",
    "    encoder, \n",
    "    out_sz=config.n_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize bias according to prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU: model.cuda()\n",
    "else: model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.val_ratio > 0.0:\n",
    "    train_labels = pd.read_csv(DATA_ROOT / \"train_wo_val.csv\")[label_cols].values\n",
    "else:\n",
    "    train_labels = pd.read_csv(DATA_ROOT / \"train.csv\")[label_cols].values\n",
    "if config.testing: train_labels = train_labels[:len(train_ds), :]\n",
    "if config.use_augmented:\n",
    "    train_aux_labels = pd.read_csv(DATA_ROOT / \"train_extra.csv\")[label_cols].values\n",
    "    if config.testing: train_aux_labels = train_aux_labels[:len(train_ds), :]\n",
    "    train_labels = np.concatenate([train_labels, train_aux_labels], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = nn_util.move_to_device(batch, 0 if USE_GPU else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(list(model.word_embeddings.parameters())[0].detach().numpy()).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.isnan(x.detach().numpy()).any() for x in list(model.encoder.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.isinf(x.detach().numpy()).any() for x in list(model.encoder.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = batch[\"tokens\"]\n",
    "labels = batch\n",
    "\n",
    "mask = get_text_field_mask(tokens) == 0\n",
    "embeddings = model.word_embeddings(tokens)\n",
    "state = model.encoder(embeddings, mask)\n",
    "class_logits = model.projection(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(**batch)[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.grad for x in list(model.encoder.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model can learn small data perfectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training import trainer as _trainer\n",
    "from allennlp.training.trainer import *\n",
    "import math\n",
    "logger = _trainer.logger\n",
    "\n",
    "N_BATCHES_PER_UPDATE = config.batch_size // config.computational_batch_size\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    @gpu_mem_restore\n",
    "    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Trains one epoch and returns metrics. Copied from source\n",
    "        \"\"\"\n",
    "        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n",
    "        peak_cpu_usage = peak_memory_mb()\n",
    "        logger.info(f\"Peak CPU memory usage MB: {peak_cpu_usage}\")\n",
    "        gpu_usage = []\n",
    "        for gpu, memory in gpu_memory_mb().items():\n",
    "            gpu_usage.append((gpu, memory))\n",
    "            logger.info(f\"GPU {gpu} memory usage MB: {memory}\")\n",
    "\n",
    "        train_loss = 0.0\n",
    "        # Set the model to \"train\" mode.\n",
    "        self.model.train()\n",
    "\n",
    "        # Get tqdm for the training batches\n",
    "        train_generator = self.iterator(self.train_data,\n",
    "                                        num_epochs=1,\n",
    "                                        shuffle=self.shuffle)\n",
    "        num_training_batches = self.iterator.get_num_batches(self.train_data)\n",
    "        self._last_log = time.time()\n",
    "        last_save_time = time.time()\n",
    "\n",
    "        batches_this_epoch = 0\n",
    "        if self._batch_num_total is None:\n",
    "            self._batch_num_total = 0\n",
    "\n",
    "        if self._histogram_interval is not None:\n",
    "            histogram_parameters = set(self.model.get_parameters_for_histogram_tensorboard_logging())\n",
    "\n",
    "        logger.info(\"Training\")\n",
    "        train_generator_tqdm = Tqdm.tqdm(train_generator,\n",
    "                                         total=num_training_batches)\n",
    "        cumulative_batch_size = 0\n",
    "        for batch in train_generator_tqdm:\n",
    "            batches_this_epoch += 1\n",
    "            self._batch_num_total += 1\n",
    "            batch_num_total = self._batch_num_total\n",
    "\n",
    "            self._log_histograms_this_batch = self._histogram_interval is not None and (\n",
    "                    batch_num_total % self._histogram_interval == 0)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            ###########\n",
    "            # Custom  #\n",
    "            ###########\n",
    "            loss = self.batch_loss(batch, for_training=True)\n",
    "            if torch.isnan(loss):\n",
    "                raise ValueError(\"nan loss encountered\")\n",
    "            train_loss += loss.item()\n",
    "            # wait to update\n",
    "            if (batches_this_epoch % N_BATCHES_PER_UPDATE) != 0: continue\n",
    "            ###############\n",
    "            # End Custom  #\n",
    "            ###############\n",
    "            \n",
    "            loss.backward()\n",
    "            batch_grad_norm = self.rescale_gradients()\n",
    "            \n",
    "            \n",
    "            ###########\n",
    "            # Custom  #\n",
    "            ###########\n",
    "            # test for nans\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if torch.isnan(param.data).any() or torch.isinf(param.data).any():\n",
    "                    raise ValueError(f\"Encountered inf/nan in parameter {name}: \\n{param}\")\n",
    "            ###############\n",
    "            # End Custom  #\n",
    "            ###############\n",
    "\n",
    "            # This does nothing if batch_num_total is None or you are using an\n",
    "            # LRScheduler which doesn't update per batch.\n",
    "            if self._learning_rate_scheduler:\n",
    "                self._learning_rate_scheduler.step_batch(batch_num_total)\n",
    "\n",
    "            if self._log_histograms_this_batch:\n",
    "                # get the magnitude of parameter updates for logging\n",
    "                # We need a copy of current parameters to compute magnitude of updates,\n",
    "                # and copy them to CPU so large models won't go OOM on the GPU.\n",
    "                param_updates = {name: param.detach().cpu().clone()\n",
    "                                 for name, param in self.model.named_parameters()}\n",
    "                self.optimizer.step()\n",
    "                for name, param in self.model.named_parameters():\n",
    "                    param_updates[name].sub_(param.detach().cpu())\n",
    "                    update_norm = torch.norm(param_updates[name].view(-1, ))\n",
    "                    param_norm = torch.norm(param.view(-1, )).cpu()\n",
    "                    self._tensorboard.add_train_scalar(\"gradient_update/\" + name,\n",
    "                                                       update_norm / (param_norm + 1e-7),\n",
    "                                                       batch_num_total)\n",
    "            else:\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Update the description with the latest metrics\n",
    "            metrics = self._get_metrics(train_loss, batches_this_epoch)\n",
    "            description = self._description_from_metrics(metrics)\n",
    "\n",
    "            train_generator_tqdm.set_description(description, refresh=False)\n",
    "\n",
    "            # Log parameter values to Tensorboard\n",
    "            if batch_num_total % self._summary_interval == 0:\n",
    "                if self._should_log_parameter_statistics:\n",
    "                    self._parameter_and_gradient_statistics_to_tensorboard(batch_num_total, batch_grad_norm)\n",
    "                if self._should_log_learning_rate:\n",
    "                    self._learning_rates_to_tensorboard(batch_num_total)\n",
    "                self._tensorboard.add_train_scalar(\"loss/loss_train\", metrics[\"loss\"], batch_num_total)\n",
    "                self._metrics_to_tensorboard(batch_num_total,\n",
    "                                             {\"epoch_metrics/\" + k: v for k, v in metrics.items()})\n",
    "\n",
    "            if self._log_histograms_this_batch:\n",
    "                self._histograms_to_tensorboard(batch_num_total, histogram_parameters)\n",
    "\n",
    "            if self._log_batch_size_period:\n",
    "                cur_batch = self._get_batch_size(batch)\n",
    "                cumulative_batch_size += cur_batch\n",
    "                if (batches_this_epoch - 1) % self._log_batch_size_period == 0:\n",
    "                    average = cumulative_batch_size/batches_this_epoch\n",
    "                    logger.info(f\"current batch size: {cur_batch} mean batch size: {average}\")\n",
    "                    self._tensorboard.add_train_scalar(\"current_batch_size\", cur_batch, batch_num_total)\n",
    "                    self._tensorboard.add_train_scalar(\"mean_batch_size\", average, batch_num_total)\n",
    "\n",
    "            # Save model if needed.\n",
    "            if self._model_save_interval is not None and (\n",
    "                    time.time() - last_save_time > self._model_save_interval\n",
    "            ):\n",
    "                last_save_time = time.time()\n",
    "                self._save_checkpoint(\n",
    "                        '{0}.{1}'.format(epoch, time_to_str(int(last_save_time))), [], is_best=False\n",
    "                )\n",
    "        metrics = self._get_metrics(train_loss, batches_this_epoch, reset=True)\n",
    "        metrics['cpu_memory_MB'] = peak_cpu_usage\n",
    "        for (gpu_num, memory) in gpu_usage:\n",
    "            metrics['gpu_'+str(gpu_num)+'_memory_MB'] = memory\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test performance when input is all 0s\n",
    "- If our initialization works decently, the loss should barely/not move and accuracy should stay constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.debugging:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "    state_dict = deepcopy(model.state_dict())\n",
    "    model.test_mode()\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        iterator=iterator,\n",
    "        train_dataset=train_ds[:256],\n",
    "        cuda_device=0 if USE_GPU else -1,\n",
    "        num_epochs=5,\n",
    "    )\n",
    "    metrics = trainer.train()\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.is_test_mode = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test performance on a small batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.debugging:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "    state_dict = deepcopy(model.state_dict())\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        iterator=iterator,\n",
    "        train_dataset=train_ds[:256],\n",
    "        cuda_device=0 if USE_GPU else -1,\n",
    "        num_epochs=100,\n",
    "    )\n",
    "    metrics = trainer.train()\n",
    "    model.load_state_dict(state_dict)\n",
    "    metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_options = {\n",
    "    # TODO: Add appropriate learning rate scheduler\n",
    "    \"should_log_parameter_statistics\": not config.testing,\n",
    "    \"should_log_learning_rate\": not config.testing,\n",
    "    \"num_epochs\": config.epochs,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SER_DIR = DATA_ROOT / \"ckpts\" / RUN_ID if not config.testing else None\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    iterator=iterator,\n",
    "    train_dataset=train_ds,\n",
    "    validation_dataset=val_ds,\n",
    "    serialization_dir=SER_DIR,\n",
    "    cuda_device=0 if USE_GPU else -1,\n",
    "    **training_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training import trainer as _trainer\n",
    "from allennlp.training.trainer import *\n",
    "import math\n",
    "logger = _trainer.logger\n",
    "\n",
    "N_BATCHES_PER_UPDATE = config.batch_size // config.computational_batch_size\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    @gpu_mem_restore\n",
    "    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Trains one epoch and returns metrics. Copied from source\n",
    "        \"\"\"\n",
    "        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n",
    "        peak_cpu_usage = peak_memory_mb()\n",
    "        logger.info(f\"Peak CPU memory usage MB: {peak_cpu_usage}\")\n",
    "        gpu_usage = []\n",
    "        for gpu, memory in gpu_memory_mb().items():\n",
    "            gpu_usage.append((gpu, memory))\n",
    "            logger.info(f\"GPU {gpu} memory usage MB: {memory}\")\n",
    "\n",
    "        train_loss = 0.0\n",
    "        # Set the model to \"train\" mode.\n",
    "        self.model.train()\n",
    "\n",
    "        # Get tqdm for the training batches\n",
    "        train_generator = self.iterator(self.train_data,\n",
    "                                        num_epochs=1,\n",
    "                                        shuffle=self.shuffle)\n",
    "        num_training_batches = self.iterator.get_num_batches(self.train_data)\n",
    "        self._last_log = time.time()\n",
    "        last_save_time = time.time()\n",
    "\n",
    "        batches_this_epoch = 0\n",
    "        if self._batch_num_total is None:\n",
    "            self._batch_num_total = 0\n",
    "\n",
    "        if self._histogram_interval is not None:\n",
    "            histogram_parameters = set(self.model.get_parameters_for_histogram_tensorboard_logging())\n",
    "\n",
    "        logger.info(\"Training\")\n",
    "        train_generator_tqdm = Tqdm.tqdm(train_generator,\n",
    "                                         total=num_training_batches)\n",
    "        cumulative_batch_size = 0\n",
    "        for batch in train_generator_tqdm:\n",
    "            batches_this_epoch += 1\n",
    "            self._batch_num_total += 1\n",
    "            batch_num_total = self._batch_num_total\n",
    "\n",
    "            self._log_histograms_this_batch = self._histogram_interval is not None and (\n",
    "                    batch_num_total % self._histogram_interval == 0)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            ###########\n",
    "            # Custom  #\n",
    "            ###########\n",
    "            loss = self.batch_loss(batch, for_training=True)\n",
    "            if torch.isnan(loss):\n",
    "                raise ValueError(\"nan loss encountered\")\n",
    "            train_loss += loss.item()\n",
    "            # wait to update\n",
    "            if (batches_this_epoch % N_BATCHES_PER_UPDATE) != 0: continue\n",
    "            ###############\n",
    "            # End Custom  #\n",
    "            ###############\n",
    "            \n",
    "            loss.backward()\n",
    "            batch_grad_norm = self.rescale_gradients()\n",
    "\n",
    "            # This does nothing if batch_num_total is None or you are using an\n",
    "            # LRScheduler which doesn't update per batch.\n",
    "            if self._learning_rate_scheduler:\n",
    "                self._learning_rate_scheduler.step_batch(batch_num_total)\n",
    "\n",
    "            if self._log_histograms_this_batch:\n",
    "                # get the magnitude of parameter updates for logging\n",
    "                # We need a copy of current parameters to compute magnitude of updates,\n",
    "                # and copy them to CPU so large models won't go OOM on the GPU.\n",
    "                param_updates = {name: param.detach().cpu().clone()\n",
    "                                 for name, param in self.model.named_parameters()}\n",
    "                self.optimizer.step()\n",
    "                for name, param in self.model.named_parameters():\n",
    "                    param_updates[name].sub_(param.detach().cpu())\n",
    "                    update_norm = torch.norm(param_updates[name].view(-1, ))\n",
    "                    param_norm = torch.norm(param.view(-1, )).cpu()\n",
    "                    self._tensorboard.add_train_scalar(\"gradient_update/\" + name,\n",
    "                                                       update_norm / (param_norm + 1e-7),\n",
    "                                                       batch_num_total)\n",
    "            else:\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Update the description with the latest metrics\n",
    "            metrics = self._get_metrics(train_loss, batches_this_epoch)\n",
    "            description = self._description_from_metrics(metrics)\n",
    "\n",
    "            train_generator_tqdm.set_description(description, refresh=False)\n",
    "\n",
    "            # Log parameter values to Tensorboard\n",
    "            if batch_num_total % self._summary_interval == 0:\n",
    "                if self._should_log_parameter_statistics:\n",
    "                    self._parameter_and_gradient_statistics_to_tensorboard(batch_num_total, batch_grad_norm)\n",
    "                if self._should_log_learning_rate:\n",
    "                    self._learning_rates_to_tensorboard(batch_num_total)\n",
    "                self._tensorboard.add_train_scalar(\"loss/loss_train\", metrics[\"loss\"], batch_num_total)\n",
    "                self._metrics_to_tensorboard(batch_num_total,\n",
    "                                             {\"epoch_metrics/\" + k: v for k, v in metrics.items()})\n",
    "\n",
    "            if self._log_histograms_this_batch:\n",
    "                self._histograms_to_tensorboard(batch_num_total, histogram_parameters)\n",
    "\n",
    "            if self._log_batch_size_period:\n",
    "                cur_batch = self._get_batch_size(batch)\n",
    "                cumulative_batch_size += cur_batch\n",
    "                if (batches_this_epoch - 1) % self._log_batch_size_period == 0:\n",
    "                    average = cumulative_batch_size/batches_this_epoch\n",
    "                    logger.info(f\"current batch size: {cur_batch} mean batch size: {average}\")\n",
    "                    self._tensorboard.add_train_scalar(\"current_batch_size\", cur_batch, batch_num_total)\n",
    "                    self._tensorboard.add_train_scalar(\"mean_batch_size\", average, batch_num_total)\n",
    "\n",
    "            # Save model if needed.\n",
    "            if self._model_save_interval is not None and (\n",
    "                    time.time() - last_save_time > self._model_save_interval\n",
    "            ):\n",
    "                last_save_time = time.time()\n",
    "                self._save_checkpoint(\n",
    "                        '{0}.{1}'.format(epoch, time_to_str(int(last_save_time))), [], is_best=False\n",
    "                )\n",
    "        metrics = self._get_metrics(train_loss, batches_this_epoch, reset=True)\n",
    "        metrics['cpu_memory_MB'] = peak_cpu_usage\n",
    "        for (gpu_num, memory) in gpu_usage:\n",
    "            metrics['gpu_'+str(gpu_num)+'_memory_MB'] = memory\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from collections import defaultdict\n",
    "\n",
    "def dict_append(d: Dict[str, List], upd: Dict[str, Any]) -> Dict[str, List]:\n",
    "    for k, v in upd.items(): d[k].append(v)\n",
    "\n",
    "def tonp(tsr): return tsr.detach().cpu().numpy()\n",
    "        \n",
    "class Predictor:\n",
    "    def __init__(self, model: Model, iterator: DataIterator,\n",
    "                 cuda_device: int=-1) -> None:\n",
    "        self.model = model\n",
    "        self.iterator = iterator\n",
    "        self.cuda_device = cuda_device\n",
    "        \n",
    "    def _extract_data(self, batch) -> Dict[str, np.ndarray]:\n",
    "        out_dict = self.model(**batch)\n",
    "        lens = tonp(get_text_field_mask(batch[\"tokens\"]).sum(1))\n",
    "        return {\n",
    "                \"preds\": expit(tonp(out_dict[\"class_logits\"])),\n",
    "                \"oov_ratio\": tonp((batch[\"tokens\"][\"tokens\"] == 1).sum(1)) / lens,\n",
    "                \"lens\": lens,\n",
    "               }\n",
    "        \n",
    "    def _postprocess(self, predictions: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        return {k: np.concatenate(v, axis=0) for k, v in predictions.items()}\n",
    "    \n",
    "    @gpu_mem_restore\n",
    "    def predict(self, ds: Iterable[Instance]) -> np.ndarray:\n",
    "        pred_generator = self.iterator(ds, num_epochs=1, shuffle=False)\n",
    "        self.model.eval()\n",
    "        pred_generator_tqdm = Tqdm.tqdm(pred_generator,\n",
    "                                        total=self.iterator.get_num_batches(ds))\n",
    "        preds = defaultdict(list)\n",
    "        with torch.no_grad():\n",
    "            for batch in pred_generator_tqdm:\n",
    "                batch = nn_util.move_to_device(batch, self.cuda_device)\n",
    "                dict_append(preds, self._extract_data(batch))\n",
    "        return self._postprocess(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BasicIterator\n",
    "seq_iterator = BasicIterator(batch_size=64)\n",
    "seq_iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(model, seq_iterator)\n",
    "train_meta = predictor.predict(train_ds) \n",
    "train_preds = train_meta.pop(\"preds\")\n",
    "test_meta = predictor.predict(test_ds)\n",
    "test_preds = test_meta.pop(\"preds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = pd.read_csv(DATA_ROOT / \"test_proced.csv\")[label_cols].values\n",
    "if config.testing:\n",
    "    test_labels = test_labels[:len(test_ds), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, thres=0.5):\n",
    "        if isinstance(thres, float):\n",
    "            self.thres = np.ones(len(label_cols)) * thres\n",
    "        else:\n",
    "            self.thres = thres\n",
    "    \n",
    "    def _to_metric_dict(self, t: np.ndarray, y: np.ndarray, thres: float) -> Dict:\n",
    "        tn, fp, fn, tp = confusion_matrix(t, y >= thres).ravel()\n",
    "        return {\"auc\": roc_auc_score(t, y),\n",
    "                \"f1\": f1_score(t, y >= thres),\n",
    "                \"acc\": accuracy_score(t, y >= thres),\n",
    "                \"tnr\": tn / len(t), \"fpr\": fp / len(t),\n",
    "                \"fnr\": fn / len(t), \"tpr\": tp / len(t),\n",
    "                \"precision\": tp / (tp + fp), \"recall\": tp / (tp + fn),\n",
    "        }\n",
    "\n",
    "    def _stats_per_quadrant(self, tgt, preds, metadata: Dict[str, np.ndarray]):\n",
    "        out_data = {}\n",
    "        for i, lbl in enumerate(label_cols):\n",
    "            # get indicies of each quadrant`\n",
    "            preds_bin = preds[:, i] >= self.thres[i]\n",
    "            quads = {\n",
    "                \"tp\": np.where((tgt[:, i] == 1) & preds_bin)[0],\n",
    "                \"fp\": np.where((tgt[:, i] == 0) & preds_bin)[0],\n",
    "                \"tn\": np.where((tgt[:, i] == 0) & ~preds_bin)[0],\n",
    "                \"fn\": np.where((tgt[:, i] == 1) & ~preds_bin)[0],\n",
    "            }\n",
    "            # get stats for metadata\n",
    "            out_data[lbl] = {}\n",
    "            for q, qidxs in quads.items():\n",
    "                out_data[lbl][q] = {}\n",
    "                for k, full_data in metadata.items():\n",
    "                    data = full_data[qidxs]\n",
    "                    if len(data) > 0:\n",
    "                        out_data[lbl][q][f\"{k}_mean\"] = data.mean()\n",
    "                        out_data[lbl][q][f\"{k}_std\"] = data.std()\n",
    "                        out_data[lbl][q][f\"{k}_min\"] = data.min()\n",
    "                        out_data[lbl][q][f\"{k}_max\"] = data.max()\n",
    "                    else:\n",
    "                        out_data[lbl][q][f\"{k}_mean\"] = np.nan\n",
    "                        out_data[lbl][q][f\"{k}_std\"] = np.nan\n",
    "                        out_data[lbl][q][f\"{k}_min\"] = np.nan\n",
    "                        out_data[lbl][q][f\"{k}_max\"] = np.nan\n",
    "        return out_data\n",
    "    \n",
    "    @gpu_mem_restore\n",
    "    def evaluate(self, tgt: np.ndarray, preds: np.ndarray,\n",
    "                 trn_tgt: np.ndarray, trn_preds: np.ndarray,\n",
    "                 metadata: Dict[str, np.ndarray]={}) -> Dict:\n",
    "        \"\"\"\n",
    "        Metadata: Data about the inputs (e.g. length, OOV ratio)\n",
    "        \"\"\"\n",
    "        train_label_metrics = {}\n",
    "        label_metrics = {}\n",
    "                \n",
    "        # get per-label stats\n",
    "        for i, lbl in enumerate(label_cols):\n",
    "            train_label_metrics[lbl] = self._to_metric_dict(trn_tgt[:, i],\n",
    "                                                            trn_preds[:, i],\n",
    "                                                            self.thres[i])\n",
    "            label_metrics[lbl] = self._to_metric_dict(tgt[:, i], preds[:, i],\n",
    "                                                      self.thres[i])\n",
    "            print(f\"========{lbl}=========\")\n",
    "            print(label_metrics[lbl])\n",
    "        \n",
    "        # get global stats\n",
    "        label_metrics[\"global\"] = {}\n",
    "        for mtrc in label_metrics[\"toxic\"].keys():\n",
    "            label_metrics[\"global\"][mtrc] = \\\n",
    "                np.mean([label_metrics[col][mtrc] for col in label_cols])\n",
    "            \n",
    "        # get per-label-quadrant stats\n",
    "        quad_stats = self._stats_per_quadrant(tgt, preds, metadata)\n",
    "        if len(quad_stats) > 0:\n",
    "            for c in label_cols:\n",
    "                label_metrics[c][\"quad_stats\"] = quad_stats[c]\n",
    "\n",
    "        metrics = {\n",
    "            \"train\": train_label_metrics,\n",
    "            \"test\": label_metrics,\n",
    "        }\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute best threshold based on training data\n",
    "thres = np.zeros(len(label_cols))\n",
    "for i, col in enumerate(label_cols):\n",
    "    best_score = -1\n",
    "    best_thres = -1\n",
    "    for x in np.linspace(0, 1.0, num=99):\n",
    "        scr = f1_score(train_labels[:, i], train_preds[:, i] > x)\n",
    "        if scr > best_score:\n",
    "            best_thres = x\n",
    "            best_score = scr\n",
    "    thres[i] = best_thres\n",
    "thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(thres=thres)\n",
    "label_metrics = evaluator.evaluate(\n",
    "    test_labels, test_preds,\n",
    "    train_labels, train_preds,\n",
    "    metadata=test_meta,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record results and save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ[\"IS_COLAB\"] == \"True\":\n",
    "    pass\n",
    "else:\n",
    "    import sys\n",
    "    sys.path.append(\"../lib\")\n",
    "    import record_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "if not config.testing:\n",
    "    experiment_log = dict(config)\n",
    "    tz = timezone('EST')\n",
    "    experiment_log[\"execution_date\"] = datetime.now(tz).strftime(\"%Y-%m-%d %H:%M %Z\")\n",
    "    experiment_log.update(metrics)\n",
    "    experiment_log.update(label_metrics)\n",
    "    record(experiment_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output tensorboard outputs and training logs to s3\n",
    "\n",
    "(Remove weights since they take up too much space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm {SER_DIR / \"*.th\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {SER_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync {SER_DIR} s3://nnfornlp/ckpts/{RUN_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
