{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "dependencies"
    ]
   },
   "outputs": [],
   "source": [
    "depends_on = [\n",
    "    \"preproc_jigsaw\",\n",
    "    \"jigsaw_create_augmented_data\",\n",
    "    \"create_fasttext_matrix\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for seamlessly running on colab\n",
    "import os\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    os.environ[\"IS_COLAB\"] = \"True\"\n",
    "except ImportError:\n",
    "    os.environ[\"IS_COLAB\"] = \"False\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"SLACK_TOKEN\" not in os.environ:\n",
    "    os.environ[\"SLACK_TOKEN\"] = \"\" # TODO: insert here for slack notifications\n",
    "if \"SLACK_ID\" not in os.environ:\n",
    "    os.environ[\"SLACK_ID\"] = \"\" # TODO: insert here for slack notifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ \"$IS_COLAB\" = \"True\" ]; then\n",
    "    pip install git+https://github.com/facebookresearch/fastText.git\n",
    "    pip install torch\n",
    "    pip install torchvision\n",
    "    pip install --upgrade git+https://github.com/keitakurita/allennlp.git@develop\n",
    "    pip install dnspython\n",
    "    pip install jupyter_slack\n",
    "    pip install git+https://github.com/keitakurita/Better_LSTM_PyTorch.git\n",
    "    pip install nmslib\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import *\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from overrides import overrides\n",
    "import warnings\n",
    "\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.nn import util as nn_util\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "    \n",
    "import functools\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "def get_ref_free_exc_info():\n",
    "    \"Free traceback from references to locals/globals to avoid circular reference leading to gc.collect() unable to reclaim memory\"\n",
    "    type, val, tb = sys.exc_info()\n",
    "    traceback.clear_frames(tb)\n",
    "    return (type, val, tb)\n",
    "\n",
    "def gpu_mem_restore(func):\n",
    "    \"Reclaim GPU RAM if CUDA out of memory happened, or execution was interrupted\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except:\n",
    "            type, val, tb = get_ref_free_exc_info() # must!\n",
    "            raise type(val).with_traceback(tb) from None\n",
    "    return wrapper\n",
    "\n",
    "def ifnone(a: Any, alt: Any): return alt if a is None else a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TypeVar(\"T\")\n",
    "TensorDict = Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]  # pylint: disable=invalid-name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# for papermill\n",
    "testing = False # set to False when running experiments\n",
    "debugging = False\n",
    "seed = 1\n",
    "use_bt = False\n",
    "computational_batch_size = 128\n",
    "batch_size = 128\n",
    "lr = 4e-3\n",
    "lr_schedule = \"slanted_triangular\"\n",
    "epochs = 4 if not testing else 1\n",
    "hidden_sz = 128\n",
    "dataset = \"jigsaw\"\n",
    "n_classes = 6\n",
    "max_seq_len = 512\n",
    "download_data = False\n",
    "ft_model_path = \"../data/jigsaw/ft_basic_toks.txt\"\n",
    "max_vocab_size = 400000\n",
    "dropouti = 0.2\n",
    "dropoutw = 0.0\n",
    "dropoute = 0.1\n",
    "dropoute_max = None\n",
    "dropoutr = 0.3 # TODO: Implement\n",
    "val_ratio = 0.0\n",
    "use_mbern_loss = False\n",
    "use_focal_loss = False\n",
    "label_smoothing_eps = 1.\n",
    "focal_loss_alpha = 1.\n",
    "focal_loss_gamma = 2.\n",
    "use_augmented = False\n",
    "freeze_embeddings = True\n",
    "mixup_ratio = 0.0\n",
    "discrete_mixup_ratio = 0.0\n",
    "attention_bias = True\n",
    "use_attention_aux = False\n",
    "weight_decay = 0.\n",
    "bias_init = True\n",
    "neg_splits = 1\n",
    "num_layers = 2\n",
    "rnn_type = \"lstm\"\n",
    "rnn_residual = False\n",
    "pooling_type = \"augmented_multipool\" # attention or multipool or augmented_multipool\n",
    "model_type = \"standard\"\n",
    "cache_elmo_embeddings = True\n",
    "use_word_level_features = False\n",
    "use_sentence_level_features = False\n",
    "bucket = True\n",
    "compute_thres_on_test = False\n",
    "find_lr = False\n",
    "permute_sentences = False\n",
    "run_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Can we make this play better with papermill?\n",
    "config = Config(\n",
    "    testing=testing,\n",
    "    debugging=debugging,\n",
    "    seed=seed,\n",
    "    use_bt=use_bt,\n",
    "    computational_batch_size=computational_batch_size,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    lr_schedule=lr_schedule,\n",
    "    epochs=epochs,\n",
    "    hidden_sz=hidden_sz,\n",
    "    dataset=dataset,\n",
    "    n_classes=n_classes,\n",
    "    max_seq_len=max_seq_len, # necessary to limit memory usage\n",
    "    ft_model_path=ft_model_path,\n",
    "    max_vocab_size=max_vocab_size,\n",
    "    dropouti=dropouti,\n",
    "    dropoutw=dropoutw,\n",
    "    dropoute=dropoute,\n",
    "    dropoute_max=dropoute_max,\n",
    "    dropoutr=dropoutr,\n",
    "    val_ratio=val_ratio,\n",
    "    use_mbern_loss=use_mbern_loss,\n",
    "    use_focal_loss=use_focal_loss,\n",
    "    label_smoothing_eps=label_smoothing_eps,\n",
    "    focal_loss_alpha=focal_loss_alpha,\n",
    "    focal_loss_gamma=focal_loss_gamma,\n",
    "    use_augmented=use_augmented,\n",
    "    freeze_embeddings=freeze_embeddings,\n",
    "    attention_bias=attention_bias,\n",
    "    use_attention_aux=use_attention_aux,\n",
    "    weight_decay=weight_decay,\n",
    "    bias_init=bias_init,\n",
    "    neg_splits=neg_splits,\n",
    "    num_layers=num_layers,\n",
    "    rnn_type=rnn_type,\n",
    "    rnn_residual=rnn_residual,\n",
    "    pooling_type=pooling_type,\n",
    "    model_type=model_type,\n",
    "    cache_elmo_embeddings=cache_elmo_embeddings,\n",
    "    use_word_level_features=use_word_level_features,\n",
    "    use_sentence_level_features=use_sentence_level_features,\n",
    "    mixup_ratio=mixup_ratio,\n",
    "    discrete_mixup_ratio=discrete_mixup_ratio,\n",
    "    bucket=bucket,\n",
    "    compute_thres_on_test=compute_thres_on_test,\n",
    "    permute_sentences=permute_sentences,\n",
    "    find_lr=find_lr,\n",
    "    run_id=run_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common.checks import ConfigurationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.model_type != \"standard\" and \"bert\" not in config.model_type and \"elmo\" not in config.model_type:\n",
    "    raise ConfigurationError(f\"Invalid model type {config.model_type} given\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.mixup_ratio > 0. and config.bucket:\n",
    "    raise ConfigurationError(f\"Mixup should be combined with complete random shuffling of the input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"bert\" in config.model_type and config.computational_batch_size > 16:\n",
    "    raise ConfigurationError(\"Batch size too large for BERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.permute_sentences and (config.use_word_level_features or config.use_sentence_level_features):\n",
    "    raise ConfigurationError(\"Token shuffling does not yet transfer to wlf and slf.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "RUN_ID = config.run_id if config.run_id is not None else now.strftime(\"%m_%d_%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ[\"IS_COLAB\"] != \"True\":\n",
    "    DATA_ROOT = Path(\"../data\") / config.dataset\n",
    "else:\n",
    "    DATA_ROOT = Path(\"./gdrive/My Drive/Colab_Workspace/Colab Notebooks/data\") / config.dataset\n",
    "    config.ft_model_path = str(DATA_ROOT / \"ft_basic_toks.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {DATA_ROOT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "if download_data:\n",
    "    \n",
    "    if config.val_ratio > 0.0:\n",
    "        #fnames = [\"train_wo_val.csv\", \"test_proced.csv\", \"val.csv\", \"ft_model.txt\"]\n",
    "        raise ConfigurationError(f\"Validation dataset not processed.\")\n",
    "    \n",
    "    else:\n",
    "        fnames = [\"train_basic.jsonl\", \"test_basic.jsonl\", \"ft_basic_toks.txt\"]\n",
    "    \n",
    "    if config.use_augmented or config.discrete_mixup_ratio > 0.0: \n",
    "        #fnames.append(\"train_extra.csv\")\n",
    "        raise ConfigurationError(f\"Augmented datasets not processed.\")\n",
    "    \n",
    "    for fname in fnames:\n",
    "        if not (DATA_ROOT / fname).exists():\n",
    "            print(subprocess.Popen([f\"aws s3 cp s3://nnfornlp/raw_data/jigsaw/{fname} {str(DATA_ROOT)}\"],\n",
    "                                   shell=True, stdout=subprocess.PIPE).stdout.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_oov_map.bin\t\t       test.csv\r\n",
      "ckpts\t\t\t\t       test_ds.bin\r\n",
      "data_ft_vocab\t\t\t       test_labels.csv\r\n",
      "data_vocab.bin\t\t\t       test_proced.csv\r\n",
      "debug.csv\t\t\t       test_proced_no_oov1.csv\r\n",
      "example.csv\t\t\t       tmp_model.pth\r\n",
      "ft_basic_toks.npy\t\t       train_augmented.jsonl\r\n",
      "ft_basic_toks.txt\t\t       train_basic.jsonl\r\n",
      "ft_basic_toks.txt.md\t\t       train.csv\r\n",
      "ft_basic_toks.txt.sm\t\t       train_ds.bin\r\n",
      "ft_compiled.npy\t\t\t       train_extra.csv\r\n",
      "ft_model_bert_basic_tok.npy\t       train_extra_interpolated.csv\r\n",
      "ft_model_bert_basic_tok.txt\t       train_no_oov1.csv\r\n",
      "ft_model_no_bt.txt\t\t       train_with_bt.csv\r\n",
      "ft_model.txt\t\t\t       train_wo_val.csv\r\n",
      "sample_pred_bert_oov.csv\t       val.csv\r\n",
      "sample_pred_bert_oov_fasttext_knn.csv  val_ds.bin\r\n",
      "sample_pred.csv\t\t\t       val_no_oov1.csv\r\n",
      "sample_submission.csv\t\t       voc_basic_toks\r\n",
      "test_augmented.jsonl\t\t       wiki.en.bin\r\n",
      "test_basic.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!ls {DATA_ROOT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seed manually to replicate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8445e68bf0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.dataset_readers import DatasetReader, StanfordSentimentTreeBankDatasetReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ[\"IS_COLAB\"] != \"True\":\n",
    "    import sys\n",
    "    sys.path.append(\"../preprocessing\")\n",
    "    from data_loader import *\n",
    "    from basic_processing import word_level_features, sentence_level_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = [\"toxic\", \"severe_toxic\", \"obscene\",\n",
    "              \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "from enum import IntEnum\n",
    "ColIdx = IntEnum('ColIdx', [(x.upper(), i) for i, x in enumerate(label_cols)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare token handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from functools import wraps\n",
    "\n",
    "def maybeshuffle(_tokenize):\n",
    "    def func(*args, **kwargs):\n",
    "        arr = _tokenize(*args, **kwargs)\n",
    "        if config.permute_sentences:\n",
    "            random.shuffle(arr)\n",
    "        return arr\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.data.token_indexers import WordpieceIndexer, SingleIdTokenIndexer\n",
    "\n",
    "\n",
    "if config.model_type == \"standard\" or (\"elmo\" in config.model_type and config.cache_elmo_embeddings):\n",
    "    \n",
    "    from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "    token_indexer = SingleIdTokenIndexer(\n",
    "        lowercase_tokens=True,\n",
    "    )\n",
    "    \n",
    "    @maybeshuffle\n",
    "    def token_extender(x: List[str]):\n",
    "        return x\n",
    "    \n",
    "elif \"elmo\" in config.model_type:\n",
    "    \n",
    "    from allennlp.data.token_indexers.elmo_indexer import ELMoTokenCharactersIndexer\n",
    "    token_indexer = ELMoTokenCharactersIndexer()\n",
    "    \n",
    "    @maybeshuffle\n",
    "    def token_extender(x: List[str]):\n",
    "        # add start and end of sentence tokens\n",
    "        return [\"<S>\"] + x + [\"</S>\"]\n",
    "    \n",
    "    if config.use_word_level_features or config.use_sentence_level_features:\n",
    "        raise ConfigurationError(\"Elmo tokenization does not yet transfer to wlf and slf.\")\n",
    "    \n",
    "elif \"bert\" in config.model_type:\n",
    "    \n",
    "    #def flatten(x: List[List[T]]) -> List[T]:\n",
    "    #    return [item for sublist in x for item in sublist]\n",
    "\n",
    "    from allennlp.data.token_indexers import PretrainedBertIndexer\n",
    "    token_indexer = PretrainedBertIndexer(\n",
    "        pretrained_model=config.model_type,\n",
    "        max_pieces=config.max_seq_len,\n",
    "        do_lowercase=True,\n",
    "     )\n",
    "    \n",
    "    @maybeshuffle\n",
    "    def token_extender(x: List[str]):\n",
    "        return x\n",
    "\n",
    "    if config.use_word_level_features or config.use_sentence_level_features:\n",
    "        raise ConfigurationError(\"BERT tokenization does not yet transfer to wlf and slf.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer at 0x7f83e537bbe0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = JigsawDatasetJSONLReader(\n",
    "    token_extender = token_extender,\n",
    "    oov_token_swapper = OOVTokenSwapper(),\n",
    "    token_indexers={\"tokens\": token_indexer},\n",
    "    testing = config.testing\n",
    "   )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "159571it [00:24, 6423.81it/s]\n",
      "63978it [00:10, 6108.57it/s]\n"
     ]
    }
   ],
   "source": [
    "if config.val_ratio > 0.0:\n",
    "    \n",
    "    raise ConfigurationError(\"Validation dataset not processed.\")\n",
    "    \n",
    "    #train_ds, val_ds, test_ds = (reader.read(DATA_ROOT / fname) for fname in [\"train_wo_val.csv\",\n",
    "    #                                                                          \"val.csv\",\n",
    "    #                                                                          \"test_proced.csv\"])\n",
    "else:\n",
    "    \n",
    "    if config.use_bt:\n",
    "        raise ConfigurationError(\"BT dataset not processed.\")\n",
    "    \n",
    "    else:\n",
    "        train_ds, test_ds = (reader.read(DATA_ROOT / fname) for fname in [\"train_basic.jsonl\",\"test_basic.jsonl\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.use_augmented or config.discrete_mixup_ratio > 0.0:\n",
    "    raise ConfigurationError(\"Augmented dataset not processed.\")\n",
    "    # TODO: Handle data leak for validation!\n",
    "    #train_aug_ds = reader.read(DATA_ROOT / \"train_extra.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Explanation',\n",
       "  'Why',\n",
       "  'the',\n",
       "  'edits',\n",
       "  'made',\n",
       "  'under',\n",
       "  'my',\n",
       "  'username',\n",
       "  'Hardcore',\n",
       "  'Metallica',\n",
       "  'Fan',\n",
       "  'were',\n",
       "  'reverted',\n",
       "  '?',\n",
       "  'They',\n",
       "  'weren',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'vandalisms',\n",
       "  ',',\n",
       "  'just',\n",
       "  'closure',\n",
       "  'on',\n",
       "  'some',\n",
       "  'Gas',\n",
       "  'after',\n",
       "  'I',\n",
       "  'voted',\n",
       "  'at',\n",
       "  'New',\n",
       "  'York',\n",
       "  'Dolls',\n",
       "  'FAC',\n",
       "  '.',\n",
       "  'And',\n",
       "  'please',\n",
       "  'don',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'remove',\n",
       "  'the',\n",
       "  'template',\n",
       "  'from',\n",
       "  'the',\n",
       "  'talk',\n",
       "  'page',\n",
       "  'since',\n",
       "  'I',\n",
       "  \"'\",\n",
       "  'm',\n",
       "  'retired',\n",
       "  'now.89.205.38.27'],\n",
       " '_token_indexers': {'tokens': <allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer at 0x7f83e537bbe0>},\n",
       " '_indexed_tokens': None,\n",
       " '_indexer_name_to_indexed_token': None}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_ds[0].fields[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.val_ratio > 0.0:\n",
    "    \n",
    "    train_labels = pd.read_csv(DATA_ROOT / \"train_wo_val.csv\")[label_cols].values\n",
    "    \n",
    "else:\n",
    "    train_labels = pd.read_csv(DATA_ROOT / \"train.csv\")[label_cols].values\n",
    "    \n",
    "if config.testing:\n",
    "    \n",
    "    train_labels = train_labels[:len(train_ds), :]\n",
    "    \n",
    "if config.use_augmented:\n",
    "    \n",
    "    train_aux_labels = pd.read_csv(DATA_ROOT / \"train_extra.csv\")[label_cols].values\n",
    "    \n",
    "    if config.testing: train_aux_labels = train_aux_labels[:len(train_ds), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223549/223549 [00:18<00:00, 12066.16it/s]\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "if \"bert\" in config.model_type:\n",
    "    vocab = Vocabulary()\n",
    "    \n",
    "elif config.model_type == \"standard\" or config.cache_elmo_embeddings:\n",
    "    \n",
    "    full_ds = train_ds + test_ds\n",
    "    \n",
    "    if config.val_ratio > 0.0: \n",
    "        full_ds = full_ds + val_ds\n",
    "        \n",
    "    vocab = Vocabulary.from_instances(full_ds, max_vocab_size=config.max_vocab_size)\n",
    "    \n",
    "else:\n",
    "    vocab = Vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BucketIterator, DataIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class Sampler:\n",
    "    def sample(self, ds: List[Instance]) -> List[Instance]:\n",
    "        return ds\n",
    "    def sample_size(self, ds: List[Instance]) -> int:\n",
    "        return len(ds)\n",
    "\n",
    "class BiasedSampler(Sampler):\n",
    "    def __init__(self, mask: np.ndarray, n_splits: int):\n",
    "        self.mask = mask\n",
    "        self.n_splits = n_splits\n",
    "        self.pos = np.where(self.mask)[0]\n",
    "        self.neg = np.where(~self.mask)[0]\n",
    "        self._n_splits_iterated = 0\n",
    "        \n",
    "    def sample(self, ds: List[Instance]) -> List[Instance]:\n",
    "        if self._n_splits_iterated % self.n_splits == 0:\n",
    "            self.folds = KFold(n_splits=self.n_splits).split(self.neg)\n",
    "        _, neg_idxs = next(self.folds)\n",
    "        \n",
    "        p = np.random.permutation(len(self.pos) + len(neg_idxs))\n",
    "        smpl = np.r_[self.pos, self.neg[neg_idxs]][p]\n",
    "        \n",
    "        self._n_splits_iterated += 1\n",
    "        return [ds[i] for i in smpl]\n",
    "    \n",
    "    def sample_size(self, ds: List[Instance]) -> int:\n",
    "        \"\"\"Returns number of samples that would be returned upon a call to sample\"\"\"\n",
    "        # there might be a slight difference depending on the epoch, but it's okay\n",
    "        return len(self.pos) + len(self.neg) // self.n_splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoredSampler:\n",
    "    def __init__(self, mask: np.ndarray, ratio: float):\n",
    "        self.mask = mask\n",
    "        self.ratio = ratio\n",
    "        self.n_samples = int(len(self.tgt) * self.ratio)\n",
    "        self.score = mask.astype(\"int\")\n",
    "    \n",
    "    def set_score(self, score: np.ndarray):\n",
    "        assert len(score) == len(self.tgt)\n",
    "        self.score = score\n",
    "    \n",
    "    def sample(self, ds: List[Instance]):\n",
    "        \"\"\"Sample top n targets sorted by score descending\"\"\"\n",
    "        smpl = np.arange(len(self.mask))[np.argsort(-self.score)][:self.n_samples]\n",
    "        return [ds[i] for i in smpl]\n",
    "    \n",
    "    def sample_size(self, ds: List[Instance]) -> int:\n",
    "        \"\"\"Returns number of samples that would be returned upon a call to sample\"\"\"\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.common.util import lazy_groups_of, add_noise_to_dict_values\n",
    "from allennlp.data.dataset import Batch\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.iterators import DataIterator, BucketIterator, BasicIterator\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "class SamplingIteratorMixin:\n",
    "    \"\"\"Uses Python's MRO to add sampling.\n",
    "    DANGER: This is pushing the limits of OOP and might lead to bugs\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, sampler: Sampler=None, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.sampler = ifnone(sampler, Sampler())\n",
    "        \n",
    "    def get_num_batches(self, instances: List[Instance]):\n",
    "        return math.ceil(self.sampler.sample_size(instances) / self._batch_size)\n",
    "\n",
    "    def _create_batches(self, instances: Iterable[Instance], shuffle: bool) -> Iterable[Batch]:\n",
    "        yield from super()._create_batches(self.sampler.sample(instances), shuffle)\n",
    "\n",
    "# Caution: Inheritance must be in order: SamplingIteratorMixin, BucketIterator\n",
    "class CustomBucketIterator(SamplingIteratorMixin, BucketIterator): pass\n",
    "class CustomBasicIterator(SamplingIteratorMixin, BasicIterator): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Allow for customization\n",
    "if config.neg_splits > 1:\n",
    "    if config.use_augmented:\n",
    "        full_trn_labels = np.concatenate([train_labels, train_aux_labels], axis=0)\n",
    "    else:\n",
    "        full_trn_labels = train_labels\n",
    "    sampler = BiasedSampler(full_trn_labels.sum(1) >= 1,\n",
    "                            config.neg_splits)\n",
    "else:\n",
    "    sampler = Sampler()\n",
    "if config.bucket:\n",
    "    iterator = CustomBucketIterator(\n",
    "        batch_size=config.batch_size, \n",
    "        biggest_batch_first=config.testing,\n",
    "        sorting_keys=[(\"tokens\", \"num_tokens\")],\n",
    "        max_instances_in_memory=config.batch_size * 2,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "else:\n",
    "    # CAUTION: BasicIterator shuffles the dataset internally\n",
    "    # TODO: Either fix this bug or ensure evalutation can handle shuffle\n",
    "    # in the dataset order\n",
    "    iterator = CustomBasicIterator(\n",
    "        batch_size=config.batch_size, \n",
    "        max_instances_in_memory=config.batch_size * 2,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(iterator(train_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': {'tokens': tensor([[    5,   225,     3,  ...,     0,     0,     0],\n",
       "          [   52,   263,    25,  ...,     0,     0,     0],\n",
       "          [  303,    50,    14,  ...,     0,     0,     0],\n",
       "          ...,\n",
       "          [    5,    31,    31,  ...,     0,     0,     0],\n",
       "          [    5, 39905,  2774,  ...,     0,     0,     0],\n",
       "          [ 6453,     4,   338,  ...,    73,  6262,     2]])},\n",
       " 'word_level_features': tensor([[[0.0000, 1.0000, 1.0000],\n",
       "          [0.2000, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.2000, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.2000, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.0000, 1.0000, 1.0000],\n",
       "          [0.0000, 1.0000, 1.0000],\n",
       "          [0.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 1.0000, 1.0000],\n",
       "          [0.1111, 0.0000, 1.0000],\n",
       "          [0.1429, 0.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.3333, 0.0000, 1.0000],\n",
       "          [0.0000, 1.0000, 1.0000],\n",
       "          [0.2000, 0.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          [0.0000, 1.0000, 1.0000]]]),\n",
       " 'sentence_level_features': tensor([], size=(128, 0)),\n",
       " 'label': tensor([[0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.]]),\n",
       " 'id': ['002d6c9d9f85e81f',\n",
       "  '0029541a38c523a0',\n",
       "  '00537730daf8c5f1',\n",
       "  '005e2ae8f864f76c',\n",
       "  '004f981460421bdf',\n",
       "  '00280c0d0652b366',\n",
       "  '0029b87aa9c7dc4a',\n",
       "  '004f5608984d99f1',\n",
       "  '0000997932d777bf',\n",
       "  '0061b075244dd234',\n",
       "  '008f22e7b58e559b',\n",
       "  '00070ef96486d6f9',\n",
       "  '007bbfa4da2bc32d',\n",
       "  '002f0e29c60807b1',\n",
       "  '00733f0a4a58cf42',\n",
       "  '00349c6325526c11',\n",
       "  '000113f07ec002fd',\n",
       "  '0030614cfd96d9d1',\n",
       "  '000b08c464718505',\n",
       "  '007bc29766a43e3c',\n",
       "  '003dbd1b9b354c1f',\n",
       "  '0057b7710cb5ebb2',\n",
       "  '002a6beca33307b3',\n",
       "  '0052a7e684beeb1a',\n",
       "  '0028d62e8a5629aa',\n",
       "  '004f6dbe69f3545d',\n",
       "  '004de318396bbf8b',\n",
       "  '006f2c1459f3b6b1',\n",
       "  '009b3b15f1ada72f',\n",
       "  '0022cf8467ebc9fd',\n",
       "  '0053bab79133c0fc',\n",
       "  '007571394afafcb5',\n",
       "  '000f35deef84dc4a',\n",
       "  '0015f4aa35ebe9b5',\n",
       "  '00a20f187531df59',\n",
       "  '008198c5a9d85a8e',\n",
       "  '006120d209a4a46c',\n",
       "  '0082b4b42b3f07a1',\n",
       "  '00744c2f77391702',\n",
       "  '0063dd8f202a698a',\n",
       "  '0005c987bdfc9d4b',\n",
       "  '001956c382006abd',\n",
       "  '005b214511a69b4b',\n",
       "  '0074b307c2d9a100',\n",
       "  '0037e59caead9dab',\n",
       "  '008f93320e3661b8',\n",
       "  '0087c131ccffe160',\n",
       "  '000ffab30195c5e1',\n",
       "  '008344a80c43b8c9',\n",
       "  '0038f191ffc93d75',\n",
       "  '005eb6d31a8d821e',\n",
       "  '007f1839ada915e6',\n",
       "  '0013a8b1a5f26bcb',\n",
       "  '00585c1da10b448b',\n",
       "  '00961bcaadd6a278',\n",
       "  '008f1b06428778fe',\n",
       "  '008e2acf5bcf4be4',\n",
       "  '0069bf2d6881ad29',\n",
       "  '004176f28a17bf45',\n",
       "  '002264ea4d5f2887',\n",
       "  '006a4cdda4960588',\n",
       "  '0069e6d57a3beb51',\n",
       "  '005e6b1369cbe377',\n",
       "  '004b103182fb1eab',\n",
       "  '0009eaea3325de8c',\n",
       "  '007e1e47cd0e2fec',\n",
       "  '003b9f448ee4a29d',\n",
       "  '00905910dcbcc8aa',\n",
       "  '005306a4c109dab3',\n",
       "  '0060062dd4db5195',\n",
       "  '000c0dfd995809fa',\n",
       "  '009565ee1bc64e68',\n",
       "  '0034065c7b12a7a2',\n",
       "  '004d07d94cb92e35',\n",
       "  '003a19c04c079bf7',\n",
       "  '00037261f536c51d',\n",
       "  '009820fe28cd24dc',\n",
       "  '001ee16c46a99262',\n",
       "  '0001b41b1c6bb37e',\n",
       "  '0082b5a7b4a67da2',\n",
       "  '00548f16a392d8ed',\n",
       "  '00822d0d01752c7e',\n",
       "  '006774d59329b7bd',\n",
       "  '0048de0c9422f64f',\n",
       "  '00a317acddff8a62',\n",
       "  '004b073d5b456b15',\n",
       "  '005f59485fcddeb0',\n",
       "  '001363e1dbe91225',\n",
       "  '00078f8ce7eb276d',\n",
       "  '005cec874506e9d9',\n",
       "  '001810bf8c45bf5f',\n",
       "  '007f127033d66db5',\n",
       "  '001cadfd324f8087',\n",
       "  '0006f16e4e9f292e',\n",
       "  '007db1f1477ea977',\n",
       "  '006d11791d76b9f3',\n",
       "  '00148d055a169b93',\n",
       "  '005ed4dfecd86188',\n",
       "  '001b2dd65d9d925c',\n",
       "  '00218d74784ce50b',\n",
       "  '00480b6e1f19601b',\n",
       "  '00584d887401f47b',\n",
       "  '00957fadc476d7d9',\n",
       "  '0038d1dc2ad29469',\n",
       "  '006854d70298693e',\n",
       "  '00882ab8cfa42274',\n",
       "  '0091aec11b57d12e',\n",
       "  '009e3f1c0c757e43',\n",
       "  '00328eadb85b3010',\n",
       "  '001d874a4d3e8813',\n",
       "  '002c9cccf2f1d05b',\n",
       "  '0092f0871cc66dbc',\n",
       "  '002746baedcdff10',\n",
       "  '006de7a80921e04b',\n",
       "  '0016e01b742b8da3',\n",
       "  '00151a9f93c6b059',\n",
       "  '004b975fabbbffa9',\n",
       "  '000cfee90f50d471',\n",
       "  '0021fe88bc4da3e6',\n",
       "  '0098257c6952c9e3',\n",
       "  '00587c559177dcf2',\n",
       "  '0063f66706c20dfa',\n",
       "  '001d8e7be417776a',\n",
       "  '0005300084f90edc',\n",
       "  '0033b9d5ccd499fb',\n",
       "  '0063a8786a7034fc',\n",
       "  '008a1e9c45de8138',\n",
       "  '00510c3d06745849']}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    5,   225,     3,  ...,     0,     0,     0],\n",
       "        [   52,   263,    25,  ...,     0,     0,     0],\n",
       "        [  303,    50,    14,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    5,    31,    31,  ...,     0,     0,     0],\n",
       "        [    5, 39905,  2774,  ...,     0,     0,     0],\n",
       "        [ 6453,     4,   338,  ...,    73,  6262,     2]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1130])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.token_embedders.bert_token_embedder import BertEmbedder, PretrainedBertEmbedder\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.stacked_bidirectional_lstm import StackedBidirectionalLstm\n",
    "from allennlp.nn.util import get_text_field_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Seq2VecEncoder):\n",
    "    def __init__(self, inp_sz, aug_sz=None,\n",
    "                 hidden_sz=None, out_sz=None, dim=1, eps=1e-9,\n",
    "                 return_attention=False, use_bias=True):\n",
    "        super().__init__()\n",
    "        self.inp_sz, self.dim, self.eps = inp_sz, dim, eps\n",
    "        self.out_sz = ifnone(out_sz, self.inp_sz)\n",
    "        self.return_attention = return_attention\n",
    "        self.l1 = nn.Linear(inp_sz, ifnone(inp_sz * 2, hidden_sz))\n",
    "        nn.init.xavier_uniform_(self.l1.weight.data)\n",
    "        nn.init.zeros_(self.l1.bias.data)\n",
    "        \n",
    "        vw = torch.zeros(ifnone(inp_sz * 2, hidden_sz), 1)\n",
    "        nn.init.xavier_uniform_(vw)        \n",
    "        self.vw = nn.Parameter(vw)\n",
    "        self.use_bias = use_bias\n",
    "        if self.use_bias: self.b = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    @overrides\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self.inp_sz\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self.out_sz\n",
    "        \n",
    "    def forward(self, x, aug=None, mask=None):\n",
    "        e = torch.tanh(self.l1(x))\n",
    "        e = torch.einsum(\"bij,jk->bi\", [e, self.vw]) \n",
    "        if self.use_bias: e = e + self.b\n",
    "        a = torch.exp(e)\n",
    "        \n",
    "        if mask is not None: a = a.masked_fill(mask == 0, 0)\n",
    "\n",
    "        a = a / (torch.sum(a, dim=self.dim, keepdim=True) + self.eps)\n",
    "\n",
    "        weighted_input = x * a.unsqueeze(-1)\n",
    "        if self.return_attention:\n",
    "            return torch.sum(weighted_input, dim=1), a\n",
    "        else:\n",
    "            return torch.sum(weighted_input, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper, Seq2SeqEncoder\n",
    "from better_lstm import LSTM, VariationalDropout\n",
    "\n",
    "class MultiPooling(Seq2VecEncoder):\n",
    "    \"\"\"Does max and mean pooling over the temporal dimension\"\"\"\n",
    "    def __init__(self, input_sz: int):\n",
    "        super().__init__()\n",
    "        self.input_sz = input_sz\n",
    "        \n",
    "    @overrides\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self.input_sz\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self.input_sz * 2\n",
    "        \n",
    "    def forward(self, x, mask=None, aug=None):\n",
    "        max_, _ = torch.max(x, dim=1)\n",
    "        mean_ = torch.mean(x, dim=1)\n",
    "        return torch.cat([max_, mean_], dim=-1)\n",
    "\n",
    "class AugmentedMultiPool(MultiPooling):\n",
    "    def __init__(self, input_sz, aug_sz):\n",
    "        super().__init__(input_sz)\n",
    "        self.attn = Attention(input_sz, hidden_sz=input_sz, \n",
    "                              out_sz=input_sz)\n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self.input_sz * 3\n",
    "    \n",
    "    def forward(self, x, mask=None, aug=None):\n",
    "        pooled = super().forward(x, mask=mask, aug=aug)\n",
    "        attn = self.attn(x, mask=mask, aug=None)\n",
    "        return torch.cat([pooled, attn], dim=-1)\n",
    "    \n",
    "class BiRNN(Seq2SeqEncoder):\n",
    "    def __init__(self, rnn_type, n_layers, embed_sz, hidden_sz, dropoutw=0.):\n",
    "        super().__init__()\n",
    "        self.rnn_type = rnn_type\n",
    "        self.n_layers = n_layers\n",
    "        self.embed_sz = embed_sz\n",
    "        self.hidden_sz = hidden_sz\n",
    "        in_szs = [embed_sz] + [hidden_sz * 2] * (n_layers - 1)\n",
    "        if rnn_type == \"lstm\":\n",
    "            rnns = [LSTM(embed_sz, hidden_sz, batch_first=True, num_layers=n_layers,\n",
    "                         bidirectional=True, dropoutw=dropoutw)]\n",
    "        else:\n",
    "            if dropoutw > 0.0:\n",
    "                warnings.warn(\"Weight dropout not currently supported with GRUs\")\n",
    "            rnns = [nn.GRU(embed_sz, hidden_sz, batch_first=True, num_layers=n_layers, \n",
    "                           bidirectional=True)]\n",
    "            for gru in rnns:\n",
    "                for name, param in gru.named_parameters():\n",
    "                    if \"weight_hh\" in name:\n",
    "                        nn.init.orthogonal_(param.data)\n",
    "                    elif \"weight_ih\" in name:\n",
    "                        nn.init.xavier_uniform_(param.data)\n",
    "                    elif \"bias\" in name:\n",
    "                        nn.init.zeros_(param.data)\n",
    "        self.rnns = nn.ModuleList([PytorchSeq2SeqWrapper(rnn) for rnn in rnns])\n",
    "        if config.use_attention_aux:\n",
    "            self.ln = nn.Linear(embed_sz, 64) # handle attention auxillary input here\n",
    "\n",
    "    @overrides\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self.embed_sz\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        if config.rnn_residual:\n",
    "            out_sz = self.hidden_sz * 2 * self.n_layers\n",
    "        else:\n",
    "            out_sz = self.hidden_sz * 2\n",
    "        if config.use_attention_aux: out_sz += 64\n",
    "        return out_sz\n",
    "    \n",
    "    def forward(self, embeds, mask=None):\n",
    "        x = embeds\n",
    "        outputs = []\n",
    "        for rnn in self.rnns:\n",
    "            x = rnn(x, mask=mask)\n",
    "            if config.rnn_residual:\n",
    "                outputs.append(x)\n",
    "        if config.rnn_residual:\n",
    "            x = torch.cat(outputs, dim=-1)\n",
    "\n",
    "        if config.use_attention_aux:\n",
    "            x = torch.cat([torch.tanh(self.ln(embeds)), x], dim=-1)\n",
    "        return x\n",
    "    \n",
    "class BiRNNEncoder(Seq2VecEncoder):\n",
    "    def __init__(self, rnn: Seq2SeqEncoder,\n",
    "                 pooler: Seq2VecEncoder,\n",
    "                 dropouti=0.0, dropoutr=0.0):\n",
    "        super().__init__()\n",
    "        self.dropouti = VariationalDropout(dropouti, batch_first=True)\n",
    "        self.rnn = rnn\n",
    "        self.dropouto = VariationalDropout(dropoutr, batch_first=True)\n",
    "        self.pool = pooler\n",
    "        \n",
    "    @overrides\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self.rnn.get_input_dim()\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        out_dim = self.pool.get_output_dim()\n",
    "        if config.use_sentence_level_features:\n",
    "            out_dim += len(sentence_level_features)\n",
    "        return out_dim\n",
    "    \n",
    "    def _init_hidden_state(self, bs:int):\n",
    "        if self.rnn.rnn_type == \"lstm\":\n",
    "            return torch.zeros(bs, self.hidden_sz), torch.zeros(bs, self.hidden_sz)\n",
    "        else:\n",
    "            return torch.zeros(bs, self.hidden_sz)\n",
    "    \n",
    "    @overrides\n",
    "    def forward(self, x: torch.Tensor, sentence_feats: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor]=None) -> torch.Tensor:\n",
    "        seq = self.rnn(x, mask)\n",
    "        seq = self.dropouto(seq)\n",
    "        vec = self.pool(seq, aug=x, mask=mask)\n",
    "        if config.use_sentence_level_features:\n",
    "            return torch.cat([sentence_feats, vec], dim=-1)\n",
    "        else:\n",
    "            return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training.metrics import CategoricalAccuracy, BooleanAccuracy, Metric\n",
    "\n",
    "def prod(x: Iterable):\n",
    "    acc = 1\n",
    "    for v in x: acc *= v\n",
    "    return acc\n",
    "\n",
    "class MultilabelAccuracy(Metric):\n",
    "    def __init__(self, thres=0.5):\n",
    "        \n",
    "        self.thres = 0.5\n",
    "        self.correct_count = 0\n",
    "        self.total_count = 0\n",
    "    \n",
    "    def __call__(self, logits: torch.FloatTensor, \n",
    "                 t: torch.LongTensor) -> float:\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        t = t.detach().cpu().numpy() >= config.label_smoothing_eps\n",
    "        cc = ((logits >= self.thres) == t).sum()\n",
    "        tc = prod(logits.shape)\n",
    "        self.correct_count += cc\n",
    "        self.total_count += tc\n",
    "        return cc / tc\n",
    "    \n",
    "    def get_metric(self, reset: bool=False):\n",
    "        acc = self.correct_count / self.total_count\n",
    "        if reset:\n",
    "            self.reset()\n",
    "        return acc\n",
    "    \n",
    "    @overrides\n",
    "    def reset(self):\n",
    "        self.correct_count = 0\n",
    "        self.total_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.nn.util import move_to_device, has_tensor\n",
    "\n",
    "def permute(obj, p: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Given a structure (possibly) containing Tensors on the CPU,\n",
    "    permute all the Tensors\n",
    "    \"\"\"\n",
    "    if not has_tensor(obj):\n",
    "        return obj\n",
    "    elif isinstance(obj, torch.Tensor):\n",
    "        return obj[p]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: permute(value, p) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [permute(item, p) for item in obj]\n",
    "    elif isinstance(obj, tuple):\n",
    "        return tuple([permute(item, p) for item in obj])\n",
    "    else:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "\n",
    "class FocalLossWithLogits(nn.Module):\n",
    "    \"\"\"Borrowed from https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65938\"\"\"\n",
    "    def __init__(self, alpha=1., gamma=2.):\n",
    "        super().__init__()\n",
    "        self.alpha,self.gamma = alpha,gamma\n",
    "        self._loss = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        \n",
    "    def forward(self, y, t):\n",
    "        bce_loss = self._loss(y, t)\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        return (self.alpha * (1-pt) ** self.gamma * bce_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "#Reference https://stackoverflow.com/a/5898031\n",
    "from itertools import chain, combinations\n",
    "def all_subsets(ss):\n",
    "    return chain(*map(lambda x: combinations(ss, x), range(0, len(ss)+1)))\n",
    "\n",
    "\n",
    "class MBernLossWithLogits(nn.Module):\n",
    "    \"\"\" Referece https://arxiv.org/pdf/1206.1874.pdf\n",
    "        We perform a simple trick of assigning each label\n",
    "        value combination to a separate category\n",
    "        Total number of categories is 2**config.n_classes\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes = config.n_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        self._create_dicts()\n",
    "        self.n_cat = len(self.dict_cat)\n",
    "        \n",
    "    def _create_dicts(self):\n",
    "        pos = list(range(self.n_classes))\n",
    "        self.dict_cat = dict()\n",
    "        self.dict_cat_inv = dict()\n",
    "        for i, subset in enumerate(all_subsets(stuff)):\n",
    "            self.dict_cat[i] = subset\n",
    "            self.dict_cat_inv[subset] = i\n",
    "        \n",
    "    def _map_target(self,t_old):\n",
    "        \n",
    "        t_new = torch.zeros(t_old.size()[0],device=t_old.get_device(),dtype=torch.long)\n",
    "        \n",
    "        for i in range(t_old.size(0)):\n",
    "            p = (t_old[i,:]==1).nonzero().reshape(-1,).tolist()\n",
    "            t_new[i] = self.dict_cat_inv[tuple(p)]\n",
    "        \n",
    "        return t_new\n",
    "        \n",
    "    def get_marginal_probs(self,y):\n",
    "\n",
    "        probs = F.softmax(y,dim=1)\n",
    "        marginal_probs = torch.zeros(probs.size()[0],self.n_classes,device=probs.get_device())\n",
    "        \n",
    "        for i in range(probs.size(0)):\n",
    "            for j in range(probs.size(1)):\n",
    "                for p in self.dict_cat[j]:\n",
    "                    marginal_probs[i,p] += probs[i,j]\n",
    "                \n",
    "        return marginal_probs\n",
    "            \n",
    "    def forward(self, y, t):\n",
    "        t_new = self._map_target(t)\n",
    "        loss = self.loss(y, t_new)\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from torch.distributions.beta import Beta\n",
    "\n",
    "class BaselineModel(Model):\n",
    "    def __init__(self, word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 loss: nn.Module,\n",
    "                 out_sz: int=config.n_classes,\n",
    "                 multilabel: bool=True, \n",
    "                 dropouto=0.1,\n",
    "                 mixup_alpha: int=0.2):\n",
    "        super().__init__(vocab)\n",
    "        \n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        feature_sz = self.encoder.get_output_dim()\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(feature_sz, 50),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(dropouto),\n",
    "            nn.Linear(50, out_sz),\n",
    "        )        \n",
    "        \n",
    "        self.loss = loss\n",
    "\n",
    "        self.multilabel = multilabel\n",
    "        self.lambda_sampler = Beta(torch.tensor([mixup_alpha]), torch.tensor([mixup_alpha]))\n",
    "        \n",
    "        if self.multilabel:\n",
    "            self.accuracy = MultilabelAccuracy()\n",
    "            self.per_label_acc = {c: MultilabelAccuracy() for c in label_cols}\n",
    "        \n",
    "        elif self.loss._get_name()==\"MBernLossWithLogits\":\n",
    "            self.accuracy = CategoricalAccuracy(thres=0.6224593312)\n",
    "        \n",
    "        else:\n",
    "            self.accuracy = CategoricalAccuracy()\n",
    "        \n",
    "        self.is_test_mode = False\n",
    "            \n",
    "    def test_mode(self, val=True):\n",
    "        self.is_test_mode = val\n",
    "        \n",
    "    def get_embeddings(self, toks: Dict[str, torch.Tensor],\n",
    "                       word_feats: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Encapsulates addition of word level features\"\"\"\n",
    "        embeddings = self.word_embeddings(toks)\n",
    "        if config.use_word_level_features:\n",
    "            embeddings = torch.cat([word_feats, embeddings], dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, tokens: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor,\n",
    "                word_level_features: torch.Tensor,\n",
    "                sentence_level_features: torch.Tensor,\n",
    "                **meta) -> torch.Tensor:\n",
    "        \n",
    "        if self.is_test_mode: tokens[\"tokens\"] *= 0\n",
    "        \n",
    "        mask = get_text_field_mask(tokens)\n",
    "        embeddings = self.get_embeddings(tokens, word_level_features)\n",
    "        \n",
    "        state = self.encoder(embeddings, \n",
    "                             sentence_feats=sentence_level_features, \n",
    "                             mask=mask)\n",
    "        \n",
    "        class_logits = self.projection(state)\n",
    "        \n",
    "        output = {\"class_logits\": class_logits}\n",
    "\n",
    "        if self.loss._get_name()==\"MBernLossWithLogits\":\n",
    "            output[\"marginal_probs\"] = self.loss.get_marginal_probs(class_logits)\n",
    "\n",
    "        if self.loss._get_name()==\"MBernLossWithLogits\":\n",
    "            output[\"accuracy\"] = self.accuracy(self.loss.get_marginal_probs(class_logits), label)\n",
    "        else:\n",
    "            output[\"accuracy\"] = self.accuracy(class_logits, label)\n",
    "        \n",
    "        output[\"loss\"] = self.loss(class_logits, label)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def mixup(self, tokens: Dict[str, torch.Tensor],\n",
    "              label: torch.Tensor,\n",
    "              word_level_features: torch.Tensor,\n",
    "              sentence_level_features: torch.Tensor,\n",
    "              **meta) -> TensorDict:\n",
    "        # generate new tokens and labels\n",
    "        bs = label.size(0)\n",
    "        shuf = torch.randperm(bs).to(label.device)\n",
    "        tokens2 = permute(tokens, shuf)\n",
    "        labels1, labels2 = label, permute(label, shuf)\n",
    "        # TODO: Think of how to handle this masking intelligently\n",
    "        mask1, mask2 = (get_text_field_mask(t) for t in (tokens, tokens2))\n",
    "        embs1, embs2 = (self.get_embeddings(t, word_level_features) for t in (tokens, tokens2))\n",
    "        # interpolate\n",
    "        ratios = self.lambda_sampler.sample((bs, 1)).to(label.device)\n",
    "        embs = ratios * embs1 + (1-ratios) * embs2\n",
    "        label = ratios.squeeze(2) * labels1 + (1-ratios.squeeze(2)) * labels2\n",
    "        \n",
    "        # remaining process is the same\n",
    "        # TODO: Handle stat feats\n",
    "        state = self.encoder(embs, sentence_level_features, mask1 * mask2) # TODO: Handle masking\n",
    "        class_logits = self.projection(state)\n",
    "        \n",
    "        output = {\"loss\": self.loss(class_logits, label)}\n",
    "        return output\n",
    "    \n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set(\"vocab_size\", min(vocab.get_vocab_size(), config.max_vocab_size))\n",
    "if config.model_type == \"standard\":\n",
    "    config.set(\"embedding_dim\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def get_fasttext_embeddings(model_path: str, vocab: Vocabulary):\n",
    "    prog_bar = tqdm(open(model_path, encoding=\"utf8\", errors='ignore'))\n",
    "    prog_bar.set_description(\"Loading embeddings\")\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in prog_bar\n",
    "                             if len(o)>100)\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "\n",
    "    embeddings = np.zeros((config.vocab_size + 5, 300))\n",
    "    n_missing_tokens = 0\n",
    "    prog_bar = tqdm(vocab.get_index_to_token_vocabulary().items())\n",
    "    prog_bar.set_description(\"Creating matrix\")\n",
    "    for idx, token in prog_bar:\n",
    "        if idx == 0: continue # keep padding as all zeros\n",
    "        if idx == 1: continue # Treat unknown words as dropped words\n",
    "        if token == \"[MASK]\":\n",
    "            embeddings[idx, :] = np.random.randn(300) * 0.5\n",
    "        if token not in embeddings_index:\n",
    "            n_missing_tokens += 1\n",
    "            if n_missing_tokens < 10:\n",
    "                warnings.warn(f\"Token {token} not in embeddings: did you change preprocessing?\")\n",
    "            if n_missing_tokens == 10:\n",
    "                warnings.warn(f\"More than {n_missing_tokens} missing, supressing warnings\")\n",
    "        else:\n",
    "            embeddings[idx, :] = embeddings_index[token]\n",
    "    \n",
    "    print(\"MISSING tokens: \", n_missing_tokens)\n",
    "    if n_missing_tokens > 0:\n",
    "        warnings.warn(f\"{n_missing_tokens} in total are missing from embedding text file\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading embeddings: : 320356it [00:30, 10462.11it/s]\n",
      "/home/anna/neuralnlp/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "Creating matrix: 100%|██████████| 306326/306326 [00:01<00:00, 231734.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING tokens:  0\n",
      "[Loading embeddings] done in 33 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Loading embeddings\"):\n",
    "    if config.model_type == \"standard\":\n",
    "        embedding_weights = get_fasttext_embeddings(config.ft_model_path, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmbedding(Embedding):\n",
    "    # TODO: Fix (make this decently efficient: currently allocating two embeddings)\n",
    "    def __init__(self, num_embeddings, embedding_dim,\n",
    "                 padding_index=None, max_norm=None, trainable=True,\n",
    "                 weight=None, dropout=0., scale=None):\n",
    "        super().__init__(num_embeddings, embedding_dim, weight=weight,\n",
    "                         padding_index=padding_index, max_norm=max_norm,\n",
    "                         trainable=trainable)\n",
    "        self.dropout = dropout\n",
    "        self.scale = scale\n",
    "        self.padding_idx = padding_index\n",
    "\n",
    "    def forward(self, words):\n",
    "        weight = self.weight\n",
    "        if self.dropout > 0.0 and self.training:\n",
    "            mask = weight.data.new().resize_((weight.size(0), 1)).bernoulli_(1 - self.dropout).expand_as(weight) / (1 - self.dropout)\n",
    "            masked_embed_weight = mask * weight\n",
    "        else:\n",
    "            masked_embed_weight = weight\n",
    "        if self.scale:\n",
    "            masked_embed_weight = scale.expand_as(masked_embed_weight) * masked_embed_weight\n",
    "\n",
    "        padding_idx = self.padding_idx\n",
    "        if padding_idx is None:\n",
    "            padding_idx = -1\n",
    "\n",
    "        X = torch.nn.functional.embedding(words, masked_embed_weight,\n",
    "            padding_idx, self.max_norm, self.norm_type,\n",
    "            self.scale_grad_by_freq, self.sparse\n",
    "          )\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.text_field_embedders.text_field_embedder import TextFieldEmbedder\n",
    "from allennlp.modules.time_distributed import TimeDistributed\n",
    "\n",
    "# TODO: Implement\n",
    "class ElmoTextFieldEmbedder(TextFieldEmbedder):\n",
    "    # AllenNLP support for caching sucks by default\n",
    "    # so we have to write our own embedder to bypass this problem\n",
    "    def __init__(self,\n",
    "                 token_embedders: Dict[str, Any],\n",
    "                 embedder_to_indexer_map: Dict[str, List[str]] = None,\n",
    "                 allow_unmatched_keys: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self._token_embedders = token_embedders\n",
    "        self._embedder_to_indexer_map = embedder_to_indexer_map\n",
    "        for key, embedder in token_embedders.items():\n",
    "            name = 'token_embedder_%s' % key\n",
    "            self.add_module(name, embedder)\n",
    "        self._allow_unmatched_keys = allow_unmatched_keys\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        output_dim = 0\n",
    "        for embedder in self._token_embedders.values():\n",
    "            output_dim += embedder.get_output_dim()\n",
    "        return output_dim\n",
    "    \n",
    "    def forward(self, text_field_input: Dict[str, torch.Tensor],\n",
    "                num_wrapping_dims: int = 0) -> torch.Tensor:\n",
    "        if self._token_embedders.keys() != text_field_input.keys():\n",
    "            if not self._allow_unmatched_keys:\n",
    "                message = \"Mismatched token keys: %s and %s\" % (str(self._token_embedders.keys()),\n",
    "                                                                str(text_field_input.keys()))\n",
    "                raise ConfigurationError(message)\n",
    "        embedded_representations = []\n",
    "        keys = sorted(self._token_embedders.keys())\n",
    "        for key in keys:\n",
    "            # If we pre-specified a mapping explictly, use that.\n",
    "            if self._embedder_to_indexer_map is not None:\n",
    "                tensors = [text_field_input[indexer_key] for\n",
    "                           indexer_key in self._embedder_to_indexer_map[key]]\n",
    "            else:\n",
    "                # otherwise, we assume the mapping between indexers and embedders\n",
    "                # is bijective and just use the key directly.\n",
    "                tensors = [text_field_input[key]]\n",
    "            # Note: need to use getattr here so that the pytorch voodoo\n",
    "            # with submodules works with multiple GPUs.\n",
    "            embedder = getattr(self, 'token_embedder_{}'.format(key))\n",
    "            for _ in range(num_wrapping_dims):\n",
    "                embedder = TimeDistributed(embedder)\n",
    "            # Force embedder to use word inputs\n",
    "            if key == \"tokens\":\n",
    "                token_vectors = embedder(tensors[0], \n",
    "                                         word_inputs=tensors[0] if config.cache_elmo_embeddings else None)\n",
    "            else:\n",
    "                token_vectors = embedder(*tensors)\n",
    "            embedded_representations.append(token_vectors)\n",
    "        return torch.cat(embedded_representations, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "from allennlp.modules.token_embedders.token_embedder import TokenEmbedder\n",
    "\n",
    "class CustomBertEmbedder(TokenEmbedder):\n",
    "    \"\"\"\n",
    "    A ``TokenEmbedder`` that produces BERT embeddings for your tokens.\n",
    "    Should be paired with a ``BertIndexer``, which produces wordpiece ids.\n",
    "    Sums last 4 hidden layers for now (might use scalar mix in the future)\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_model: str,\n",
    "                 use_scalar_mix: bool = False,\n",
    "                 fine_tune: bool = False,\n",
    "                 n_hidden_layers: int = 4) -> None:\n",
    "        super().__init__()\n",
    "        if use_scalar_mix and fine_tune:\n",
    "            raise ConfigurationError(\"Choose mix or fine tuning\")\n",
    "        \n",
    "        self.bert_model = BertModel.from_pretrained(pretrained_model)\n",
    "        for param in self.bert_model.parameters():\n",
    "            param.requires_grad = fine_tune\n",
    "        self.output_dim = self.bert_model.config.hidden_size\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        if use_scalar_mix:\n",
    "            self._scalar_mix = ScalarMix(n_hidden_layers,\n",
    "                                         do_layer_norm=False)\n",
    "        else:\n",
    "            self._scalar_mix = None\n",
    "\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self.output_dim\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: torch.LongTensor,\n",
    "                offsets: torch.LongTensor = None,\n",
    "                token_type_ids: torch.LongTensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : ``torch.LongTensor``\n",
    "            The (batch_size, ..., max_sequence_length) tensor of wordpiece ids.\n",
    "        offsets : ``torch.LongTensor``, optional\n",
    "            The BERT embeddings are one per wordpiece. However it's possible/likely\n",
    "            you might want one per original token. In that case, ``offsets``\n",
    "            represents the indices of the desired wordpiece for each original token.\n",
    "            Depending on how your token indexer is configured, this could be the\n",
    "            position of the last wordpiece for each token, or it could be the position\n",
    "            of the first wordpiece for each token.\n",
    "\n",
    "            For example, if you had the sentence \"Definitely not\", and if the corresponding\n",
    "            wordpieces were [\"Def\", \"##in\", \"##ite\", \"##ly\", \"not\"], then the input_ids\n",
    "            would be 5 wordpiece ids, and the \"last wordpiece\" offsets would be [3, 4].\n",
    "            If offsets are provided, the returned tensor will contain only the wordpiece\n",
    "            embeddings at those positions, and (in particular) will contain one embedding\n",
    "            per token. If offsets are not provided, the entire tensor of wordpiece embeddings\n",
    "            will be returned.\n",
    "        token_type_ids : ``torch.LongTensor``, optional\n",
    "            If an input consists of two sentences (as in the BERT paper),\n",
    "            tokens from the first sentence should have type 0 and tokens from\n",
    "            the second sentence should have type 1.  If you don't provide this\n",
    "            (the default BertIndexer doesn't) then it's assumed to be all 0s.\n",
    "        \"\"\"\n",
    "        # pylint: disable=arguments-differ\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        input_mask = (input_ids != 0).long()\n",
    "\n",
    "        # input_ids may have extra dimensions, so we reshape down to 2-d\n",
    "        # before calling the BERT model and then reshape back at the end.\n",
    "        all_encoder_layers, _ = self.bert_model(input_ids=nn_util.combine_initial_dims(input_ids),\n",
    "                                                token_type_ids=nn_util.combine_initial_dims(token_type_ids),\n",
    "                                                attention_mask=nn_util.combine_initial_dims(input_mask))\n",
    "        if self._scalar_mix is not None:\n",
    "            mix = self._scalar_mix(all_encoder_layers[-self.n_hidden_layers:], input_mask)\n",
    "        else:\n",
    "            mix = torch.stack(all_encoder_layers[-self.n_hidden_layers:]).mean(dim=0)\n",
    "\n",
    "        # At this point, mix is (batch_size * d1 * ... * dn, sequence_length, embedding_dim)\n",
    "\n",
    "        if offsets is None:\n",
    "            # Resize to (batch_size, d1, ..., dn, sequence_length, embedding_dim)\n",
    "            return nn_util.uncombine_initial_dims(mix, input_ids.size())\n",
    "        else:\n",
    "            # offsets is (batch_size, d1, ..., dn, orig_sequence_length)\n",
    "            offsets2d = nn_util.combine_initial_dims(offsets)\n",
    "            # now offsets is (batch_size * d1 * ... * dn, orig_sequence_length)\n",
    "            range_vector = nn_util.get_range_vector(offsets2d.size(0),\n",
    "                                                 device=nn_util.get_device_of(mix)).unsqueeze(1)\n",
    "            # selected embeddings is also (batch_size * d1 * ... * dn, orig_sequence_length)\n",
    "            selected_embeddings = mix[range_vector, offsets2d]\n",
    "\n",
    "            return util.uncombine_initial_dims(selected_embeddings, offsets.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "\n",
    "if config.model_type == \"standard\":\n",
    "    token_embedding = CustomEmbedding(num_embeddings=config.vocab_size + 5,\n",
    "                                      embedding_dim=config.embedding_dim,\n",
    "                                      trainable=not config.freeze_embeddings,\n",
    "                                      weight=torch.tensor(embedding_weights, dtype=torch.float),\n",
    "                                      dropout=config.dropoute, padding_index=0)\n",
    "    word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "    \n",
    "elif \"elmo\" in config.model_type:\n",
    "    from allennlp.modules.token_embedders import ElmoTokenEmbedder\n",
    "    from allennlp.modules.elmo import Elmo\n",
    "\n",
    "    options_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_options.json'\n",
    "    weight_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_weights.hdf5'\n",
    "    \n",
    "    all_words_ordered = [w for i, w in sorted([(i, w) for w, i in vocab.get_token_to_index_vocabulary().items()])]\n",
    "    elmo_embedder = ElmoTokenEmbedder(\n",
    "        options_file, weight_file, dropout=0.5, # recommended value\n",
    "        vocab_to_cache=all_words_ordered if config.cache_elmo_embeddings else None,\n",
    "        do_layer_norm=False,\n",
    "        requires_grad=False,\n",
    "    )\n",
    "    # TODO: Find a way to skip character encodings\n",
    "    word_embeddings = ElmoTextFieldEmbedder({\"tokens\": elmo_embedder})\n",
    "    \n",
    "elif \"bert\" in config.model_type:\n",
    "    from allennlp.modules.token_embedders.bert_token_embedder import PretrainedBertEmbedder\n",
    "    bert_embedder = CustomBertEmbedder(\n",
    "            pretrained_model=config.model_type,\n",
    "            fine_tune=False, use_scalar_mix=False,\n",
    "    )\n",
    "    word_embeddings: TextFieldEmbedder = BasicTextFieldEmbedder({\"tokens\": bert_embedder},\n",
    "                                                                 # we'll be ignoring masks so we'll need to set this to True\n",
    "                                                                allow_unmatched_keys = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.pooling_type != \"bert_pool\":\n",
    "    embed_sz = word_embeddings.get_output_dim()\n",
    "    if config.use_word_level_features: \n",
    "        embed_sz += len(word_level_features)\n",
    "    rnn = BiRNN(rnn_type=rnn_type, n_layers=config.num_layers, \n",
    "                embed_sz=embed_sz, hidden_sz=config.hidden_sz, \n",
    "                dropoutw=config.dropoutw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.pooling_type != \"bert_pool\":\n",
    "    if config.pooling_type == \"attention\":\n",
    "        pooler = Attention(rnn.get_output_dim(), hidden_sz=rnn.get_output_dim(),\n",
    "                           out_sz=rnn.get_output_dim(), dim=1, \n",
    "                           use_bias=config.attention_bias)\n",
    "    elif config.pooling_type == \"multipool\":\n",
    "        pooler = MultiPooling(rnn.get_output_dim())\n",
    "    elif config.pooling_type == \"augmented_multipool\":\n",
    "        pooler = AugmentedMultiPool(rnn.get_output_dim(), \n",
    "                                    aug_sz=embed_sz)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid pooling type {config.pooling_type}\")\n",
    "\n",
    "    encoder = BiRNNEncoder(\n",
    "            rnn,\n",
    "            pooler,\n",
    "            dropouti=config.dropouti,\n",
    "            dropoutr=config.dropoutr,\n",
    "        )\n",
    "else:\n",
    "    BERT_DIM = word_embeddings.get_output_dim()\n",
    "\n",
    "    class BertSentencePooler(Seq2VecEncoder):\n",
    "        def forward(self, embs: torch.tensor, \n",
    "                    mask: torch.tensor=None,\n",
    "                    **kwargs,\n",
    "                   ) -> torch.tensor:\n",
    "            # extract first token tensor\n",
    "            return embs[:, 0]\n",
    "\n",
    "        @overrides\n",
    "        def get_output_dim(self) -> int:\n",
    "            return BERT_DIM\n",
    "\n",
    "    encoder = BertSentencePooler(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.n_classes > 2:\n",
    "    if config.use_focal_loss:\n",
    "        loss = FocalLossWithLogits(config.focal_loss_alpha,\n",
    "                                   config.focal_loss_gamma)\n",
    "    elif config.use_mbern_loss:\n",
    "        loss = MBernLossWithLogits()\n",
    "        \n",
    "    else:\n",
    "        loss = nn.BCEWithLogitsLoss()\n",
    "else:\n",
    "    loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineModel(\n",
    "    word_embeddings, \n",
    "    encoder, \n",
    "    loss,\n",
    "    out_sz=2**config.n_classes if use_mbern_loss else config.n_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss._get_name()==\"MBernLossWithLogits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaselineModel(\n",
       "  (word_embeddings): BasicTextFieldEmbedder(\n",
       "    (token_embedder_tokens): CustomEmbedding()\n",
       "  )\n",
       "  (encoder): BiRNNEncoder(\n",
       "    (dropouti): VariationalDropout()\n",
       "    (rnn): BiRNN(\n",
       "      (rnns): ModuleList(\n",
       "        (0): PytorchSeq2SeqWrapper(\n",
       "          (_module): LSTM(\n",
       "            300, 128, num_layers=2, batch_first=True, bidirectional=True\n",
       "            (input_drop): VariationalDropout()\n",
       "            (output_drop): VariationalDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dropouto): VariationalDropout()\n",
       "    (pool): AugmentedMultiPool(\n",
       "      (attn): Attention(\n",
       "        (l1): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projection): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=50, bias=True)\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Dropout(p=0.1)\n",
       "    (3): Linear(in_features=50, out_features=6, bias=True)\n",
       "  )\n",
       "  (loss): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize bias according to prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.bias_init:\n",
    "    \n",
    "    class_bias = torch.zeros(2**config.n_classes if use_mbern_loss else config.n_classes)\n",
    "    \n",
    "    for i, _ in enumerate(label_cols):\n",
    "        p = train_labels[:, i].mean()\n",
    "        class_bias[i] = np.log(p / (1-p))\n",
    "\n",
    "    model.projection[-1].bias.data = class_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU: model.cuda()\n",
    "else: model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), DATA_ROOT / \"tmp_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = nn_util.move_to_device(batch, 0 if USE_GPU else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = batch[\"tokens\"]\n",
    "labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([[    5,   225,     3,  ...,     0,     0,     0],\n",
       "         [   52,   263,    25,  ...,     0,     0,     0],\n",
       "         [  303,    50,    14,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    5,    31,    31,  ...,     0,     0,     0],\n",
       "         [    5, 39905,  2774,  ...,     0,     0,     0],\n",
       "         [ 6453,     4,   338,  ...,    73,  6262,     2]], device='cuda:0')}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = get_text_field_mask(tokens)\n",
    "wlfs = batch[\"word_level_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.word_embeddings(tokens)\n",
    "if config.use_word_level_features:\n",
    "    embeddings = torch.cat([wlfs, embeddings], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.use_word_level_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1130, 300])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1130])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.pooling_type != \"bert_pool\":\n",
    "    encoded = model.encoder.rnn(embeddings, mask=mask)\n",
    "else:\n",
    "    encoded = model.encoder(embeddings, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1130, 256])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_logits': tensor([[-2.3088, -4.5534, -2.9362, -5.8598, -2.9710, -4.7661],\n",
       "         [-2.3681, -4.5656, -2.8244, -5.9046, -2.9174, -4.8062],\n",
       "         [-2.3243, -4.5575, -2.9952, -5.9677, -2.8724, -4.7600],\n",
       "         [-2.3390, -4.5136, -2.9447, -5.8095, -2.8404, -4.8257],\n",
       "         [-2.3837, -4.6073, -2.9077, -5.8893, -2.9191, -4.7274],\n",
       "         [-2.4114, -4.4682, -2.8669, -5.9123, -2.8894, -4.7523],\n",
       "         [-2.4325, -4.5400, -2.9938, -5.9195, -2.9185, -4.7320],\n",
       "         [-2.4159, -4.6048, -2.8821, -5.8920, -2.9248, -4.7173],\n",
       "         [-2.3663, -4.5228, -2.8581, -5.8475, -2.9108, -4.7301],\n",
       "         [-2.3527, -4.6007, -2.9507, -5.9153, -2.8797, -4.7642],\n",
       "         [-2.3791, -4.5274, -2.9435, -5.9607, -2.9185, -4.7756],\n",
       "         [-2.3842, -4.5516, -2.8707, -5.8145, -2.8721, -4.7087],\n",
       "         [-2.3525, -4.6068, -2.9674, -5.9033, -2.9546, -4.7640],\n",
       "         [-2.3625, -4.4743, -2.9217, -5.8944, -2.8890, -4.7299],\n",
       "         [-2.4061, -4.6555, -2.8937, -5.8678, -2.9134, -4.7465],\n",
       "         [-2.4053, -4.4833, -2.9418, -5.8365, -2.9291, -4.7368],\n",
       "         [-2.4081, -4.5172, -2.9066, -5.9313, -2.9797, -4.7112],\n",
       "         [-2.4305, -4.4819, -2.9713, -5.9329, -2.8991, -4.7446],\n",
       "         [-2.3857, -4.4770, -2.8986, -5.9503, -2.9564, -4.6905],\n",
       "         [-2.3177, -4.5319, -2.8545, -5.9358, -2.9140, -4.7075],\n",
       "         [-2.3705, -4.5179, -2.8849, -5.9079, -2.9282, -4.7989],\n",
       "         [-2.3837, -4.5186, -2.9589, -5.8212, -2.9454, -4.6910],\n",
       "         [-2.3267, -4.5414, -2.8964, -5.9379, -3.0143, -4.7274],\n",
       "         [-2.4501, -4.4506, -3.0023, -5.8299, -2.9012, -4.6149],\n",
       "         [-2.4346, -4.5347, -2.8887, -5.8970, -2.8791, -4.8427],\n",
       "         [-2.3493, -4.4671, -2.9364, -5.8260, -2.8894, -4.7381],\n",
       "         [-2.3133, -4.5533, -2.9535, -5.9873, -3.0110, -4.6289],\n",
       "         [-2.4042, -4.4961, -2.9518, -5.9004, -2.9410, -4.6767],\n",
       "         [-2.3006, -4.5632, -2.9295, -5.8897, -2.9186, -4.7628],\n",
       "         [-2.4554, -4.5690, -2.8902, -5.8503, -2.8225, -4.6744],\n",
       "         [-2.4132, -4.5740, -2.9396, -5.8786, -2.8837, -4.6816],\n",
       "         [-2.3970, -4.5690, -2.9546, -5.8375, -2.9403, -4.7152],\n",
       "         [-2.3978, -4.4701, -3.0360, -5.9978, -2.9770, -4.7346],\n",
       "         [-2.4412, -4.6352, -2.8861, -5.8140, -2.8656, -4.7034],\n",
       "         [-2.4705, -4.6000, -2.8654, -5.8577, -2.8992, -4.6863],\n",
       "         [-2.4607, -4.5262, -2.9471, -5.9502, -2.9233, -4.7649],\n",
       "         [-2.4347, -4.5918, -3.0258, -5.8850, -2.8330, -4.7649],\n",
       "         [-2.4061, -4.6144, -2.8913, -5.9115, -3.0065, -4.7677],\n",
       "         [-2.2937, -4.5820, -2.8055, -5.8763, -2.9191, -4.7789],\n",
       "         [-2.4172, -4.5129, -2.8961, -5.9467, -2.9107, -4.7489],\n",
       "         [-2.4459, -4.5216, -2.9321, -5.8150, -2.9139, -4.6496],\n",
       "         [-2.4230, -4.5282, -2.9027, -5.9428, -2.9231, -4.6179],\n",
       "         [-2.4490, -4.5417, -2.8525, -5.8750, -2.8759, -4.7224],\n",
       "         [-2.3977, -4.6060, -3.0067, -5.8914, -2.9022, -4.7237],\n",
       "         [-2.4266, -4.5384, -2.9833, -5.9415, -2.9801, -4.6963],\n",
       "         [-2.3464, -4.5224, -2.8627, -5.7193, -2.8626, -4.7799],\n",
       "         [-2.5066, -4.5050, -2.8465, -5.8869, -2.9006, -4.6153],\n",
       "         [-2.4195, -4.5693, -2.9108, -5.8664, -2.9885, -4.7434],\n",
       "         [-2.3879, -4.5958, -2.9163, -5.8426, -2.9318, -4.6341],\n",
       "         [-2.4382, -4.5114, -2.8479, -5.9029, -2.9970, -4.7108],\n",
       "         [-2.5019, -4.5465, -2.9575, -5.9714, -2.9158, -4.7765],\n",
       "         [-2.4497, -4.4595, -2.8920, -5.8995, -2.9119, -4.6352],\n",
       "         [-2.3607, -4.4890, -2.9687, -5.8908, -2.9006, -4.6972],\n",
       "         [-2.4332, -4.4969, -3.0143, -5.7829, -2.8530, -4.6721],\n",
       "         [-2.4086, -4.5688, -2.8689, -5.8235, -2.8460, -4.7626],\n",
       "         [-2.3747, -4.5592, -2.9672, -5.9250, -3.0030, -4.6530],\n",
       "         [-2.2705, -4.6018, -2.9393, -5.9186, -2.9416, -4.7437],\n",
       "         [-2.4008, -4.5321, -2.9273, -5.9363, -2.9878, -4.7374],\n",
       "         [-2.3875, -4.5361, -2.8951, -5.8219, -2.8916, -4.6090],\n",
       "         [-2.3887, -4.5864, -2.9169, -5.9246, -2.9298, -4.6397],\n",
       "         [-2.4379, -4.5147, -3.0332, -6.0239, -2.8703, -4.7169],\n",
       "         [-2.3355, -4.6262, -2.8591, -5.9706, -2.9231, -4.7997],\n",
       "         [-2.4949, -4.5645, -3.0100, -5.9491, -2.8917, -4.6816],\n",
       "         [-2.3820, -4.4987, -2.9067, -5.9779, -2.9950, -4.6070],\n",
       "         [-2.2999, -4.5421, -2.8482, -5.8392, -3.0005, -4.5641],\n",
       "         [-2.3823, -4.4700, -2.9267, -5.9322, -3.0226, -4.6843],\n",
       "         [-2.4233, -4.5438, -3.0886, -5.8860, -2.9393, -4.7432],\n",
       "         [-2.3098, -4.4199, -2.9660, -5.8095, -2.8792, -4.6443],\n",
       "         [-2.4933, -4.5122, -2.9624, -5.8938, -2.8262, -4.8317],\n",
       "         [-2.3418, -4.5164, -2.9114, -5.8273, -2.8131, -4.6660],\n",
       "         [-2.4345, -4.5546, -2.8544, -5.8801, -2.8933, -4.6659],\n",
       "         [-2.4182, -4.5633, -2.9624, -5.9496, -2.9154, -4.7278],\n",
       "         [-2.3383, -4.4570, -2.9307, -5.9592, -2.8644, -4.8111],\n",
       "         [-2.4182, -4.5345, -3.0112, -5.9345, -2.9833, -4.7507],\n",
       "         [-2.3694, -4.4496, -2.9272, -5.8975, -2.8821, -4.7219],\n",
       "         [-2.4450, -4.4806, -2.8764, -5.9780, -2.9799, -4.7776],\n",
       "         [-2.4404, -4.4249, -2.9696, -5.8622, -2.9329, -4.7012],\n",
       "         [-2.4830, -4.5340, -2.9964, -6.0544, -2.8971, -4.7724],\n",
       "         [-2.4297, -4.6883, -2.9921, -5.9456, -2.9765, -4.7972],\n",
       "         [-2.4775, -4.5494, -2.8937, -5.8690, -2.8519, -4.7035],\n",
       "         [-2.3664, -4.5560, -2.9261, -5.8749, -2.9628, -4.6608],\n",
       "         [-2.3077, -4.5671, -2.9355, -6.0024, -2.9776, -4.7871],\n",
       "         [-2.3714, -4.4794, -2.9139, -5.8960, -2.9082, -4.7145],\n",
       "         [-2.4102, -4.5348, -2.9558, -5.9261, -3.0174, -4.6214],\n",
       "         [-2.3439, -4.5148, -2.8743, -5.8729, -2.9159, -4.7698],\n",
       "         [-2.3200, -4.4028, -2.8717, -5.8285, -2.8860, -4.5756],\n",
       "         [-2.4610, -4.6334, -3.0114, -5.9165, -2.7792, -4.6246],\n",
       "         [-2.4650, -4.6714, -2.9495, -5.8826, -2.9459, -4.7079],\n",
       "         [-2.5390, -4.4980, -2.9244, -6.0357, -2.8747, -4.6190],\n",
       "         [-2.4013, -4.6428, -2.9480, -5.8475, -2.9226, -4.6652],\n",
       "         [-2.3508, -4.6253, -2.9361, -5.9607, -2.8742, -4.7734],\n",
       "         [-2.2530, -4.5441, -2.9331, -5.8452, -2.9157, -4.7432],\n",
       "         [-2.3403, -4.4965, -3.0287, -5.9683, -2.8626, -4.6995],\n",
       "         [-2.4718, -4.5630, -2.9118, -5.8937, -2.8254, -4.5219],\n",
       "         [-2.4505, -4.3521, -2.9929, -5.9267, -2.9717, -4.6490],\n",
       "         [-2.4786, -4.4903, -3.0409, -5.9910, -2.9306, -4.7633],\n",
       "         [-2.4215, -4.5586, -2.9855, -5.8056, -2.8500, -4.6222],\n",
       "         [-2.4727, -4.4704, -3.0118, -5.8520, -2.8499, -4.5437],\n",
       "         [-2.4439, -4.5483, -3.0464, -5.9826, -3.0070, -4.7092],\n",
       "         [-2.4155, -4.6756, -2.8987, -6.0135, -2.8536, -4.6998],\n",
       "         [-2.4299, -4.5006, -3.0114, -5.8823, -2.9264, -4.6087],\n",
       "         [-2.4352, -4.6205, -2.9357, -5.8997, -2.8732, -4.7073],\n",
       "         [-2.4205, -4.6034, -2.9615, -5.9020, -2.9139, -4.7964],\n",
       "         [-2.4423, -4.6741, -2.9437, -5.9720, -2.9140, -4.8640],\n",
       "         [-2.3872, -4.6362, -2.9241, -5.8659, -2.9319, -4.7427],\n",
       "         [-2.3927, -4.5747, -2.9290, -5.9438, -2.9817, -4.7847],\n",
       "         [-2.3835, -4.5896, -2.9347, -6.0760, -2.9868, -4.8607],\n",
       "         [-2.4314, -4.5726, -2.8554, -5.9368, -2.9976, -4.6419],\n",
       "         [-2.4963, -4.5275, -3.0331, -6.0219, -2.8993, -4.7282],\n",
       "         [-2.3407, -4.5287, -2.8636, -5.8517, -3.0193, -4.7641],\n",
       "         [-2.3949, -4.5016, -2.9723, -5.8342, -2.8095, -4.6314],\n",
       "         [-2.3744, -4.5503, -2.9595, -5.9619, -2.8784, -4.7212],\n",
       "         [-2.4171, -4.6853, -2.9917, -5.9807, -2.8196, -4.7296],\n",
       "         [-2.4390, -4.5721, -2.9525, -5.8542, -2.9049, -4.6098],\n",
       "         [-2.5221, -4.5440, -3.0765, -5.9976, -2.9689, -4.6718],\n",
       "         [-2.4862, -4.5873, -3.0068, -5.8871, -2.9001, -4.6116],\n",
       "         [-2.4582, -4.4913, -2.9170, -5.9627, -2.8770, -4.8203],\n",
       "         [-2.4537, -4.4699, -2.8636, -5.7539, -2.8396, -4.6713],\n",
       "         [-2.3926, -4.6355, -2.9870, -5.9859, -2.9062, -4.7459],\n",
       "         [-2.4999, -4.5953, -2.9371, -6.0242, -2.9232, -4.7146],\n",
       "         [-2.5383, -4.5917, -3.0088, -5.9450, -2.9314, -4.8307],\n",
       "         [-2.3801, -4.5156, -2.8723, -5.9511, -2.9667, -4.8273],\n",
       "         [-2.4801, -4.5741, -3.0199, -6.0022, -2.9419, -4.8009],\n",
       "         [-2.3191, -4.5257, -2.9077, -5.8830, -2.9382, -4.6668],\n",
       "         [-2.5114, -4.5186, -2.9909, -5.8138, -2.8141, -4.6177],\n",
       "         [-2.5110, -4.6229, -3.0073, -5.9126, -2.9290, -4.7462],\n",
       "         [-2.3545, -4.5383, -2.8907, -5.7582, -2.9687, -4.7216],\n",
       "         [-2.4107, -4.5262, -3.0591, -5.8427, -2.8044, -4.7501]],\n",
       "        device='cuda:0', grad_fn=<AddmmBackward>),\n",
       " 'accuracy': 0.9778645833333334,\n",
       " 'loss': tensor(0.0967, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(**batch)[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0965, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"label\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.mixup(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopTraining(Exception): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanWeightMonitor(Callback):\n",
    "    def on_backward_end(self, data):\n",
    "        for name, param in self.trainer.model.named_parameters():\n",
    "            if torch.isnan(param.data).any() or torch.isinf(param.data).any():\n",
    "                raise StopTraining(f\"Nan/Inf weights in param {name}: \\n {param}\")\n",
    "try:\n",
    "    import jupyter_slack\n",
    "    can_notify = True\n",
    "except:\n",
    "    jupyter_slack = None\n",
    "    can_notify = False\n",
    "\n",
    "class SlackNotification(Callback):\n",
    "    def __init__(self, silent):\n",
    "        self.silent = silent\n",
    "    def on_train_end(self, data):\n",
    "        if not self.silent and can_notify:\n",
    "            try:\n",
    "                jupyter_slack.notify_self(f\"Finished training with state {self.trainer._state}\")\n",
    "            except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorboardCallback(Callback):\n",
    "    \"\"\"For now, delegate all processing to the trainer's own methods\"\"\"\n",
    "    def on_batch_begin(self):\n",
    "        self._log_histograms_this_batch = \\\n",
    "        self.trainer._histogram_interval is not None and (\n",
    "            self.trainer._batch_num_total % self.trainer._histogram_interval == 0)\n",
    "    \n",
    "    def on_backward_end(self, loss):\n",
    "        if self._log_histograms_this_batch:\n",
    "            # get the magnitude of parameter updates for logging\n",
    "            # We need a copy of current parameters to compute magnitude of updates,\n",
    "            # and copy them to CPU so large models won't go OOM on the GPU.\n",
    "            self.param_updates = {\n",
    "                name: param.detach().cpu().clone()\n",
    "                for name, param in self.trainer.model.named_parameters()\n",
    "            }\n",
    "    \n",
    "    def on_step_end(self, loss):\n",
    "        if self._log_histograms_this_batch:\n",
    "            for name, param in self.trainer.model.named_parameters():\n",
    "                self.param_updates[name].sub_(param.detach().cpu())\n",
    "                update_norm = torch.norm(self.param_updates[name].view(-1, ))\n",
    "                param_norm = torch.norm(param.view(-1, )).cpu()\n",
    "                self.trainer._tensorboard.add_train_scalar(\n",
    "                    \"gradient_update/\" + name,\n",
    "                     update_norm / (param_norm + 1e-7),\n",
    "                     batch_num_total\n",
    "                )\n",
    "            self.param_updates = {} # release memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixup(Callback):\n",
    "    \"\"\"Does mixup in embedding space\n",
    "    TODO: Figure out how to best handle masking...\n",
    "    \"\"\"\n",
    "    def __init__(self, weight: float, \n",
    "                 batch_iterator: Optional[Iterable[TensorDict]]=None):\n",
    "        self.weight = weight\n",
    "        self.batch_iterator = batch_iterator\n",
    "        \n",
    "    def on_batch_loss(self, batch: TensorDict, for_training=True):\n",
    "        if for_training and self.weight > 0.:\n",
    "            # use mixup iterator if exists\n",
    "            if self.batch_iterator is not None: batch = next(self.batch_iterator)\n",
    "            mixup_output_dict = self.model.mixup(**batch)\n",
    "            return mixup_output_dict[\"loss\"] * self.weight\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteMixup(Callback):\n",
    "    \"\"\"Mixes up and concatenates sentences within a batch\"\"\"\n",
    "    def __init__(self, weight: float, \n",
    "                 batch_iterator: Optional[Iterable[TensorDict]]=None):\n",
    "        self.weight = weight\n",
    "        self.batch_iterator = batch_iterator\n",
    "    \n",
    "    def on_batch_loss(self, batch: TensorDict, for_training=True):\n",
    "        if for_training and self.weight > 0.:\n",
    "            # use mixup iterator if exists\n",
    "            if self.batch_iterator is not None: \n",
    "                batch = next(self.batch_iterator)\n",
    "                \n",
    "            # create permutation\n",
    "            tokens, label = batch[\"tokens\"], batch[\"label\"]\n",
    "            bs = label.size(0)\n",
    "            shuf = torch.randperm(bs).to(label.device)\n",
    "            tokens2 = permute(tokens, shuf)\n",
    "            labels2 = permute(label, shuf)\n",
    "            \n",
    "            # join the sentences\n",
    "            n_tokens1 = get_text_field_mask(tokens).sum(1)\n",
    "            n_tokens2 = get_text_field_mask(tokens2).sum(1)\n",
    "            maxlen = min(config.max_seq_len, (n_tokens1 + n_tokens2).sum())\n",
    "            # TODO: Is there a faster way?\n",
    "            new_tokens = torch.zeros(bs, maxlen, \n",
    "                                     dtype=torch.long).to(label.device)\n",
    "            for i, (t1, t2) in enumerate(zip(tokens[\"tokens\"], tokens2[\"tokens\"])):\n",
    "                l1, l2 = n_tokens1[i].item(), n_tokens2[i].item()\n",
    "                new_tokens[i, :l1] = t1 # TODO: Fairly divide the capacity\n",
    "                new_tokens[i, l1:min(maxlen, l1+l2)] = \\\n",
    "                    t2[:min(maxlen-l1, l2)]\n",
    "            \n",
    "            # compute loss on new batch\n",
    "            new_batch = {k: v for k, v in batch.items()}\n",
    "            new_batch[\"tokens\"] = {\"tokens\": new_tokens}\n",
    "            new_batch[\"label\"] = new_label\n",
    "            return model(**new_batch)[\"loss\"] * self.weight\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearEpochDropoutSchedule:\n",
    "    def __init__(self, n_epochs, start_do, end_do):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.start_do = start_do\n",
    "        self.end_do = end_do\n",
    "        if n_epochs > 1:\n",
    "            self.delta_do = (end_do - start_do) / (n_epochs - 1)\n",
    "        else:\n",
    "            self.delta_do = 0 # cannot change dropout if only one epoch\n",
    "\n",
    "    def __call__(self, epochs, batches_this_epoch, batches_total):\n",
    "        return self.start_do + self.delta_do * epochs\n",
    "\n",
    "class DropoutScheduler(Callback):\n",
    "    def __init__(self, \n",
    "                 module: nn.Module,\n",
    "                 schedule: Callable[[int, int, int], float]):\n",
    "        self._module = module\n",
    "        self._schedule = schedule\n",
    "        self.epoch = 0\n",
    "    \n",
    "    def on_batch_end(self, data):\n",
    "        self._module.dropout = self._schedule(self.epoch, \n",
    "                                              data[\"batches_this_epoch\"],\n",
    "                                              data[\"batch_num_total\"],\n",
    "                                             )\n",
    "    def on_epoch_end(self, data):\n",
    "        # handle per-epoch schedules\n",
    "        self.epoch += 1\n",
    "        self._module.dropout = self._schedule(self.epoch, -1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test performance when input is all 0s\n",
    "- If our initialization works decently, the loss should barely/not move and accuracy should stay constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training import TrainerWithCallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.debugging:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "    model.test_mode()\n",
    "    trainer = TrainerWithCallbacks(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        iterator=iterator,\n",
    "        train_dataset=train_ds[:256],\n",
    "        cuda_device=0 if USE_GPU else -1,\n",
    "        num_epochs=5,\n",
    "    )\n",
    "    metrics = trainer.train()\n",
    "    model.load_state_dict(torch.load(DATA_ROOT / \"tmp_model.pth\"))\n",
    "    model.test_mode(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test performance on a small batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.debugging:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "    state_dict = deepcopy(model.state_dict())\n",
    "    trainer = TrainerWithCallbacks(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        iterator=iterator,\n",
    "        train_dataset=train_ds[:256],\n",
    "        cuda_device=0 if USE_GPU else -1,\n",
    "        num_epochs=50,\n",
    "    )\n",
    "    metrics = trainer.train()\n",
    "    model.load_state_dict(torch.load(DATA_ROOT / \"tmp_model.pth\"))\n",
    "    metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "\n",
    "class LRFinder(Callback):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.lrs = []\n",
    "        self.best_loss = 1e9\n",
    "\n",
    "    def on_step_end(self, loss, **kwargs):\n",
    "        # Log the learning rate\n",
    "        self.losses.append(loss.item())\n",
    "        self.lrs.append(self.trainer.optimizer.state_dict()['param_groups'][0][\"lr\"])\n",
    "\n",
    "    def plot_loss(self, n_skip_beginning=10, n_skip_end=5):\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.xlabel(\"learning rate (log scale)\")\n",
    "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n",
    "        plt.xscale('log')\n",
    "\n",
    "    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, y_lim=(-0.01, 0.01)):\n",
    "        assert sma >= 1\n",
    "        derivatives = [0] * sma\n",
    "        for i in range(sma, len(self.lrs)):\n",
    "            derivative = (self.losses[i] - self.losses[i - sma]) / sma\n",
    "            derivatives.append(derivative)\n",
    "\n",
    "        plt.ylabel(\"rate of loss change\")\n",
    "        plt.xlabel(\"learning rate (log scale)\")\n",
    "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], derivatives[n_skip_beginning:-n_skip_end])\n",
    "        plt.xscale('log')\n",
    "        plt.ylim(y_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.find_lr:\n",
    "    from copy import deepcopy\n",
    "    \n",
    "    class ExponentialIncrease(torch.optim.lr_scheduler._LRScheduler):\n",
    "        def __init__(self, optimizer: torch.optim.Optimizer, n_iters: int,\n",
    "                     lr_start=1e-6, lr_end=2.0) -> None:\n",
    "            self.n_iters = n_iters\n",
    "            self.steps = 0\n",
    "            self.lr_start = lr_start\n",
    "            self.gamma = (lr_end / lr_start) ** (1 / n_iters)\n",
    "            super().__init__(optimizer)\n",
    "        def step(self, epoch=None): pass\n",
    "        def step_batch(self, epoch=None):\n",
    "            self.steps += 1\n",
    "            if epoch is None: epoch = self.last_epoch + 1\n",
    "            self.last_epoch = epoch\n",
    "            for param_group, learning_rate in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "                param_group['lr'] = learning_rate\n",
    "        def get_lr(self):\n",
    "            return [self.lr_start * (self.gamma ** self.steps) for _ in self.base_lrs]\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
    "    lr_finder = LRFinder()\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        iterator=iterator,\n",
    "        train_dataset=train_ds,\n",
    "        learning_rate_scheduler=ExponentialIncrease(optimizer, \n",
    "                                                    iterator.get_num_batches(train_ds)),\n",
    "        cuda_device=0 if USE_GPU else -1,\n",
    "        num_epochs=1,\n",
    "        callbacks=[lr_finder],\n",
    "    )\n",
    "    trainer.train()\n",
    "    model.load_state_dict(torch.load(DATA_ROOT / \"tmp_model.pth\"))\n",
    "    del model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.find_lr:\n",
    "    lr_finder.plot_loss(n_skip_beginning=0, n_skip_end=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(DATA_ROOT / \"tmp_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"elmo\" in config.model_type:\n",
    "    # apply a small amount of l2 regularization to scalar params as recommended\n",
    "    # here: https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md\n",
    "    pgroup_1 = [p for nm, p in model.named_parameters() if \"scalar_parameters\" not in nm and p.requires_grad]\n",
    "    pgroup_2 = [p for nm, p in model.named_parameters() if \"scalar_parameters\" in nm]\n",
    "    optimizer = optim.Adam([{\"params\": pgroup_1}, {\"params\": pgroup_2, \"weight_decay\": 0.001}], lr=config.lr, weight_decay=config.weight_decay)\n",
    "else:\n",
    "    optimizer = optim.Adam(model.parameters(), \n",
    "                           lr=config.lr, weight_decay=config.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1006437"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _prod(args):\n",
    "    acc = 1\n",
    "    for a in args: acc *= a\n",
    "    return acc\n",
    "num_trainable_params = sum([_prod(p.shape) for p in model.parameters() if p.requires_grad])\n",
    "num_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training.learning_rate_schedulers import SlantedTriangular, CosineWithRestarts\n",
    "if config.lr_schedule == \"slanted_triangular\":\n",
    "    lr_sched = SlantedTriangular(optimizer, \n",
    "                                 num_epochs=config.epochs, \n",
    "                                 num_steps_per_epoch=iterator.get_num_batches(train_ds))\n",
    "elif config.lr_scheduler == \"cosine_annealing\":\n",
    "    lr_sched = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=iterator.get_num_batches(train_ds) * config.epochs,\n",
    "    )\n",
    "elif config.lr_scheduler is None:\n",
    "    lr_sched = None\n",
    "else:\n",
    "    raise ConfigurationError(f\"Invalid lr schedule {config.lr_scheduler} passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_options = {\n",
    "    # TODO: Add appropriate learning rate scheduler\n",
    "    \"should_log_parameter_statistics\": True,\n",
    "    \"should_log_learning_rate\": True,\n",
    "    \"num_epochs\": config.epochs,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [NanWeightMonitor(), \n",
    "             SlackNotification(silent=config.testing)]\n",
    "if config.dropoute_max is not None:\n",
    "    callbacks.append(DropoutScheduler(\n",
    "        model.word_embeddings.token_embedder_tokens,\n",
    "        LinearEpochDropoutSchedule(config.epochs, config.dropoute, config.dropoute_max)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.environ[\"IS_COLAB\"] != \"True\" and not config.testing):\n",
    "    SER_DIR = DATA_ROOT / \"ckpts\" / RUN_ID\n",
    "else:\n",
    "    SER_DIR = None\n",
    "\n",
    "trainer = TrainerWithCallbacks(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    iterator=iterator,\n",
    "    train_dataset=train_ds + train_aug_ds if config.use_augmented else train_ds,\n",
    "    validation_dataset=val_ds if config.val_ratio > 0.0 else None,\n",
    "    serialization_dir=SER_DIR,\n",
    "    cuda_device=0 if USE_GPU else -1,\n",
    "    gradient_accumulation_steps=config.batch_size // config.computational_batch_size,\n",
    "    callbacks=callbacks,\n",
    "    learning_rate_scheduler=lr_sched,\n",
    "    **training_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9781, loss: 0.0609 ||: 100%|██████████| 1247/1247 [05:44<00:00,  4.16it/s]\n",
      "accuracy: 0.9815, loss: 0.0477 ||: 100%|██████████| 1247/1247 [05:23<00:00,  3.60it/s]\n",
      "accuracy: 0.9828, loss: 0.0441 ||: 100%|██████████| 1247/1247 [05:24<00:00,  3.71it/s]\n",
      "accuracy: 0.9841, loss: 0.0397 ||: 100%|██████████| 1247/1247 [05:25<00:00,  3.75it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 3,\n",
       " 'peak_cpu_memory_MB': 5494.148,\n",
       " 'peak_gpu_0_memory_MB': 10765,\n",
       " 'training_duration': '00:22:04',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 3,\n",
       " 'epoch': 3,\n",
       " 'training_accuracy': 0.984087543058158,\n",
       " 'training_loss': 0.03973933342887565,\n",
       " 'training_cpu_memory_MB': 5494.148,\n",
       " 'training_gpu_0_memory_MB': 10765}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('word_embeddings.token_embedder_tokens.weight', Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1111, -0.0014, -0.1778,  ...,  0.0634, -0.1216,  0.0393],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0'))\n",
      "('encoder.rnn.rnns.0._module.weight_ih_l0', Parameter containing:\n",
      "tensor([[-0.0148, -0.2024, -0.1957,  ..., -0.0874,  0.0425,  0.0464],\n",
      "        [ 0.1560, -0.2544, -0.0314,  ..., -0.0601,  0.0942, -0.3170],\n",
      "        [ 0.1502, -0.1211, -0.0022,  ..., -0.0678, -0.1003, -0.0496],\n",
      "        ...,\n",
      "        [ 0.0070, -0.3101,  0.2812,  ..., -0.0555, -0.0425,  0.0005],\n",
      "        [-0.0180, -0.1727,  0.0309,  ..., -0.0645, -0.1030, -0.0978],\n",
      "        [ 0.2370,  0.0748, -0.0191,  ...,  0.0973, -0.0917,  0.3764]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.weight_hh_l0', Parameter containing:\n",
      "tensor([[-0.0217, -0.0115, -0.0793,  ...,  0.0101,  0.0445, -0.0039],\n",
      "        [ 0.0264, -0.0092, -0.0220,  ...,  0.0529, -0.0911, -0.1217],\n",
      "        [-0.0164, -0.0078, -0.0243,  ..., -0.0038, -0.0941,  0.0985],\n",
      "        ...,\n",
      "        [ 0.0759, -0.0415, -0.1399,  ..., -0.0018, -0.0342,  0.2430],\n",
      "        [ 0.0153, -0.0906, -0.0696,  ..., -0.0090, -0.0215, -0.0067],\n",
      "        [ 0.0566, -0.1314, -0.2096,  ...,  0.0849,  0.0922,  0.2329]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_ih_l0', Parameter containing:\n",
      "tensor([-3.0099e-02, -8.8367e-02, -1.1904e-01, -1.3363e-01, -1.7939e-01,\n",
      "        -1.0041e-01, -6.6087e-02, -1.1786e-01, -7.7164e-02, -5.0369e-02,\n",
      "        -1.9816e-01, -1.5740e-01, -8.4144e-02, -1.9587e-01, -7.9442e-02,\n",
      "        -9.1442e-02, -1.5942e-01, -6.6777e-02, -6.5170e-02, -2.2313e-02,\n",
      "        -1.0904e-01, -9.0088e-02, -1.0681e-01, -4.8370e-02, -4.3007e-02,\n",
      "        -8.3249e-02, -2.4372e-02, -1.0728e-01, -1.3397e-01, -5.2685e-02,\n",
      "        -4.7181e-02, -9.9396e-02, -1.0180e-01, -6.7464e-02, -1.2156e-01,\n",
      "        -1.0569e-01, -9.9519e-02, -3.1821e-02, -8.0916e-02, -6.5652e-02,\n",
      "        -1.1707e-01, -4.2758e-02, -6.1732e-02, -9.9318e-02, -1.0507e-01,\n",
      "        -1.0495e-01, -9.2788e-02, -9.3884e-02, -3.7128e-02, -7.9436e-02,\n",
      "        -8.8310e-02, -7.7755e-02,  2.7896e-03, -7.9066e-02, -7.9659e-03,\n",
      "        -1.1790e-01, -8.8144e-02, -1.0864e-01, -2.1798e-01, -7.1761e-02,\n",
      "        -5.7040e-02, -3.7779e-02, -9.1795e-02, -1.0867e-01, -9.5217e-02,\n",
      "        -7.4970e-02, -1.1112e-01, -1.5120e-01, -5.8751e-02, -6.6798e-02,\n",
      "        -3.0685e-02, -1.2958e-01, -9.3459e-02, -1.8892e-01, -1.3500e-01,\n",
      "        -7.3967e-02, -1.3860e-01, -1.1898e-01, -1.3018e-01, -2.4142e-02,\n",
      "        -1.0517e-01, -2.0741e-01, -2.6280e-01, -1.3006e-01, -7.1005e-02,\n",
      "        -7.7142e-02, -5.0738e-02, -4.3041e-02, -1.9944e-01, -1.2092e-01,\n",
      "        -5.9756e-02, -1.0553e-01, -1.5819e-01, -7.8308e-02, -7.2909e-02,\n",
      "        -4.3850e-02, -4.3706e-02, -8.1550e-02, -1.5914e-01, -8.7369e-02,\n",
      "        -1.7404e-01, -8.9674e-02, -1.6780e-01, -9.2946e-02, -2.5644e-01,\n",
      "        -4.6896e-02, -8.1815e-02, -5.0242e-02, -1.0565e-01, -4.9504e-02,\n",
      "        -7.0196e-02, -1.6323e-01, -8.2493e-02, -1.2498e-01, -1.0050e-01,\n",
      "        -1.2015e-01, -8.0996e-02, -1.1801e-01, -3.8362e-02, -1.6460e-01,\n",
      "        -5.8383e-02,  1.2501e-02, -5.3703e-02,  1.0010e-02, -1.8964e-01,\n",
      "        -8.9500e-02, -1.0203e-01, -1.6384e-01,  8.9583e-01,  9.5303e-01,\n",
      "         9.5609e-01,  1.1322e+00,  1.2417e+00,  1.0671e+00,  9.9961e-01,\n",
      "         1.0020e+00,  1.0502e+00,  9.8678e-01,  1.1449e+00,  1.1181e+00,\n",
      "         9.7687e-01,  1.0457e+00,  9.1638e-01,  1.0253e+00,  8.3581e-01,\n",
      "         9.3465e-01,  9.3172e-01,  9.2299e-01,  1.0222e+00,  9.4751e-01,\n",
      "         8.9437e-01,  9.7274e-01,  9.8820e-01,  9.2477e-01,  9.1218e-01,\n",
      "         9.6715e-01,  9.3618e-01,  9.2626e-01,  9.8919e-01,  9.6073e-01,\n",
      "         9.6518e-01,  9.3030e-01,  9.0363e-01,  9.0175e-01,  9.7266e-01,\n",
      "         1.2462e+00,  9.7534e-01,  1.0210e+00,  9.6764e-01,  8.5791e-01,\n",
      "         9.9701e-01,  1.0087e+00,  9.1663e-01,  9.4440e-01,  1.0617e+00,\n",
      "         9.5303e-01,  8.7967e-01,  9.5039e-01,  9.6020e-01,  9.5591e-01,\n",
      "         8.7597e-01,  1.1430e+00,  9.7008e-01,  1.0102e+00,  1.1062e+00,\n",
      "         9.7958e-01,  9.5897e-01,  9.1100e-01,  8.9833e-01,  9.8968e-01,\n",
      "         9.4821e-01,  9.0927e-01,  9.9213e-01,  1.1058e+00,  9.3918e-01,\n",
      "         9.8612e-01,  9.7384e-01,  9.9189e-01,  9.2694e-01,  9.5446e-01,\n",
      "         9.9207e-01,  1.0503e+00,  1.0659e+00,  9.3625e-01,  1.0307e+00,\n",
      "         1.0242e+00,  9.7667e-01,  9.9754e-01,  9.8352e-01,  9.6462e-01,\n",
      "         1.0524e+00,  9.7561e-01,  1.0354e+00,  9.4594e-01,  1.0047e+00,\n",
      "         9.3076e-01,  9.7074e-01,  9.4404e-01,  9.0260e-01,  1.0601e+00,\n",
      "         1.0333e+00,  1.0992e+00,  1.1068e+00,  9.8715e-01,  1.0198e+00,\n",
      "         8.6920e-01,  9.3857e-01,  8.9495e-01,  9.0619e-01,  9.8652e-01,\n",
      "         9.5215e-01,  9.6085e-01,  1.0839e+00,  9.7484e-01,  9.3575e-01,\n",
      "         1.0382e+00,  9.2664e-01,  1.0558e+00,  9.6721e-01,  1.1211e+00,\n",
      "         8.8713e-01,  9.9981e-01,  9.2341e-01,  1.0144e+00,  9.2795e-01,\n",
      "         9.7130e-01,  1.0334e+00,  1.0982e+00,  9.9090e-01,  9.5840e-01,\n",
      "         9.2901e-01,  8.3271e-01,  8.6732e-01,  1.0694e+00,  9.2910e-01,\n",
      "         9.9615e-01,  5.6463e-02, -3.0762e-02, -4.0296e-02, -1.6551e-03,\n",
      "        -2.4582e-02, -2.0356e-02, -2.9149e-02, -2.0202e-02, -6.3271e-02,\n",
      "         5.5673e-02,  1.9953e-02, -2.9145e-03,  4.5947e-02,  2.0114e-02,\n",
      "        -3.7872e-02, -7.4469e-03,  3.1152e-02,  1.2393e-02,  1.0032e-02,\n",
      "         2.0466e-02, -7.1528e-03, -3.9856e-02, -1.6172e-02,  2.7692e-02,\n",
      "        -1.9449e-02,  2.6537e-02,  3.2456e-02,  8.6021e-02, -1.7427e-02,\n",
      "        -7.4899e-02, -7.1488e-02, -1.9537e-03, -4.8470e-02,  2.9578e-03,\n",
      "        -5.1787e-02, -2.7350e-02, -1.8364e-02, -3.3774e-02, -3.6384e-02,\n",
      "        -2.1718e-02, -1.0540e-02,  3.4350e-02,  2.1532e-02,  3.6314e-03,\n",
      "         4.6436e-02,  4.7523e-02, -3.9024e-02, -3.3153e-02,  1.1143e-02,\n",
      "         4.2672e-02,  6.0973e-02,  1.8944e-02,  2.1012e-02, -3.7828e-03,\n",
      "         3.5001e-03, -2.2911e-02, -3.5210e-02,  1.7014e-02,  4.1207e-02,\n",
      "        -4.0604e-02, -2.3150e-02,  1.0541e-02,  3.2970e-02,  6.0174e-03,\n",
      "         7.5077e-03,  1.2714e-02, -3.4102e-03, -3.3707e-02,  1.3774e-03,\n",
      "        -3.7632e-02,  7.1354e-02, -8.0078e-03,  2.1433e-02,  2.1698e-02,\n",
      "        -7.1451e-03,  3.2432e-02, -5.1561e-03,  3.9605e-03,  3.5634e-02,\n",
      "         2.4647e-02, -2.4095e-02, -1.6382e-02,  2.7953e-02, -5.3943e-03,\n",
      "         1.1968e-02, -3.4789e-02, -2.3557e-03, -2.6193e-02, -1.3869e-02,\n",
      "        -2.2701e-02,  1.0906e-03,  5.6243e-03,  3.4249e-04, -3.3371e-02,\n",
      "         2.5330e-02,  2.5433e-02,  3.0982e-02, -2.0393e-02, -1.4759e-02,\n",
      "         2.0017e-02,  5.6465e-02,  8.2804e-02,  3.4145e-02,  2.0576e-02,\n",
      "         1.8074e-02,  2.4437e-02,  9.7141e-03,  4.8765e-02, -3.5733e-02,\n",
      "        -2.8198e-02, -7.0794e-02, -3.7391e-02,  2.0101e-03, -1.5451e-02,\n",
      "        -5.7724e-03,  1.3284e-02, -3.3724e-02,  2.0705e-02,  8.9909e-02,\n",
      "        -2.1395e-02, -7.4685e-03,  6.6585e-03,  2.3567e-02,  4.1409e-03,\n",
      "        -3.8084e-02,  1.1453e-02, -3.5931e-02, -2.4519e-02, -8.4085e-02,\n",
      "        -8.8019e-02, -1.0372e-01, -9.2175e-02, -3.7110e-02, -1.6225e-02,\n",
      "        -3.0096e-02, -6.3849e-02, -3.7534e-02, -2.5088e-02, -9.3941e-02,\n",
      "        -5.6099e-02, -3.6913e-02, -8.7161e-02, -5.8957e-02, -1.0941e-01,\n",
      "        -1.1972e-01, -5.9592e-02, -8.6965e-02, -9.2732e-02, -9.3693e-02,\n",
      "        -6.6785e-02, -6.6307e-02, -2.5950e-02, -6.5901e-02, -9.2897e-02,\n",
      "         6.0604e-03, -4.6808e-02, -1.1890e-01, -4.3475e-02,  4.1510e-02,\n",
      "        -3.6531e-02, -5.9480e-02, -1.0677e-02, -1.1525e-01, -9.5471e-02,\n",
      "        -4.1282e-02, -8.1596e-03, -4.9193e-02, -9.5521e-02, -5.7448e-02,\n",
      "        -3.4892e-02, -2.6282e-02,  5.6305e-03, -7.0756e-02, -8.8192e-02,\n",
      "        -5.2613e-02, -7.1480e-02, -8.3024e-02, -8.4708e-02,  5.3367e-02,\n",
      "        -3.2035e-02,  2.6310e-02, -8.2050e-02,  6.4835e-02, -1.0615e-01,\n",
      "        -1.2411e-01, -8.9719e-02, -1.1896e-02, -2.6031e-02, -3.7953e-02,\n",
      "        -2.2181e-02, -7.4437e-02, -1.0713e-01, -2.1847e-03, -4.8399e-02,\n",
      "        -6.3015e-02,  1.3467e-01,  1.0022e-01, -7.7953e-02, -1.4585e-02,\n",
      "        -9.5226e-02, -5.3825e-02, -3.7841e-02, -4.3155e-02, -5.6267e-02,\n",
      "        -8.1262e-02, -4.7395e-02, -1.6721e-01, -3.3864e-02, -9.6671e-04,\n",
      "         3.4721e-02, -1.7680e-02, -3.8176e-02, -5.4155e-02, -7.6622e-02,\n",
      "        -2.5638e-02, -2.6729e-02, -9.0520e-02, -1.2034e-01, -1.0942e-01,\n",
      "        -1.0702e-01, -1.5469e-01,  1.3542e-03, -7.0728e-02, -3.7776e-02,\n",
      "        -2.0649e-02, -5.1187e-02,  7.6014e-02, -5.5916e-02, -1.3204e-01,\n",
      "        -7.3878e-02, -1.5920e-01, -3.4707e-02,  3.9763e-02, -1.0900e-02,\n",
      "        -5.4916e-02, -3.2853e-02, -9.3979e-02, -9.5986e-02, -9.4734e-02,\n",
      "        -4.8531e-02, -8.2240e-02, -1.1301e-01, -8.4583e-02, -6.5219e-02,\n",
      "        -2.0244e-02, -5.4015e-02,  5.6119e-02, -1.3394e-01, -4.6452e-02,\n",
      "         3.6145e-02, -3.7986e-02, -9.2982e-04, -1.3029e-01, -4.3019e-02,\n",
      "        -1.3941e-01, -3.7650e-02], device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_hh_l0', Parameter containing:\n",
      "tensor([-3.0099e-02, -8.8367e-02, -1.1904e-01, -1.3363e-01, -1.7939e-01,\n",
      "        -1.0041e-01, -6.6087e-02, -1.1786e-01, -7.7164e-02, -5.0369e-02,\n",
      "        -1.9816e-01, -1.5740e-01, -8.4144e-02, -1.9587e-01, -7.9442e-02,\n",
      "        -9.1442e-02, -1.5942e-01, -6.6777e-02, -6.5170e-02, -2.2313e-02,\n",
      "        -1.0904e-01, -9.0088e-02, -1.0681e-01, -4.8370e-02, -4.3007e-02,\n",
      "        -8.3249e-02, -2.4372e-02, -1.0728e-01, -1.3397e-01, -5.2685e-02,\n",
      "        -4.7181e-02, -9.9396e-02, -1.0180e-01, -6.7464e-02, -1.2156e-01,\n",
      "        -1.0569e-01, -9.9519e-02, -3.1821e-02, -8.0916e-02, -6.5652e-02,\n",
      "        -1.1707e-01, -4.2758e-02, -6.1732e-02, -9.9318e-02, -1.0507e-01,\n",
      "        -1.0495e-01, -9.2788e-02, -9.3884e-02, -3.7128e-02, -7.9436e-02,\n",
      "        -8.8310e-02, -7.7755e-02,  2.7896e-03, -7.9066e-02, -7.9659e-03,\n",
      "        -1.1790e-01, -8.8144e-02, -1.0864e-01, -2.1798e-01, -7.1761e-02,\n",
      "        -5.7040e-02, -3.7779e-02, -9.1795e-02, -1.0867e-01, -9.5217e-02,\n",
      "        -7.4970e-02, -1.1112e-01, -1.5120e-01, -5.8751e-02, -6.6798e-02,\n",
      "        -3.0685e-02, -1.2958e-01, -9.3459e-02, -1.8892e-01, -1.3500e-01,\n",
      "        -7.3967e-02, -1.3860e-01, -1.1898e-01, -1.3018e-01, -2.4142e-02,\n",
      "        -1.0517e-01, -2.0741e-01, -2.6280e-01, -1.3006e-01, -7.1005e-02,\n",
      "        -7.7142e-02, -5.0738e-02, -4.3041e-02, -1.9944e-01, -1.2092e-01,\n",
      "        -5.9756e-02, -1.0553e-01, -1.5819e-01, -7.8308e-02, -7.2909e-02,\n",
      "        -4.3850e-02, -4.3706e-02, -8.1550e-02, -1.5914e-01, -8.7369e-02,\n",
      "        -1.7404e-01, -8.9674e-02, -1.6780e-01, -9.2946e-02, -2.5644e-01,\n",
      "        -4.6896e-02, -8.1815e-02, -5.0242e-02, -1.0565e-01, -4.9504e-02,\n",
      "        -7.0196e-02, -1.6323e-01, -8.2493e-02, -1.2498e-01, -1.0050e-01,\n",
      "        -1.2015e-01, -8.0996e-02, -1.1801e-01, -3.8362e-02, -1.6460e-01,\n",
      "        -5.8383e-02,  1.2501e-02, -5.3703e-02,  1.0010e-02, -1.8964e-01,\n",
      "        -8.9500e-02, -1.0203e-01, -1.6384e-01,  8.9583e-01,  9.5303e-01,\n",
      "         9.5609e-01,  1.1322e+00,  1.2417e+00,  1.0671e+00,  9.9961e-01,\n",
      "         1.0020e+00,  1.0502e+00,  9.8678e-01,  1.1449e+00,  1.1181e+00,\n",
      "         9.7687e-01,  1.0457e+00,  9.1638e-01,  1.0253e+00,  8.3581e-01,\n",
      "         9.3465e-01,  9.3172e-01,  9.2299e-01,  1.0222e+00,  9.4751e-01,\n",
      "         8.9437e-01,  9.7274e-01,  9.8820e-01,  9.2477e-01,  9.1218e-01,\n",
      "         9.6715e-01,  9.3618e-01,  9.2626e-01,  9.8919e-01,  9.6073e-01,\n",
      "         9.6518e-01,  9.3030e-01,  9.0363e-01,  9.0175e-01,  9.7266e-01,\n",
      "         1.2462e+00,  9.7534e-01,  1.0210e+00,  9.6764e-01,  8.5791e-01,\n",
      "         9.9701e-01,  1.0087e+00,  9.1663e-01,  9.4440e-01,  1.0617e+00,\n",
      "         9.5303e-01,  8.7967e-01,  9.5039e-01,  9.6020e-01,  9.5591e-01,\n",
      "         8.7597e-01,  1.1430e+00,  9.7008e-01,  1.0102e+00,  1.1062e+00,\n",
      "         9.7958e-01,  9.5897e-01,  9.1100e-01,  8.9833e-01,  9.8968e-01,\n",
      "         9.4821e-01,  9.0927e-01,  9.9213e-01,  1.1058e+00,  9.3918e-01,\n",
      "         9.8612e-01,  9.7384e-01,  9.9189e-01,  9.2694e-01,  9.5446e-01,\n",
      "         9.9207e-01,  1.0503e+00,  1.0659e+00,  9.3625e-01,  1.0307e+00,\n",
      "         1.0242e+00,  9.7667e-01,  9.9754e-01,  9.8352e-01,  9.6462e-01,\n",
      "         1.0524e+00,  9.7561e-01,  1.0354e+00,  9.4594e-01,  1.0047e+00,\n",
      "         9.3076e-01,  9.7074e-01,  9.4404e-01,  9.0260e-01,  1.0601e+00,\n",
      "         1.0333e+00,  1.0992e+00,  1.1068e+00,  9.8715e-01,  1.0198e+00,\n",
      "         8.6920e-01,  9.3857e-01,  8.9495e-01,  9.0619e-01,  9.8652e-01,\n",
      "         9.5215e-01,  9.6085e-01,  1.0839e+00,  9.7484e-01,  9.3575e-01,\n",
      "         1.0382e+00,  9.2664e-01,  1.0558e+00,  9.6721e-01,  1.1211e+00,\n",
      "         8.8713e-01,  9.9981e-01,  9.2341e-01,  1.0144e+00,  9.2795e-01,\n",
      "         9.7130e-01,  1.0334e+00,  1.0982e+00,  9.9090e-01,  9.5840e-01,\n",
      "         9.2901e-01,  8.3271e-01,  8.6732e-01,  1.0694e+00,  9.2910e-01,\n",
      "         9.9615e-01,  5.6463e-02, -3.0762e-02, -4.0296e-02, -1.6551e-03,\n",
      "        -2.4582e-02, -2.0356e-02, -2.9149e-02, -2.0202e-02, -6.3271e-02,\n",
      "         5.5673e-02,  1.9953e-02, -2.9145e-03,  4.5947e-02,  2.0114e-02,\n",
      "        -3.7872e-02, -7.4469e-03,  3.1152e-02,  1.2393e-02,  1.0032e-02,\n",
      "         2.0466e-02, -7.1528e-03, -3.9856e-02, -1.6172e-02,  2.7692e-02,\n",
      "        -1.9449e-02,  2.6537e-02,  3.2456e-02,  8.6021e-02, -1.7427e-02,\n",
      "        -7.4899e-02, -7.1488e-02, -1.9537e-03, -4.8470e-02,  2.9578e-03,\n",
      "        -5.1787e-02, -2.7350e-02, -1.8364e-02, -3.3774e-02, -3.6384e-02,\n",
      "        -2.1718e-02, -1.0540e-02,  3.4350e-02,  2.1532e-02,  3.6314e-03,\n",
      "         4.6436e-02,  4.7523e-02, -3.9024e-02, -3.3153e-02,  1.1143e-02,\n",
      "         4.2672e-02,  6.0973e-02,  1.8944e-02,  2.1012e-02, -3.7828e-03,\n",
      "         3.5001e-03, -2.2911e-02, -3.5210e-02,  1.7014e-02,  4.1207e-02,\n",
      "        -4.0604e-02, -2.3150e-02,  1.0541e-02,  3.2970e-02,  6.0174e-03,\n",
      "         7.5077e-03,  1.2714e-02, -3.4102e-03, -3.3707e-02,  1.3774e-03,\n",
      "        -3.7632e-02,  7.1354e-02, -8.0078e-03,  2.1433e-02,  2.1698e-02,\n",
      "        -7.1451e-03,  3.2432e-02, -5.1561e-03,  3.9605e-03,  3.5634e-02,\n",
      "         2.4647e-02, -2.4095e-02, -1.6382e-02,  2.7953e-02, -5.3943e-03,\n",
      "         1.1968e-02, -3.4789e-02, -2.3557e-03, -2.6193e-02, -1.3869e-02,\n",
      "        -2.2701e-02,  1.0906e-03,  5.6243e-03,  3.4249e-04, -3.3371e-02,\n",
      "         2.5330e-02,  2.5433e-02,  3.0982e-02, -2.0393e-02, -1.4759e-02,\n",
      "         2.0017e-02,  5.6465e-02,  8.2804e-02,  3.4145e-02,  2.0576e-02,\n",
      "         1.8074e-02,  2.4437e-02,  9.7141e-03,  4.8765e-02, -3.5733e-02,\n",
      "        -2.8198e-02, -7.0794e-02, -3.7391e-02,  2.0101e-03, -1.5451e-02,\n",
      "        -5.7724e-03,  1.3284e-02, -3.3724e-02,  2.0705e-02,  8.9909e-02,\n",
      "        -2.1395e-02, -7.4685e-03,  6.6585e-03,  2.3567e-02,  4.1409e-03,\n",
      "        -3.8084e-02,  1.1453e-02, -3.5931e-02, -2.4519e-02, -8.4085e-02,\n",
      "        -8.8019e-02, -1.0372e-01, -9.2175e-02, -3.7110e-02, -1.6225e-02,\n",
      "        -3.0096e-02, -6.3849e-02, -3.7534e-02, -2.5088e-02, -9.3941e-02,\n",
      "        -5.6099e-02, -3.6913e-02, -8.7161e-02, -5.8957e-02, -1.0941e-01,\n",
      "        -1.1972e-01, -5.9592e-02, -8.6965e-02, -9.2732e-02, -9.3693e-02,\n",
      "        -6.6785e-02, -6.6307e-02, -2.5950e-02, -6.5901e-02, -9.2897e-02,\n",
      "         6.0604e-03, -4.6808e-02, -1.1890e-01, -4.3475e-02,  4.1510e-02,\n",
      "        -3.6531e-02, -5.9480e-02, -1.0677e-02, -1.1525e-01, -9.5471e-02,\n",
      "        -4.1282e-02, -8.1596e-03, -4.9193e-02, -9.5521e-02, -5.7448e-02,\n",
      "        -3.4892e-02, -2.6282e-02,  5.6305e-03, -7.0756e-02, -8.8192e-02,\n",
      "        -5.2613e-02, -7.1480e-02, -8.3024e-02, -8.4708e-02,  5.3367e-02,\n",
      "        -3.2035e-02,  2.6310e-02, -8.2050e-02,  6.4835e-02, -1.0615e-01,\n",
      "        -1.2411e-01, -8.9719e-02, -1.1896e-02, -2.6031e-02, -3.7953e-02,\n",
      "        -2.2181e-02, -7.4437e-02, -1.0713e-01, -2.1847e-03, -4.8399e-02,\n",
      "        -6.3015e-02,  1.3467e-01,  1.0022e-01, -7.7953e-02, -1.4585e-02,\n",
      "        -9.5226e-02, -5.3825e-02, -3.7841e-02, -4.3155e-02, -5.6267e-02,\n",
      "        -8.1262e-02, -4.7395e-02, -1.6721e-01, -3.3864e-02, -9.6671e-04,\n",
      "         3.4721e-02, -1.7680e-02, -3.8176e-02, -5.4155e-02, -7.6622e-02,\n",
      "        -2.5638e-02, -2.6729e-02, -9.0520e-02, -1.2034e-01, -1.0942e-01,\n",
      "        -1.0702e-01, -1.5469e-01,  1.3542e-03, -7.0728e-02, -3.7776e-02,\n",
      "        -2.0649e-02, -5.1187e-02,  7.6014e-02, -5.5916e-02, -1.3204e-01,\n",
      "        -7.3878e-02, -1.5920e-01, -3.4707e-02,  3.9763e-02, -1.0900e-02,\n",
      "        -5.4916e-02, -3.2853e-02, -9.3979e-02, -9.5986e-02, -9.4734e-02,\n",
      "        -4.8531e-02, -8.2240e-02, -1.1301e-01, -8.4583e-02, -6.5219e-02,\n",
      "        -2.0244e-02, -5.4015e-02,  5.6119e-02, -1.3394e-01, -4.6452e-02,\n",
      "         3.6145e-02, -3.7986e-02, -9.2982e-04, -1.3029e-01, -4.3019e-02,\n",
      "        -1.3941e-01, -3.7650e-02], device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.weight_ih_l0_reverse', Parameter containing:\n",
      "tensor([[ 0.1512, -0.2788, -0.3342,  ..., -0.0895, -0.0965, -0.0388],\n",
      "        [ 0.1254, -0.0963, -0.2859,  ..., -0.0718,  0.0631,  0.2983],\n",
      "        [ 0.0135, -0.0581, -0.0285,  ...,  0.0676, -0.0845, -0.2349],\n",
      "        ...,\n",
      "        [ 0.0617, -0.1297,  0.0962,  ..., -0.0542, -0.0175,  0.0168],\n",
      "        [ 0.2837,  0.0482,  0.0437,  ...,  0.0270,  0.3281, -0.2592],\n",
      "        [ 0.0795, -0.1992, -0.0945,  ..., -0.0390, -0.0702,  0.0614]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.weight_hh_l0_reverse', Parameter containing:\n",
      "tensor([[ 0.2002,  0.1124,  0.0606,  ...,  0.2583,  0.1489,  0.0557],\n",
      "        [-0.0382, -0.2372, -0.1620,  ...,  0.1300, -0.0985, -0.1760],\n",
      "        [ 0.1378, -0.0420,  0.0307,  ...,  0.1305,  0.2597, -0.0321],\n",
      "        ...,\n",
      "        [-0.0658,  0.0362, -0.0198,  ...,  0.0319,  0.0189, -0.0593],\n",
      "        [-0.1154,  0.1246,  0.0229,  ...,  0.0296,  0.1935,  0.1286],\n",
      "        [-0.0223, -0.0271,  0.0381,  ...,  0.0492, -0.0243,  0.0211]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_ih_l0_reverse', Parameter containing:\n",
      "tensor([-1.9066e-01, -1.1691e-01, -5.3584e-02, -1.0994e-01, -1.3145e-01,\n",
      "        -4.0441e-02, -9.7780e-02, -1.8854e-01, -9.6097e-02, -8.9907e-02,\n",
      "        -1.7790e-01, -1.3019e-01, -5.2310e-04, -5.8973e-02, -1.0688e-01,\n",
      "        -1.8176e-01, -1.0032e-01, -8.7740e-02, -8.3766e-02, -1.7463e-01,\n",
      "        -1.8156e-01, -6.9478e-02, -2.8854e-01, -6.9956e-02, -1.3848e-01,\n",
      "        -1.8509e-01, -2.1467e-02, -8.2978e-02, -6.0315e-02, -1.2975e-01,\n",
      "        -1.4541e-01, -7.9534e-02, -1.2929e-01, -2.0177e-01, -7.7616e-02,\n",
      "        -1.4835e-01, -2.2845e-01, -1.0793e-01, -1.2142e-01, -1.3890e-01,\n",
      "        -1.4462e-01, -2.0988e-01, -1.5167e-01, -1.2112e-01, -7.7304e-02,\n",
      "        -7.2497e-02, -8.0048e-02, -2.5499e-01, -2.2103e-02, -1.5579e-01,\n",
      "        -6.3705e-02, -6.6581e-02, -1.2612e-01, -8.8395e-02, -1.5539e-01,\n",
      "        -6.4373e-02, -2.2380e-01, -2.1426e-02, -1.6581e-01, -1.7347e-01,\n",
      "        -4.1875e-02, -2.3440e-01, -6.1865e-02, -1.3448e-01, -9.3503e-02,\n",
      "        -2.9457e-01, -1.2725e-01, -6.9692e-02, -1.9211e-01, -1.0847e-01,\n",
      "        -4.9706e-02, -9.3441e-02, -7.3358e-02, -1.7055e-01, -1.2805e-01,\n",
      "        -1.2443e-01, -1.5218e-01, -1.4182e-01, -2.0001e-01,  9.9461e-03,\n",
      "        -1.1484e-01, -1.4171e-01, -8.2530e-02, -9.6967e-02, -4.8440e-02,\n",
      "        -7.0491e-02, -1.3257e-01, -1.5344e-01, -1.1931e-01, -1.2553e-01,\n",
      "        -1.8516e-01, -2.2000e-01, -1.3251e-01, -1.5166e-01, -1.3497e-01,\n",
      "        -1.5948e-01, -1.5945e-01, -8.4051e-02, -1.4338e-01, -8.1696e-02,\n",
      "        -2.3348e-01, -9.2535e-02, -2.3143e-01, -1.1227e-01, -1.6230e-01,\n",
      "        -5.1706e-02, -1.6580e-01, -7.7801e-02, -1.4756e-01, -1.2483e-01,\n",
      "        -1.0225e-01, -1.3558e-01, -1.0928e-01, -8.9775e-02, -2.7500e-02,\n",
      "        -5.5037e-02, -2.2988e-01, -1.6522e-01, -7.6701e-02, -1.7371e-01,\n",
      "        -7.4756e-02, -2.3463e-01, -1.4293e-01, -1.4941e-01, -1.2748e-01,\n",
      "        -9.5639e-02,  1.0958e-02, -1.2552e-01,  1.0273e+00,  1.0261e+00,\n",
      "         8.9623e-01,  1.0050e+00,  9.4506e-01,  1.0303e+00,  9.3197e-01,\n",
      "         8.5493e-01,  8.9594e-01,  9.4826e-01,  9.3688e-01,  8.9474e-01,\n",
      "         8.4777e-01,  8.9757e-01,  9.7078e-01,  9.2283e-01,  9.6907e-01,\n",
      "         9.9921e-01,  1.0255e+00,  9.3569e-01,  9.3675e-01,  9.5762e-01,\n",
      "         1.1316e+00,  9.4023e-01,  9.5464e-01,  9.2859e-01,  8.9678e-01,\n",
      "         9.1854e-01,  9.2313e-01,  8.8322e-01,  9.1420e-01,  9.8348e-01,\n",
      "         9.2327e-01,  1.1520e+00,  8.6186e-01,  9.0877e-01,  9.5389e-01,\n",
      "         1.1063e+00,  8.7351e-01,  9.4947e-01,  9.4503e-01,  8.0142e-01,\n",
      "         9.2509e-01,  8.9908e-01,  9.5958e-01,  9.7451e-01,  8.9454e-01,\n",
      "         9.7906e-01,  8.7750e-01,  9.9149e-01,  9.1605e-01,  9.2458e-01,\n",
      "         9.9343e-01,  9.2473e-01,  8.9162e-01,  9.6697e-01,  9.2314e-01,\n",
      "         9.4203e-01,  7.9355e-01,  8.9813e-01,  9.2235e-01,  8.5262e-01,\n",
      "         9.7520e-01,  9.3183e-01,  9.6109e-01,  1.1077e+00,  9.0880e-01,\n",
      "         9.7842e-01,  1.1167e+00,  8.9112e-01,  9.7327e-01,  9.3560e-01,\n",
      "         9.0158e-01,  9.9980e-01,  9.8663e-01,  1.0261e+00,  9.3085e-01,\n",
      "         1.0370e+00,  1.0916e+00,  8.0207e-01,  9.3864e-01,  9.1771e-01,\n",
      "         9.6090e-01,  9.8644e-01,  9.4517e-01,  9.0437e-01,  8.9563e-01,\n",
      "         8.9170e-01,  8.9308e-01,  9.0695e-01,  9.4084e-01,  1.1612e+00,\n",
      "         9.2827e-01,  9.5915e-01,  8.9661e-01,  1.0042e+00,  9.6864e-01,\n",
      "         7.7847e-01,  8.0386e-01,  9.0081e-01,  9.9999e-01,  9.1849e-01,\n",
      "         9.6650e-01,  8.8224e-01,  9.2721e-01,  9.7627e-01,  1.1218e+00,\n",
      "         9.3666e-01,  9.4089e-01,  8.8504e-01,  9.1491e-01,  8.9770e-01,\n",
      "         9.6417e-01,  9.6683e-01,  9.1433e-01,  9.3547e-01,  1.0296e+00,\n",
      "         9.5618e-01,  9.3199e-01,  1.1067e+00,  9.4212e-01,  1.2166e+00,\n",
      "         9.6661e-01,  9.1969e-01,  8.7161e-01,  9.3129e-01,  9.2292e-01,\n",
      "         9.0987e-01, -1.6194e-02,  9.2569e-04, -8.9018e-02,  1.0698e-02,\n",
      "        -1.0049e-02, -3.1169e-02,  1.7390e-02, -5.1653e-03,  2.8222e-02,\n",
      "        -1.2465e-02, -4.1307e-02, -2.6581e-02,  2.1157e-02, -1.1309e-02,\n",
      "         9.0069e-03, -7.8277e-04, -5.9162e-03, -1.3043e-02, -2.7551e-02,\n",
      "         7.9858e-02, -2.3749e-02, -1.4290e-03,  1.9627e-02,  4.5387e-03,\n",
      "         3.0558e-02, -3.2172e-02, -1.6320e-02,  5.3852e-02, -1.7333e-02,\n",
      "        -2.4809e-03,  1.8063e-02,  2.7336e-02, -1.1488e-02, -7.0138e-03,\n",
      "         3.4418e-02, -5.7055e-02, -7.7494e-02, -2.7992e-02, -2.6270e-02,\n",
      "        -1.2934e-02,  3.2468e-02,  1.8819e-02, -8.2865e-03, -1.6373e-02,\n",
      "         5.7489e-03, -1.3260e-02,  1.7041e-02,  2.7313e-02,  4.6200e-03,\n",
      "        -3.2059e-02,  7.9626e-02,  2.1670e-02, -8.8134e-03,  2.5010e-03,\n",
      "        -8.3515e-02, -1.3999e-02, -4.0573e-03, -5.5663e-02, -3.6304e-03,\n",
      "         2.1086e-02,  4.9357e-03, -5.1592e-03, -2.9483e-03,  2.5996e-03,\n",
      "         1.6401e-02,  9.8248e-03, -6.7059e-03, -4.0041e-02, -1.5718e-02,\n",
      "        -1.2054e-02,  1.0842e-02, -1.8775e-02, -2.3739e-02,  2.4045e-02,\n",
      "        -2.9344e-02,  8.7078e-03, -2.3472e-02,  9.1915e-03, -5.9291e-03,\n",
      "        -4.3240e-02, -4.5454e-03, -2.6794e-03,  2.4460e-02, -2.2470e-02,\n",
      "        -1.1615e-03, -1.3843e-02, -2.9997e-04, -1.2952e-02, -4.2937e-03,\n",
      "        -3.9899e-03,  3.9118e-02, -1.0560e-02, -1.2635e-02,  1.9040e-02,\n",
      "         2.9277e-02,  6.2332e-03,  4.6236e-03, -6.9440e-03,  3.3310e-02,\n",
      "        -1.1209e-02, -2.2114e-02,  1.4020e-02,  2.0877e-02,  1.9590e-02,\n",
      "        -6.6175e-03,  3.8334e-03, -1.3613e-04, -5.7797e-03,  1.4949e-02,\n",
      "        -7.1094e-03, -2.5852e-02,  1.9026e-02,  3.2779e-02, -4.1300e-02,\n",
      "        -7.2701e-03, -2.6195e-03, -4.9004e-03,  1.1443e-02, -4.2998e-03,\n",
      "        -1.9788e-02,  3.6296e-02,  8.2942e-03, -5.7166e-03, -1.7628e-02,\n",
      "         4.3618e-02, -1.9182e-02, -5.2632e-02,  2.2295e-03, -9.9240e-02,\n",
      "        -2.3076e-02,  1.3216e-04, -9.5952e-02, -8.0286e-02, -8.1183e-02,\n",
      "        -1.0061e-02, -1.7775e-01, -4.9473e-02, -4.1687e-02, -1.0242e-01,\n",
      "        -4.5002e-02, -1.3403e-02, -2.3615e-03, -3.7513e-05, -1.2004e-01,\n",
      "        -1.0096e-01, -1.1224e-01, -8.2757e-02, -1.4898e-01, -1.1842e-01,\n",
      "        -4.9748e-02, -5.0108e-02, -6.2135e-04, -2.9710e-02, -2.2081e-02,\n",
      "         4.1379e-02, -3.7299e-02, -1.1483e-02, -9.4094e-02, -1.1679e-01,\n",
      "        -7.4377e-02, -5.9048e-02, -7.2486e-02, -8.5268e-02, -7.5948e-03,\n",
      "        -8.3651e-02, -4.4674e-02, -6.4218e-02, -9.1014e-02, -5.2841e-02,\n",
      "        -1.7956e-01, -3.3166e-02,  7.9404e-03, -8.2961e-02, -5.8617e-02,\n",
      "        -4.6205e-02, -4.9743e-02,  8.8014e-05, -6.2790e-02, -7.4583e-02,\n",
      "        -4.2216e-02, -8.4477e-02, -6.1199e-02, -3.7281e-02, -5.4044e-02,\n",
      "        -3.0632e-03,  1.1072e-02, -1.6146e-01, -1.3073e-01, -7.5618e-02,\n",
      "        -1.9415e-01, -5.0151e-02, -1.0811e-01, -7.4216e-02, -4.8755e-02,\n",
      "        -8.1922e-02, -7.4293e-02, -1.6517e-02, -3.2446e-02, -1.6892e-02,\n",
      "        -5.2417e-02, -7.9239e-02, -2.4681e-02, -8.5717e-02, -1.2560e-01,\n",
      "        -1.3732e-01, -9.2395e-02, -1.1551e-01, -7.3980e-02, -1.1843e-01,\n",
      "        -1.2364e-01, -7.2164e-02, -5.8909e-02, -2.8314e-02, -5.9995e-02,\n",
      "        -1.4090e-01, -1.1557e-01, -1.2712e-01, -6.5249e-02,  6.1016e-03,\n",
      "        -9.0666e-02, -8.1580e-02, -6.0610e-02, -7.2314e-02, -5.8914e-02,\n",
      "        -4.9384e-02, -6.0005e-02, -1.3150e-01, -8.2140e-02, -2.7029e-02,\n",
      "        -6.7256e-02, -3.7384e-02, -9.2444e-02, -1.0206e-01, -4.9341e-02,\n",
      "        -8.3107e-02, -1.2020e-01, -9.5759e-02, -1.0844e-01, -9.2840e-02,\n",
      "        -8.7331e-02, -7.8904e-02, -4.2439e-02,  7.1640e-03, -5.0722e-02,\n",
      "        -8.8375e-02,  3.4391e-03, -6.9348e-02, -1.1358e-01, -9.9126e-02,\n",
      "        -5.6254e-02, -1.4823e-01, -2.1775e-02, -9.0317e-02, -1.1559e-01,\n",
      "        -8.7752e-03, -7.6458e-02], device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_hh_l0_reverse', Parameter containing:\n",
      "tensor([-1.9066e-01, -1.1691e-01, -5.3584e-02, -1.0994e-01, -1.3145e-01,\n",
      "        -4.0441e-02, -9.7780e-02, -1.8854e-01, -9.6097e-02, -8.9907e-02,\n",
      "        -1.7790e-01, -1.3019e-01, -5.2310e-04, -5.8973e-02, -1.0688e-01,\n",
      "        -1.8176e-01, -1.0032e-01, -8.7740e-02, -8.3766e-02, -1.7463e-01,\n",
      "        -1.8156e-01, -6.9478e-02, -2.8854e-01, -6.9956e-02, -1.3848e-01,\n",
      "        -1.8509e-01, -2.1467e-02, -8.2978e-02, -6.0315e-02, -1.2975e-01,\n",
      "        -1.4541e-01, -7.9534e-02, -1.2929e-01, -2.0177e-01, -7.7616e-02,\n",
      "        -1.4835e-01, -2.2845e-01, -1.0793e-01, -1.2142e-01, -1.3890e-01,\n",
      "        -1.4462e-01, -2.0988e-01, -1.5167e-01, -1.2112e-01, -7.7304e-02,\n",
      "        -7.2497e-02, -8.0048e-02, -2.5499e-01, -2.2103e-02, -1.5579e-01,\n",
      "        -6.3705e-02, -6.6581e-02, -1.2612e-01, -8.8395e-02, -1.5539e-01,\n",
      "        -6.4373e-02, -2.2380e-01, -2.1426e-02, -1.6581e-01, -1.7347e-01,\n",
      "        -4.1875e-02, -2.3440e-01, -6.1865e-02, -1.3448e-01, -9.3503e-02,\n",
      "        -2.9457e-01, -1.2725e-01, -6.9692e-02, -1.9211e-01, -1.0847e-01,\n",
      "        -4.9706e-02, -9.3441e-02, -7.3358e-02, -1.7055e-01, -1.2805e-01,\n",
      "        -1.2443e-01, -1.5218e-01, -1.4182e-01, -2.0001e-01,  9.9461e-03,\n",
      "        -1.1484e-01, -1.4171e-01, -8.2530e-02, -9.6967e-02, -4.8440e-02,\n",
      "        -7.0491e-02, -1.3257e-01, -1.5344e-01, -1.1931e-01, -1.2553e-01,\n",
      "        -1.8516e-01, -2.2000e-01, -1.3251e-01, -1.5166e-01, -1.3497e-01,\n",
      "        -1.5948e-01, -1.5945e-01, -8.4051e-02, -1.4338e-01, -8.1696e-02,\n",
      "        -2.3348e-01, -9.2535e-02, -2.3143e-01, -1.1227e-01, -1.6230e-01,\n",
      "        -5.1706e-02, -1.6580e-01, -7.7801e-02, -1.4756e-01, -1.2483e-01,\n",
      "        -1.0225e-01, -1.3558e-01, -1.0928e-01, -8.9775e-02, -2.7500e-02,\n",
      "        -5.5037e-02, -2.2988e-01, -1.6522e-01, -7.6701e-02, -1.7371e-01,\n",
      "        -7.4756e-02, -2.3463e-01, -1.4293e-01, -1.4941e-01, -1.2748e-01,\n",
      "        -9.5639e-02,  1.0958e-02, -1.2552e-01,  1.0273e+00,  1.0261e+00,\n",
      "         8.9623e-01,  1.0050e+00,  9.4506e-01,  1.0303e+00,  9.3197e-01,\n",
      "         8.5493e-01,  8.9594e-01,  9.4826e-01,  9.3688e-01,  8.9474e-01,\n",
      "         8.4777e-01,  8.9757e-01,  9.7078e-01,  9.2283e-01,  9.6907e-01,\n",
      "         9.9921e-01,  1.0255e+00,  9.3569e-01,  9.3675e-01,  9.5762e-01,\n",
      "         1.1316e+00,  9.4023e-01,  9.5464e-01,  9.2859e-01,  8.9678e-01,\n",
      "         9.1854e-01,  9.2313e-01,  8.8322e-01,  9.1420e-01,  9.8348e-01,\n",
      "         9.2327e-01,  1.1520e+00,  8.6186e-01,  9.0877e-01,  9.5389e-01,\n",
      "         1.1063e+00,  8.7351e-01,  9.4947e-01,  9.4503e-01,  8.0142e-01,\n",
      "         9.2509e-01,  8.9908e-01,  9.5958e-01,  9.7451e-01,  8.9454e-01,\n",
      "         9.7906e-01,  8.7750e-01,  9.9149e-01,  9.1605e-01,  9.2458e-01,\n",
      "         9.9343e-01,  9.2473e-01,  8.9162e-01,  9.6697e-01,  9.2314e-01,\n",
      "         9.4203e-01,  7.9355e-01,  8.9813e-01,  9.2235e-01,  8.5262e-01,\n",
      "         9.7520e-01,  9.3183e-01,  9.6109e-01,  1.1077e+00,  9.0880e-01,\n",
      "         9.7842e-01,  1.1167e+00,  8.9112e-01,  9.7327e-01,  9.3560e-01,\n",
      "         9.0158e-01,  9.9980e-01,  9.8663e-01,  1.0261e+00,  9.3085e-01,\n",
      "         1.0370e+00,  1.0916e+00,  8.0207e-01,  9.3864e-01,  9.1771e-01,\n",
      "         9.6090e-01,  9.8644e-01,  9.4517e-01,  9.0437e-01,  8.9563e-01,\n",
      "         8.9170e-01,  8.9308e-01,  9.0695e-01,  9.4084e-01,  1.1612e+00,\n",
      "         9.2827e-01,  9.5915e-01,  8.9661e-01,  1.0042e+00,  9.6864e-01,\n",
      "         7.7847e-01,  8.0386e-01,  9.0081e-01,  9.9999e-01,  9.1849e-01,\n",
      "         9.6650e-01,  8.8224e-01,  9.2721e-01,  9.7627e-01,  1.1218e+00,\n",
      "         9.3666e-01,  9.4089e-01,  8.8504e-01,  9.1491e-01,  8.9770e-01,\n",
      "         9.6417e-01,  9.6683e-01,  9.1433e-01,  9.3547e-01,  1.0296e+00,\n",
      "         9.5618e-01,  9.3199e-01,  1.1067e+00,  9.4212e-01,  1.2166e+00,\n",
      "         9.6661e-01,  9.1969e-01,  8.7161e-01,  9.3129e-01,  9.2292e-01,\n",
      "         9.0987e-01, -1.6194e-02,  9.2569e-04, -8.9018e-02,  1.0698e-02,\n",
      "        -1.0049e-02, -3.1169e-02,  1.7390e-02, -5.1653e-03,  2.8222e-02,\n",
      "        -1.2465e-02, -4.1307e-02, -2.6581e-02,  2.1157e-02, -1.1309e-02,\n",
      "         9.0069e-03, -7.8277e-04, -5.9162e-03, -1.3043e-02, -2.7551e-02,\n",
      "         7.9858e-02, -2.3749e-02, -1.4290e-03,  1.9627e-02,  4.5387e-03,\n",
      "         3.0558e-02, -3.2172e-02, -1.6320e-02,  5.3852e-02, -1.7333e-02,\n",
      "        -2.4809e-03,  1.8063e-02,  2.7336e-02, -1.1488e-02, -7.0138e-03,\n",
      "         3.4418e-02, -5.7055e-02, -7.7494e-02, -2.7992e-02, -2.6270e-02,\n",
      "        -1.2934e-02,  3.2468e-02,  1.8819e-02, -8.2865e-03, -1.6373e-02,\n",
      "         5.7489e-03, -1.3260e-02,  1.7041e-02,  2.7313e-02,  4.6200e-03,\n",
      "        -3.2059e-02,  7.9626e-02,  2.1670e-02, -8.8134e-03,  2.5010e-03,\n",
      "        -8.3515e-02, -1.3999e-02, -4.0573e-03, -5.5663e-02, -3.6304e-03,\n",
      "         2.1086e-02,  4.9357e-03, -5.1592e-03, -2.9483e-03,  2.5996e-03,\n",
      "         1.6401e-02,  9.8248e-03, -6.7059e-03, -4.0041e-02, -1.5718e-02,\n",
      "        -1.2054e-02,  1.0842e-02, -1.8775e-02, -2.3739e-02,  2.4045e-02,\n",
      "        -2.9344e-02,  8.7078e-03, -2.3472e-02,  9.1915e-03, -5.9291e-03,\n",
      "        -4.3240e-02, -4.5454e-03, -2.6794e-03,  2.4460e-02, -2.2470e-02,\n",
      "        -1.1615e-03, -1.3843e-02, -2.9997e-04, -1.2952e-02, -4.2937e-03,\n",
      "        -3.9899e-03,  3.9118e-02, -1.0560e-02, -1.2635e-02,  1.9040e-02,\n",
      "         2.9277e-02,  6.2332e-03,  4.6236e-03, -6.9440e-03,  3.3310e-02,\n",
      "        -1.1209e-02, -2.2114e-02,  1.4020e-02,  2.0877e-02,  1.9590e-02,\n",
      "        -6.6175e-03,  3.8334e-03, -1.3613e-04, -5.7797e-03,  1.4949e-02,\n",
      "        -7.1094e-03, -2.5852e-02,  1.9026e-02,  3.2779e-02, -4.1300e-02,\n",
      "        -7.2701e-03, -2.6195e-03, -4.9004e-03,  1.1443e-02, -4.2998e-03,\n",
      "        -1.9788e-02,  3.6296e-02,  8.2942e-03, -5.7166e-03, -1.7628e-02,\n",
      "         4.3618e-02, -1.9182e-02, -5.2632e-02,  2.2295e-03, -9.9240e-02,\n",
      "        -2.3076e-02,  1.3216e-04, -9.5952e-02, -8.0286e-02, -8.1183e-02,\n",
      "        -1.0061e-02, -1.7775e-01, -4.9473e-02, -4.1687e-02, -1.0242e-01,\n",
      "        -4.5002e-02, -1.3403e-02, -2.3615e-03, -3.7513e-05, -1.2004e-01,\n",
      "        -1.0096e-01, -1.1224e-01, -8.2757e-02, -1.4898e-01, -1.1842e-01,\n",
      "        -4.9748e-02, -5.0108e-02, -6.2135e-04, -2.9710e-02, -2.2081e-02,\n",
      "         4.1379e-02, -3.7299e-02, -1.1483e-02, -9.4094e-02, -1.1679e-01,\n",
      "        -7.4377e-02, -5.9048e-02, -7.2486e-02, -8.5268e-02, -7.5948e-03,\n",
      "        -8.3651e-02, -4.4674e-02, -6.4218e-02, -9.1014e-02, -5.2841e-02,\n",
      "        -1.7956e-01, -3.3166e-02,  7.9404e-03, -8.2961e-02, -5.8617e-02,\n",
      "        -4.6205e-02, -4.9743e-02,  8.8014e-05, -6.2790e-02, -7.4583e-02,\n",
      "        -4.2216e-02, -8.4477e-02, -6.1199e-02, -3.7281e-02, -5.4044e-02,\n",
      "        -3.0632e-03,  1.1072e-02, -1.6146e-01, -1.3073e-01, -7.5618e-02,\n",
      "        -1.9415e-01, -5.0151e-02, -1.0811e-01, -7.4216e-02, -4.8755e-02,\n",
      "        -8.1922e-02, -7.4293e-02, -1.6517e-02, -3.2446e-02, -1.6892e-02,\n",
      "        -5.2417e-02, -7.9239e-02, -2.4681e-02, -8.5717e-02, -1.2560e-01,\n",
      "        -1.3732e-01, -9.2395e-02, -1.1551e-01, -7.3980e-02, -1.1843e-01,\n",
      "        -1.2364e-01, -7.2164e-02, -5.8909e-02, -2.8314e-02, -5.9995e-02,\n",
      "        -1.4090e-01, -1.1557e-01, -1.2712e-01, -6.5249e-02,  6.1016e-03,\n",
      "        -9.0666e-02, -8.1580e-02, -6.0610e-02, -7.2314e-02, -5.8914e-02,\n",
      "        -4.9384e-02, -6.0005e-02, -1.3150e-01, -8.2140e-02, -2.7029e-02,\n",
      "        -6.7256e-02, -3.7384e-02, -9.2444e-02, -1.0206e-01, -4.9341e-02,\n",
      "        -8.3107e-02, -1.2020e-01, -9.5759e-02, -1.0844e-01, -9.2840e-02,\n",
      "        -8.7331e-02, -7.8904e-02, -4.2439e-02,  7.1640e-03, -5.0722e-02,\n",
      "        -8.8375e-02,  3.4391e-03, -6.9348e-02, -1.1358e-01, -9.9126e-02,\n",
      "        -5.6254e-02, -1.4823e-01, -2.1775e-02, -9.0317e-02, -1.1559e-01,\n",
      "        -8.7752e-03, -7.6458e-02], device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.weight_ih_l1', Parameter containing:\n",
      "tensor([[-0.3583,  0.0595, -0.0210,  ...,  0.1729,  0.0143, -0.0374],\n",
      "        [-0.0709,  0.0661, -0.0313,  ...,  0.1340, -0.0679, -0.0225],\n",
      "        [-0.1211, -0.1250, -0.2126,  ...,  0.0617,  0.0949,  0.0400],\n",
      "        ...,\n",
      "        [-0.0624,  0.1116, -0.0214,  ..., -0.0199,  0.0081,  0.0272],\n",
      "        [ 0.0421, -0.1259, -0.0055,  ...,  0.0763,  0.1426,  0.0019],\n",
      "        [-0.2717, -0.0326,  0.0651,  ...,  0.1624,  0.1692,  0.1469]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.weight_hh_l1', Parameter containing:\n",
      "tensor([[-0.1104, -0.1417,  0.1325,  ...,  0.0041,  0.1646,  0.3330],\n",
      "        [ 0.0941,  0.0347,  0.0232,  ..., -0.1662,  0.1269,  0.0320],\n",
      "        [ 0.1184, -0.0484,  0.1012,  ..., -0.0524,  0.1927,  0.2076],\n",
      "        ...,\n",
      "        [ 0.0592,  0.0124,  0.0641,  ..., -0.0329, -0.0381,  0.0247],\n",
      "        [-0.1264, -0.0884,  0.1211,  ..., -0.0917,  0.1062,  0.0085],\n",
      "        [ 0.0897,  0.0336,  0.0253,  ..., -0.1270,  0.0785,  0.1273]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_ih_l1', Parameter containing:\n",
      "tensor([-2.9547e-01, -2.5781e-01,  4.8853e-02, -7.4827e-02, -2.1531e-01,\n",
      "        -3.4338e-01, -6.4341e-02, -8.9570e-02, -1.6538e-01, -1.4556e-01,\n",
      "        -1.0849e-01, -9.9716e-02, -6.2650e-02, -1.7547e-01, -4.1056e-02,\n",
      "        -1.7887e-01, -4.3794e-01, -1.9542e-01,  6.9918e-02, -1.3919e-01,\n",
      "        -2.6136e-01, -1.7288e-01, -1.9906e-01, -1.5861e-01, -4.7202e-02,\n",
      "         4.3114e-02, -2.5590e-01, -1.8095e-01, -4.0559e-01, -2.9289e-01,\n",
      "        -1.5487e-01,  2.5257e-01, -3.4869e-01, -2.0935e-01, -1.2081e-01,\n",
      "        -2.3163e-02, -2.0829e-01, -3.4352e-01, -1.0098e-01, -2.0019e-02,\n",
      "        -3.0924e-02,  1.2982e-02, -1.8292e-01, -1.1975e-01, -1.5452e-01,\n",
      "        -1.6811e-01, -2.7986e-01, -1.8028e-01, -4.6100e-02, -3.7310e-02,\n",
      "        -1.5989e-01, -3.8810e-02, -2.9410e-01, -2.9056e-01, -1.8246e-01,\n",
      "        -5.9985e-02, -1.6717e-01, -1.0993e-01, -9.5913e-02, -1.3192e-01,\n",
      "        -2.6486e-01, -2.9279e-01, -3.0540e-01,  4.3664e-03, -2.0210e-01,\n",
      "        -9.3853e-02, -1.3661e-01, -9.2514e-02, -2.1147e-01, -1.7715e-01,\n",
      "        -4.7710e-01, -1.5367e-01, -1.4523e-01,  8.4807e-03, -2.1322e-01,\n",
      "        -1.1175e-01, -1.0434e-01, -3.2020e-01, -1.2198e-01, -1.4653e-01,\n",
      "        -1.4253e-01, -7.1777e-02, -3.0307e-01,  2.1356e-02, -7.3373e-02,\n",
      "        -1.0480e-02, -1.5645e-01, -1.0443e-01, -7.9821e-02, -2.8204e-01,\n",
      "        -1.4932e-01, -1.4097e-01, -1.6574e-01, -1.5524e-01, -1.6231e-01,\n",
      "        -1.5258e-01, -5.4574e-02, -6.5503e-02, -1.1889e-01, -6.1286e-02,\n",
      "        -2.6716e-01, -2.0574e-01, -1.7230e-01, -2.1485e-01, -1.1217e-01,\n",
      "        -1.7971e-01, -2.4707e-01, -2.4178e-01, -3.9060e-01, -2.7192e-01,\n",
      "        -1.6459e-01, -1.6327e-01, -2.9288e-01, -8.0153e-02, -2.9186e-01,\n",
      "         1.2249e-01, -1.2212e-01, -8.0404e-02, -1.2054e-01, -1.5969e-01,\n",
      "        -9.1236e-02, -2.6403e-01, -1.7999e-01, -2.4219e-01, -1.3283e-01,\n",
      "        -1.5031e-01, -2.2932e-01, -7.5135e-02,  7.5694e-01,  7.8024e-01,\n",
      "         8.4952e-01,  9.9418e-01,  1.2173e+00,  8.2094e-01,  9.1748e-01,\n",
      "         8.5385e-01,  6.8755e-01,  7.8571e-01,  7.4894e-01,  8.7035e-01,\n",
      "         7.5965e-01,  9.3711e-01,  1.0637e+00,  8.2466e-01,  1.1260e+00,\n",
      "         6.2005e-01,  1.0898e+00,  7.3186e-01,  1.0322e+00,  7.7974e-01,\n",
      "         7.2258e-01,  8.8670e-01,  7.9998e-01,  7.8544e-01,  7.1064e-01,\n",
      "         1.2170e+00,  9.8142e-01,  7.8573e-01,  8.1473e-01,  1.2761e+00,\n",
      "         8.1691e-01,  9.5886e-01,  1.0883e+00,  7.9488e-01,  8.8080e-01,\n",
      "         7.9758e-01,  8.2922e-01,  5.7376e-01,  8.1804e-01,  6.7877e-01,\n",
      "         6.3593e-01,  7.9991e-01,  8.0107e-01,  1.1055e+00,  1.2962e+00,\n",
      "         1.0965e+00,  7.9112e-01,  9.7289e-01,  1.1052e+00,  8.7429e-01,\n",
      "         7.3909e-01,  8.6628e-01,  8.9409e-01,  8.0749e-01,  8.5654e-01,\n",
      "         8.3873e-01,  8.3528e-01,  9.4879e-01,  6.8833e-01,  1.1149e+00,\n",
      "         7.9856e-01,  7.6136e-01,  7.4636e-01,  8.7473e-01,  8.8191e-01,\n",
      "         7.9169e-01,  8.9480e-01,  8.7327e-01,  9.6185e-01,  8.2470e-01,\n",
      "         7.7993e-01,  8.4517e-01,  8.5926e-01,  8.6626e-01,  9.0942e-01,\n",
      "         1.0915e+00,  5.2703e-01,  8.4151e-01,  7.4242e-01,  1.2478e+00,\n",
      "         8.6299e-01,  6.9060e-01,  8.0147e-01,  8.1947e-01,  8.7874e-01,\n",
      "         7.7836e-01,  7.2999e-01,  6.7281e-01,  1.0976e+00,  5.8972e-01,\n",
      "         7.3231e-01,  8.2439e-01,  1.0648e+00,  6.8746e-01,  7.1571e-01,\n",
      "         1.2089e+00,  7.8251e-01,  7.0827e-01,  1.1660e+00,  7.9475e-01,\n",
      "         7.7749e-01,  1.3157e+00,  6.2266e-01,  7.7822e-01,  8.1045e-01,\n",
      "         7.8487e-01,  9.6085e-01,  7.7080e-01,  6.3174e-01,  9.1221e-01,\n",
      "         1.2264e+00,  7.7927e-01,  8.0268e-01,  9.5216e-01,  7.3514e-01,\n",
      "         7.5207e-01,  8.6145e-01,  7.3910e-01,  1.0289e+00,  1.1392e+00,\n",
      "         1.1039e+00,  7.9757e-01,  1.2131e+00,  8.5849e-01,  1.0982e+00,\n",
      "         9.4931e-01, -3.2349e-04, -3.4412e-02, -1.1434e-02, -1.4809e-02,\n",
      "        -4.6059e-02, -1.1563e-01,  5.0922e-02, -1.9846e-02, -1.2457e-01,\n",
      "        -1.0578e-02, -6.8395e-02, -1.0697e-03, -1.5801e-02, -3.1010e-02,\n",
      "        -5.6507e-02, -8.2337e-04, -1.4006e-05,  4.0074e-03, -1.4515e-01,\n",
      "         2.2053e-02,  4.3883e-02,  1.9806e-02,  1.1108e-02, -4.2910e-02,\n",
      "        -8.8017e-02, -2.2276e-02,  2.2446e-03, -9.8055e-02, -2.9140e-03,\n",
      "        -1.4145e-02, -3.1101e-02,  1.3013e-01, -1.8272e-03, -2.4852e-02,\n",
      "        -8.4591e-02, -2.6244e-02, -7.0801e-02, -7.9938e-02,  3.8901e-02,\n",
      "         4.1530e-04,  7.7424e-03,  9.1727e-02, -7.8734e-02,  4.2941e-02,\n",
      "         2.6440e-03, -3.2484e-02, -3.2189e-02,  1.5438e-02,  8.6345e-03,\n",
      "         7.6437e-02, -2.3914e-03, -3.8748e-03, -8.0136e-02, -3.8195e-02,\n",
      "        -4.8581e-03, -5.1716e-02,  2.9369e-02, -2.2396e-02,  2.9186e-02,\n",
      "         5.5575e-02, -5.9004e-02, -4.8085e-02,  7.0646e-02,  6.1892e-02,\n",
      "         8.0567e-03,  4.0748e-02,  2.9023e-02,  9.6869e-02, -5.6539e-02,\n",
      "         1.1148e-02, -1.4082e-01, -2.7580e-02, -3.7396e-03,  7.4569e-02,\n",
      "        -4.3032e-02, -1.9679e-02, -2.0736e-02,  2.9593e-02, -1.3188e-01,\n",
      "        -6.0237e-03,  2.4110e-06, -8.2085e-02, -4.3636e-02,  2.7227e-02,\n",
      "        -1.0965e-02,  3.0790e-03,  3.5181e-02,  2.2135e-02,  1.9179e-03,\n",
      "        -5.4366e-02,  5.2015e-02, -4.2126e-02,  1.1607e-01,  3.0018e-02,\n",
      "         9.0821e-03, -1.0489e-01, -3.7990e-02, -9.0960e-02,  2.4493e-02,\n",
      "         3.2883e-02, -5.2593e-02, -3.3095e-02,  7.5087e-02,  2.5759e-02,\n",
      "        -1.1901e-01, -3.7711e-02, -3.2706e-02, -1.9241e-02, -4.5106e-02,\n",
      "        -7.9471e-02, -8.6458e-02,  1.4564e-02, -7.0368e-04,  4.0998e-02,\n",
      "        -5.4306e-03,  5.4328e-02, -1.5426e-02, -4.5755e-02, -2.0794e-02,\n",
      "        -5.6798e-02,  1.3213e-01, -5.6315e-02, -2.4364e-02, -1.4559e-02,\n",
      "        -1.1499e-01,  1.8781e-02, -3.1581e-02, -5.6645e-02, -2.1405e-01,\n",
      "        -2.5635e-01, -1.4861e-01, -1.7324e-01, -2.9665e-01, -2.3466e-01,\n",
      "        -1.1773e-01, -2.2987e-01, -1.2841e-01, -2.1782e-01, -3.5209e-02,\n",
      "        -2.2395e-01, -1.0267e-01, -2.6526e-01,  2.9047e-02, -1.8098e-01,\n",
      "        -3.0342e-01, -6.7277e-02, -2.1919e-01, -1.7632e-01, -2.9649e-01,\n",
      "        -2.0891e-01, -1.6779e-01, -1.2529e-01, -2.3379e-01, -5.5053e-02,\n",
      "        -2.4637e-01, -3.2896e-01, -2.3896e-01, -1.5440e-01, -1.3072e-01,\n",
      "        -2.9380e-01, -2.9762e-01, -3.2176e-01, -1.6743e-01, -1.1963e-01,\n",
      "        -2.9539e-01, -1.9578e-01, -1.0732e-01, -1.4739e-01, -1.2402e-01,\n",
      "        -1.3764e-01, -1.8512e-01, -7.4560e-02, -1.7958e-01, -2.3074e-01,\n",
      "        -2.0390e-01, -2.2553e-01, -6.8081e-02, -2.1598e-01, -2.3960e-01,\n",
      "        -2.2185e-02, -1.7539e-01, -2.6358e-01, -1.5290e-01, -1.3413e-01,\n",
      "        -1.4342e-01, -1.8389e-01, -1.7428e-01, -1.4188e-01, -1.8421e-01,\n",
      "        -1.0205e-01, -2.6128e-01, -1.5222e-01, -1.5546e-01, -1.5890e-01,\n",
      "        -2.2796e-01, -1.7990e-01, -1.9167e-01, -1.2378e-01, -2.7040e-01,\n",
      "        -1.5325e-01, -1.6746e-01, -1.4424e-01, -1.8489e-01, -1.7607e-01,\n",
      "        -2.2954e-01, -2.5542e-01, -3.8412e-02, -1.7310e-01, -1.7730e-01,\n",
      "        -3.0041e-01, -2.2572e-01, -1.7746e-01, -1.8948e-01, -9.7175e-02,\n",
      "        -1.9414e-01, -1.6361e-01, -2.0341e-01, -1.8079e-01, -3.1701e-01,\n",
      "        -1.1179e-01, -2.8005e-01, -2.2941e-01,  2.2499e-02, -2.0124e-01,\n",
      "        -1.3794e-01, -1.5841e-01, -9.7127e-02, -2.2875e-01, -2.7401e-01,\n",
      "        -1.8908e-01, -1.6065e-01, -2.5755e-01, -1.4870e-01, -1.2133e-01,\n",
      "        -1.4306e-01, -2.1733e-01, -4.3870e-01, -1.8657e-01, -1.8392e-01,\n",
      "        -1.9309e-01, -2.8025e-01, -9.6479e-02, -2.3682e-01, -2.0865e-01,\n",
      "        -1.2981e-01, -1.5526e-01, -2.6003e-01, -1.4267e-01, -1.3995e-01,\n",
      "        -1.2896e-01, -9.6430e-02, -2.4919e-01, -2.4994e-01, -1.2468e-01,\n",
      "        -2.5938e-01, -2.0039e-01], device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_hh_l1', Parameter containing:\n",
      "tensor([-2.9547e-01, -2.5781e-01,  4.8853e-02, -7.4827e-02, -2.1531e-01,\n",
      "        -3.4338e-01, -6.4341e-02, -8.9570e-02, -1.6538e-01, -1.4556e-01,\n",
      "        -1.0849e-01, -9.9716e-02, -6.2650e-02, -1.7547e-01, -4.1056e-02,\n",
      "        -1.7887e-01, -4.3794e-01, -1.9542e-01,  6.9918e-02, -1.3919e-01,\n",
      "        -2.6136e-01, -1.7288e-01, -1.9906e-01, -1.5861e-01, -4.7202e-02,\n",
      "         4.3114e-02, -2.5590e-01, -1.8095e-01, -4.0559e-01, -2.9289e-01,\n",
      "        -1.5487e-01,  2.5257e-01, -3.4869e-01, -2.0935e-01, -1.2081e-01,\n",
      "        -2.3163e-02, -2.0829e-01, -3.4352e-01, -1.0098e-01, -2.0019e-02,\n",
      "        -3.0924e-02,  1.2982e-02, -1.8292e-01, -1.1975e-01, -1.5452e-01,\n",
      "        -1.6811e-01, -2.7986e-01, -1.8028e-01, -4.6100e-02, -3.7310e-02,\n",
      "        -1.5989e-01, -3.8810e-02, -2.9410e-01, -2.9056e-01, -1.8246e-01,\n",
      "        -5.9985e-02, -1.6717e-01, -1.0993e-01, -9.5913e-02, -1.3192e-01,\n",
      "        -2.6486e-01, -2.9279e-01, -3.0540e-01,  4.3664e-03, -2.0210e-01,\n",
      "        -9.3853e-02, -1.3661e-01, -9.2514e-02, -2.1147e-01, -1.7715e-01,\n",
      "        -4.7710e-01, -1.5367e-01, -1.4523e-01,  8.4807e-03, -2.1322e-01,\n",
      "        -1.1175e-01, -1.0434e-01, -3.2020e-01, -1.2198e-01, -1.4653e-01,\n",
      "        -1.4253e-01, -7.1777e-02, -3.0307e-01,  2.1356e-02, -7.3373e-02,\n",
      "        -1.0480e-02, -1.5645e-01, -1.0443e-01, -7.9821e-02, -2.8204e-01,\n",
      "        -1.4932e-01, -1.4097e-01, -1.6574e-01, -1.5524e-01, -1.6231e-01,\n",
      "        -1.5258e-01, -5.4574e-02, -6.5503e-02, -1.1889e-01, -6.1286e-02,\n",
      "        -2.6716e-01, -2.0574e-01, -1.7230e-01, -2.1485e-01, -1.1217e-01,\n",
      "        -1.7971e-01, -2.4707e-01, -2.4178e-01, -3.9060e-01, -2.7192e-01,\n",
      "        -1.6459e-01, -1.6327e-01, -2.9288e-01, -8.0153e-02, -2.9186e-01,\n",
      "         1.2249e-01, -1.2212e-01, -8.0404e-02, -1.2054e-01, -1.5969e-01,\n",
      "        -9.1236e-02, -2.6403e-01, -1.7999e-01, -2.4219e-01, -1.3283e-01,\n",
      "        -1.5031e-01, -2.2932e-01, -7.5135e-02,  7.5694e-01,  7.8024e-01,\n",
      "         8.4952e-01,  9.9418e-01,  1.2173e+00,  8.2094e-01,  9.1748e-01,\n",
      "         8.5385e-01,  6.8755e-01,  7.8571e-01,  7.4894e-01,  8.7035e-01,\n",
      "         7.5965e-01,  9.3711e-01,  1.0637e+00,  8.2466e-01,  1.1260e+00,\n",
      "         6.2005e-01,  1.0898e+00,  7.3186e-01,  1.0322e+00,  7.7974e-01,\n",
      "         7.2258e-01,  8.8670e-01,  7.9998e-01,  7.8544e-01,  7.1064e-01,\n",
      "         1.2170e+00,  9.8142e-01,  7.8573e-01,  8.1473e-01,  1.2761e+00,\n",
      "         8.1691e-01,  9.5886e-01,  1.0883e+00,  7.9488e-01,  8.8080e-01,\n",
      "         7.9758e-01,  8.2922e-01,  5.7376e-01,  8.1804e-01,  6.7877e-01,\n",
      "         6.3593e-01,  7.9991e-01,  8.0107e-01,  1.1055e+00,  1.2962e+00,\n",
      "         1.0965e+00,  7.9112e-01,  9.7289e-01,  1.1052e+00,  8.7429e-01,\n",
      "         7.3909e-01,  8.6628e-01,  8.9409e-01,  8.0749e-01,  8.5654e-01,\n",
      "         8.3873e-01,  8.3528e-01,  9.4879e-01,  6.8833e-01,  1.1149e+00,\n",
      "         7.9856e-01,  7.6136e-01,  7.4636e-01,  8.7473e-01,  8.8191e-01,\n",
      "         7.9169e-01,  8.9480e-01,  8.7327e-01,  9.6185e-01,  8.2470e-01,\n",
      "         7.7993e-01,  8.4517e-01,  8.5926e-01,  8.6626e-01,  9.0942e-01,\n",
      "         1.0915e+00,  5.2703e-01,  8.4151e-01,  7.4242e-01,  1.2478e+00,\n",
      "         8.6299e-01,  6.9060e-01,  8.0147e-01,  8.1947e-01,  8.7874e-01,\n",
      "         7.7836e-01,  7.2999e-01,  6.7281e-01,  1.0976e+00,  5.8972e-01,\n",
      "         7.3231e-01,  8.2439e-01,  1.0648e+00,  6.8746e-01,  7.1571e-01,\n",
      "         1.2089e+00,  7.8251e-01,  7.0827e-01,  1.1660e+00,  7.9475e-01,\n",
      "         7.7749e-01,  1.3157e+00,  6.2266e-01,  7.7822e-01,  8.1045e-01,\n",
      "         7.8487e-01,  9.6085e-01,  7.7080e-01,  6.3174e-01,  9.1221e-01,\n",
      "         1.2264e+00,  7.7927e-01,  8.0268e-01,  9.5216e-01,  7.3514e-01,\n",
      "         7.5207e-01,  8.6145e-01,  7.3910e-01,  1.0289e+00,  1.1392e+00,\n",
      "         1.1039e+00,  7.9757e-01,  1.2131e+00,  8.5849e-01,  1.0982e+00,\n",
      "         9.4931e-01, -3.2349e-04, -3.4412e-02, -1.1434e-02, -1.4809e-02,\n",
      "        -4.6059e-02, -1.1563e-01,  5.0922e-02, -1.9846e-02, -1.2457e-01,\n",
      "        -1.0578e-02, -6.8395e-02, -1.0697e-03, -1.5801e-02, -3.1010e-02,\n",
      "        -5.6507e-02, -8.2337e-04, -1.4006e-05,  4.0074e-03, -1.4515e-01,\n",
      "         2.2053e-02,  4.3883e-02,  1.9806e-02,  1.1108e-02, -4.2910e-02,\n",
      "        -8.8017e-02, -2.2276e-02,  2.2446e-03, -9.8055e-02, -2.9140e-03,\n",
      "        -1.4145e-02, -3.1101e-02,  1.3013e-01, -1.8272e-03, -2.4852e-02,\n",
      "        -8.4591e-02, -2.6244e-02, -7.0801e-02, -7.9938e-02,  3.8901e-02,\n",
      "         4.1530e-04,  7.7424e-03,  9.1727e-02, -7.8734e-02,  4.2941e-02,\n",
      "         2.6440e-03, -3.2484e-02, -3.2189e-02,  1.5438e-02,  8.6345e-03,\n",
      "         7.6437e-02, -2.3914e-03, -3.8748e-03, -8.0136e-02, -3.8195e-02,\n",
      "        -4.8581e-03, -5.1716e-02,  2.9369e-02, -2.2396e-02,  2.9186e-02,\n",
      "         5.5575e-02, -5.9004e-02, -4.8085e-02,  7.0646e-02,  6.1892e-02,\n",
      "         8.0567e-03,  4.0748e-02,  2.9023e-02,  9.6869e-02, -5.6539e-02,\n",
      "         1.1148e-02, -1.4082e-01, -2.7580e-02, -3.7396e-03,  7.4569e-02,\n",
      "        -4.3032e-02, -1.9679e-02, -2.0736e-02,  2.9593e-02, -1.3188e-01,\n",
      "        -6.0237e-03,  2.4110e-06, -8.2085e-02, -4.3636e-02,  2.7227e-02,\n",
      "        -1.0965e-02,  3.0790e-03,  3.5181e-02,  2.2135e-02,  1.9179e-03,\n",
      "        -5.4366e-02,  5.2015e-02, -4.2126e-02,  1.1607e-01,  3.0018e-02,\n",
      "         9.0821e-03, -1.0489e-01, -3.7990e-02, -9.0960e-02,  2.4493e-02,\n",
      "         3.2883e-02, -5.2593e-02, -3.3095e-02,  7.5087e-02,  2.5759e-02,\n",
      "        -1.1901e-01, -3.7711e-02, -3.2706e-02, -1.9241e-02, -4.5106e-02,\n",
      "        -7.9471e-02, -8.6458e-02,  1.4564e-02, -7.0368e-04,  4.0998e-02,\n",
      "        -5.4306e-03,  5.4328e-02, -1.5426e-02, -4.5755e-02, -2.0794e-02,\n",
      "        -5.6798e-02,  1.3213e-01, -5.6315e-02, -2.4364e-02, -1.4559e-02,\n",
      "        -1.1499e-01,  1.8781e-02, -3.1581e-02, -5.6645e-02, -2.1405e-01,\n",
      "        -2.5635e-01, -1.4861e-01, -1.7324e-01, -2.9665e-01, -2.3466e-01,\n",
      "        -1.1773e-01, -2.2987e-01, -1.2841e-01, -2.1782e-01, -3.5209e-02,\n",
      "        -2.2395e-01, -1.0267e-01, -2.6526e-01,  2.9047e-02, -1.8098e-01,\n",
      "        -3.0342e-01, -6.7277e-02, -2.1919e-01, -1.7632e-01, -2.9649e-01,\n",
      "        -2.0891e-01, -1.6779e-01, -1.2529e-01, -2.3379e-01, -5.5053e-02,\n",
      "        -2.4637e-01, -3.2896e-01, -2.3896e-01, -1.5440e-01, -1.3072e-01,\n",
      "        -2.9380e-01, -2.9762e-01, -3.2176e-01, -1.6743e-01, -1.1963e-01,\n",
      "        -2.9539e-01, -1.9578e-01, -1.0732e-01, -1.4739e-01, -1.2402e-01,\n",
      "        -1.3764e-01, -1.8512e-01, -7.4560e-02, -1.7958e-01, -2.3074e-01,\n",
      "        -2.0390e-01, -2.2553e-01, -6.8081e-02, -2.1598e-01, -2.3960e-01,\n",
      "        -2.2185e-02, -1.7539e-01, -2.6358e-01, -1.5290e-01, -1.3413e-01,\n",
      "        -1.4342e-01, -1.8389e-01, -1.7428e-01, -1.4188e-01, -1.8421e-01,\n",
      "        -1.0205e-01, -2.6128e-01, -1.5222e-01, -1.5546e-01, -1.5890e-01,\n",
      "        -2.2796e-01, -1.7990e-01, -1.9167e-01, -1.2378e-01, -2.7040e-01,\n",
      "        -1.5325e-01, -1.6746e-01, -1.4424e-01, -1.8489e-01, -1.7607e-01,\n",
      "        -2.2954e-01, -2.5542e-01, -3.8412e-02, -1.7310e-01, -1.7730e-01,\n",
      "        -3.0041e-01, -2.2572e-01, -1.7746e-01, -1.8948e-01, -9.7175e-02,\n",
      "        -1.9414e-01, -1.6361e-01, -2.0341e-01, -1.8079e-01, -3.1701e-01,\n",
      "        -1.1179e-01, -2.8005e-01, -2.2941e-01,  2.2499e-02, -2.0124e-01,\n",
      "        -1.3794e-01, -1.5841e-01, -9.7127e-02, -2.2875e-01, -2.7401e-01,\n",
      "        -1.8908e-01, -1.6065e-01, -2.5755e-01, -1.4870e-01, -1.2133e-01,\n",
      "        -1.4306e-01, -2.1733e-01, -4.3870e-01, -1.8657e-01, -1.8392e-01,\n",
      "        -1.9309e-01, -2.8025e-01, -9.6479e-02, -2.3682e-01, -2.0865e-01,\n",
      "        -1.2981e-01, -1.5526e-01, -2.6003e-01, -1.4267e-01, -1.3995e-01,\n",
      "        -1.2896e-01, -9.6430e-02, -2.4919e-01, -2.4994e-01, -1.2468e-01,\n",
      "        -2.5938e-01, -2.0039e-01], device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.weight_ih_l1_reverse', Parameter containing:\n",
      "tensor([[-0.0906, -0.0521, -0.0029,  ...,  0.0695,  0.1494, -0.2131],\n",
      "        [-0.1702, -0.0625, -0.0709,  ...,  0.0176,  0.0060, -0.0190],\n",
      "        [ 0.0565,  0.0735, -0.0676,  ...,  0.0177, -0.1039,  0.0385],\n",
      "        ...,\n",
      "        [-0.2318,  0.0589,  0.0603,  ...,  0.1225,  0.1727,  0.0739],\n",
      "        [-0.1393, -0.0865,  0.0202,  ..., -0.0409, -0.0576,  0.0899],\n",
      "        [-0.1378, -0.0077,  0.0657,  ...,  0.1775, -0.0211, -0.1550]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.weight_hh_l1_reverse', Parameter containing:\n",
      "tensor([[-0.0647,  0.0313, -0.0487,  ...,  0.2626, -0.1101,  0.1530],\n",
      "        [ 0.1728,  0.0542, -0.0171,  ...,  0.0090, -0.0521,  0.1745],\n",
      "        [ 0.2387,  0.0606, -0.1232,  ..., -0.4168,  0.1369,  0.2696],\n",
      "        ...,\n",
      "        [ 0.1906,  0.1749,  0.0943,  ..., -0.1297,  0.0384,  0.1372],\n",
      "        [ 0.1105, -0.0145, -0.0892,  ...,  0.0331, -0.2099,  0.1074],\n",
      "        [ 0.0776, -0.0053,  0.0027,  ..., -0.0155, -0.0079,  0.1752]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_ih_l1_reverse', Parameter containing:\n",
      "tensor([-1.5295e-01, -1.5582e-01, -1.1114e-01, -1.2044e-01, -1.8836e-01,\n",
      "         4.0213e-02, -3.9636e-02, -1.4118e-01,  6.3306e-02, -2.0133e-01,\n",
      "        -2.3427e-01, -1.8358e-01,  2.4879e-01, -2.8794e-01, -2.8517e-01,\n",
      "        -1.6045e-01,  5.6759e-02,  5.4707e-02, -2.2074e-01,  7.4424e-02,\n",
      "        -2.9609e-01, -1.9579e-01, -1.7267e-01, -1.4249e-01, -1.2110e-01,\n",
      "        -7.3635e-02, -1.8567e-01, -2.1102e-01, -2.0391e-01, -2.3247e-01,\n",
      "        -1.8721e-01,  1.2753e-03, -5.0462e-02, -1.2953e-01, -2.4505e-01,\n",
      "        -3.3437e-02, -9.8451e-02, -1.8601e-01, -2.1449e-01, -1.9470e-01,\n",
      "        -9.1382e-03, -1.3506e-01, -2.4323e-01, -1.2248e-01, -1.5521e-01,\n",
      "        -2.0318e-01, -1.6794e-01, -1.9339e-01, -2.0383e-01, -9.3254e-03,\n",
      "        -2.0214e-01, -1.9173e-01, -6.3698e-02, -2.9976e-01, -1.8780e-02,\n",
      "        -2.6094e-01, -1.6871e-01, -2.9401e-01, -1.6075e-01, -1.0399e-01,\n",
      "        -1.7390e-01,  2.0300e-02, -1.5882e-01, -1.6664e-01, -5.3452e-02,\n",
      "        -2.6237e-01, -2.1343e-01, -2.1539e-01,  4.0874e-02, -1.1206e-01,\n",
      "        -1.0909e-01,  6.3332e-02, -5.3154e-02,  4.1531e-02, -1.8346e-01,\n",
      "        -1.8736e-01, -9.1554e-02,  4.1870e-02, -2.8807e-01, -1.1443e-01,\n",
      "        -3.6788e-02, -1.1727e-01, -1.3078e-01, -1.9858e-02, -2.3557e-01,\n",
      "        -1.7973e-01, -1.2167e-01, -2.1598e-01, -1.8215e-01, -1.1481e-01,\n",
      "        -1.0821e-01, -2.3046e-01, -9.5246e-02, -1.3423e-01, -1.7285e-01,\n",
      "        -3.6823e-01, -1.4185e-01, -1.2896e-01,  5.5591e-03, -2.8418e-01,\n",
      "        -2.4874e-01, -2.2842e-01, -2.4048e-01, -1.7485e-01, -1.3571e-01,\n",
      "        -9.3415e-02, -2.7145e-01, -6.4755e-02, -1.0464e-01,  6.4163e-02,\n",
      "        -1.5238e-01, -1.0846e-01, -2.2835e-01, -2.1487e-01, -1.7003e-01,\n",
      "        -1.3865e-01,  7.2606e-03, -2.1672e-01, -2.1676e-02, -2.9345e-01,\n",
      "        -1.2880e-01, -1.5460e-01, -1.4811e-01, -8.8599e-02, -1.5846e-01,\n",
      "        -9.6354e-02, -1.7506e-01, -1.7213e-01,  1.2259e+00,  8.6872e-01,\n",
      "         8.7365e-01,  1.0983e+00,  8.7516e-01,  9.5860e-01,  8.7790e-01,\n",
      "         8.6063e-01,  1.1170e+00,  8.3750e-01,  1.1713e+00,  8.1022e-01,\n",
      "         1.1551e+00,  1.1704e+00,  7.5639e-01,  8.4147e-01,  1.0966e+00,\n",
      "         9.0409e-01,  9.2131e-01,  8.7307e-01,  7.5169e-01,  8.5225e-01,\n",
      "         1.0204e+00,  1.1006e+00,  9.4455e-01,  9.6721e-01,  8.9342e-01,\n",
      "         7.8555e-01,  9.5005e-01,  8.3536e-01,  8.1481e-01,  1.0733e+00,\n",
      "         8.8772e-01,  8.1078e-01,  8.3964e-01,  9.0241e-01,  8.7489e-01,\n",
      "         8.3509e-01,  1.0710e+00,  7.4858e-01,  1.1428e+00,  7.6223e-01,\n",
      "         1.1585e+00,  8.6464e-01,  8.9032e-01,  8.3007e-01,  8.2285e-01,\n",
      "         9.2732e-01,  9.7679e-01,  9.9068e-01,  8.5312e-01,  7.2817e-01,\n",
      "         7.8881e-01,  1.2650e+00,  9.1782e-01,  8.9912e-01,  1.0696e+00,\n",
      "         1.2933e+00,  1.1469e+00,  7.9201e-01,  7.4443e-01,  1.2235e+00,\n",
      "         9.3859e-01,  1.2391e+00,  1.0722e+00,  1.0818e+00,  7.2817e-01,\n",
      "         8.7090e-01,  9.1306e-01,  8.9971e-01,  1.0185e+00,  1.0058e+00,\n",
      "         9.1361e-01,  1.0362e+00,  7.8937e-01,  9.5448e-01,  1.0123e+00,\n",
      "         1.0510e+00,  8.1006e-01,  8.0694e-01,  8.0811e-01,  1.0387e+00,\n",
      "         8.5145e-01,  8.2202e-01,  8.6605e-01,  9.2519e-01,  7.6133e-01,\n",
      "         9.9302e-01,  7.6289e-01,  7.5260e-01,  9.0982e-01,  8.8633e-01,\n",
      "         8.5336e-01,  8.1978e-01,  8.3643e-01,  8.7252e-01,  8.8456e-01,\n",
      "         6.6695e-01,  9.3047e-01,  8.2171e-01,  6.2774e-01,  9.2351e-01,\n",
      "         7.8654e-01,  8.6029e-01,  8.9677e-01,  9.9557e-01,  8.0343e-01,\n",
      "         1.0861e+00,  9.1513e-01,  9.7221e-01,  8.5641e-01,  1.1813e+00,\n",
      "         7.9516e-01,  1.2202e+00,  8.6395e-01,  8.7579e-01,  1.0394e+00,\n",
      "         8.1255e-01,  1.1811e+00,  1.0914e+00,  9.0257e-01,  7.1634e-01,\n",
      "         8.2340e-01,  9.1424e-01,  8.4564e-01,  9.8729e-01,  9.4590e-01,\n",
      "         1.2217e+00, -7.1641e-02, -3.4044e-03,  1.1018e-01, -1.5542e-02,\n",
      "        -2.8840e-02,  6.2272e-02,  4.9574e-02, -4.1769e-02, -9.4388e-02,\n",
      "        -6.3108e-02, -1.6485e-02, -1.4578e-02, -5.2249e-04, -1.8142e-02,\n",
      "        -4.0550e-02, -8.3113e-02,  7.8052e-02,  5.7460e-02, -1.0550e-01,\n",
      "         4.2196e-02,  8.6914e-03,  7.4367e-02,  3.9399e-02, -3.0022e-02,\n",
      "        -1.1657e-04, -4.9952e-03,  4.7743e-02, -1.1866e-02, -6.2616e-02,\n",
      "        -3.1819e-02, -2.6725e-02,  8.3507e-02,  3.0468e-02,  7.6056e-02,\n",
      "        -1.3513e-01,  9.4240e-03,  1.2233e-02, -4.7065e-02, -4.1845e-03,\n",
      "        -2.0901e-02,  7.1538e-02, -1.1730e-01,  3.3687e-02,  6.0242e-02,\n",
      "         5.0409e-02, -6.7334e-02, -7.1842e-02, -7.8734e-02, -1.2554e-02,\n",
      "        -3.7550e-02, -3.5105e-02,  1.1945e-02, -5.9007e-02,  8.6506e-02,\n",
      "         8.8213e-02,  3.3888e-02, -4.5356e-03,  4.1854e-02,  3.6998e-03,\n",
      "         6.2512e-03, -8.5746e-02, -5.1942e-02, -1.3068e-02,  4.5790e-02,\n",
      "         1.4276e-02, -3.2488e-02,  6.2249e-02, -8.0496e-02,  8.6311e-02,\n",
      "        -9.5075e-02,  1.7184e-02,  1.7046e-02,  7.9405e-02,  2.5371e-03,\n",
      "         1.2962e-02, -1.0745e-02, -1.0220e-01,  1.1335e-01,  3.1582e-02,\n",
      "        -2.9309e-03,  8.3130e-02, -6.5508e-03,  6.9417e-02,  6.0673e-02,\n",
      "        -5.0971e-02,  1.7481e-02, -7.4655e-03,  3.4951e-02, -1.3421e-02,\n",
      "         1.3608e-02,  4.7094e-02,  1.0794e-02,  7.8791e-02, -5.7541e-02,\n",
      "        -3.9177e-02,  5.4583e-02, -3.5982e-02,  5.1368e-02,  8.5814e-02,\n",
      "        -9.5923e-02, -4.7504e-02, -2.2657e-02,  1.4430e-02, -4.4008e-03,\n",
      "        -1.1413e-02,  2.5479e-02, -1.5094e-02,  2.4675e-02, -5.8582e-02,\n",
      "         9.8098e-02, -7.5598e-03, -7.8777e-03, -6.1616e-02, -5.5749e-02,\n",
      "         3.1458e-02,  1.9515e-03,  6.0178e-02,  8.1278e-02, -6.0119e-02,\n",
      "        -9.6536e-02, -2.9930e-02,  1.6005e-02,  5.6060e-02, -6.0936e-02,\n",
      "         7.6064e-02, -3.5598e-02,  9.1639e-03, -4.5128e-02, -5.4270e-02,\n",
      "        -1.6013e-01, -1.5079e-01, -1.6372e-01, -1.6992e-01, -2.2550e-01,\n",
      "        -2.0630e-01, -1.5898e-01, -4.1932e-01, -9.2180e-02, -2.2364e-01,\n",
      "        -2.0321e-01, -2.4220e-01, -1.1255e-01, -2.1209e-01, -1.5791e-01,\n",
      "        -1.9063e-01, -1.5383e-02, -2.4097e-01, -1.9999e-01, -2.7020e-01,\n",
      "        -2.0398e-01, -8.5186e-02, -3.1893e-01, -2.0688e-01, -2.2182e-01,\n",
      "        -8.4042e-02, -1.8165e-01, -2.4828e-01, -2.0968e-01, -1.0988e-01,\n",
      "        -2.2952e-01, -8.1507e-02, -2.0758e-01, -2.0775e-01, -1.2281e-01,\n",
      "        -5.5817e-02, -2.3465e-01, -2.6195e-01, -1.5604e-01, -2.1571e-01,\n",
      "        -1.4522e-01, -1.3733e-01, -1.6004e-01, -1.7747e-01, -2.1488e-01,\n",
      "        -1.6371e-01, -1.6030e-01, -2.4948e-01, -2.1718e-01, -1.9786e-01,\n",
      "        -2.1175e-01, -9.0023e-02, -6.0040e-02, -2.0449e-01, -2.0075e-01,\n",
      "        -1.6409e-01, -7.7259e-02, -2.9532e-01, -7.5731e-02, -6.5056e-02,\n",
      "         7.5978e-03, -1.9832e-01, -2.1607e-01, -2.1301e-01, -6.2120e-02,\n",
      "        -2.2714e-01, -1.5947e-01, -2.6279e-01, -1.1807e-01, -1.9690e-01,\n",
      "        -1.5496e-01, -1.2285e-01, -3.1263e-01, -1.8269e-01, -1.4257e-01,\n",
      "        -2.5302e-01, -2.3705e-01, -2.0309e-01, -2.2195e-01, -9.2276e-02,\n",
      "        -1.1653e-01, -1.6040e-01, -1.8953e-01, -6.8446e-02, -2.0809e-01,\n",
      "        -2.4437e-01, -2.2254e-01, -1.4950e-01, -1.5778e-01, -1.3016e-01,\n",
      "        -7.7422e-02, -6.1831e-02, -4.8562e-02, -1.6860e-01, -2.8356e-01,\n",
      "        -1.9654e-01, -1.9626e-01, -1.2115e-01, -1.2149e-01, -1.1051e-01,\n",
      "        -2.5896e-01, -2.6988e-01, -1.7246e-01, -1.7086e-01, -2.3336e-01,\n",
      "        -2.7351e-01, -3.4381e-01, -1.2471e-01, -2.5117e-01, -1.5541e-01,\n",
      "        -1.6429e-01, -2.1946e-01, -2.2087e-01, -1.8965e-01, -2.3465e-01,\n",
      "        -2.8677e-01, -2.0221e-01, -2.3702e-01, -2.7086e-01, -3.5039e-01,\n",
      "        -2.0238e-01, -2.2300e-01, -1.4579e-01, -1.0458e-01, -1.5048e-01,\n",
      "        -1.9541e-01, -1.2190e-01], device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_hh_l1_reverse', Parameter containing:\n",
      "tensor([-1.5295e-01, -1.5582e-01, -1.1114e-01, -1.2044e-01, -1.8836e-01,\n",
      "         4.0213e-02, -3.9636e-02, -1.4118e-01,  6.3306e-02, -2.0133e-01,\n",
      "        -2.3427e-01, -1.8358e-01,  2.4879e-01, -2.8794e-01, -2.8517e-01,\n",
      "        -1.6045e-01,  5.6759e-02,  5.4707e-02, -2.2074e-01,  7.4424e-02,\n",
      "        -2.9609e-01, -1.9579e-01, -1.7267e-01, -1.4249e-01, -1.2110e-01,\n",
      "        -7.3635e-02, -1.8567e-01, -2.1102e-01, -2.0391e-01, -2.3247e-01,\n",
      "        -1.8721e-01,  1.2753e-03, -5.0462e-02, -1.2953e-01, -2.4505e-01,\n",
      "        -3.3437e-02, -9.8451e-02, -1.8601e-01, -2.1449e-01, -1.9470e-01,\n",
      "        -9.1382e-03, -1.3506e-01, -2.4323e-01, -1.2248e-01, -1.5521e-01,\n",
      "        -2.0318e-01, -1.6794e-01, -1.9339e-01, -2.0383e-01, -9.3254e-03,\n",
      "        -2.0214e-01, -1.9173e-01, -6.3698e-02, -2.9976e-01, -1.8780e-02,\n",
      "        -2.6094e-01, -1.6871e-01, -2.9401e-01, -1.6075e-01, -1.0399e-01,\n",
      "        -1.7390e-01,  2.0300e-02, -1.5882e-01, -1.6664e-01, -5.3452e-02,\n",
      "        -2.6237e-01, -2.1343e-01, -2.1539e-01,  4.0874e-02, -1.1206e-01,\n",
      "        -1.0909e-01,  6.3332e-02, -5.3154e-02,  4.1531e-02, -1.8346e-01,\n",
      "        -1.8736e-01, -9.1554e-02,  4.1870e-02, -2.8807e-01, -1.1443e-01,\n",
      "        -3.6788e-02, -1.1727e-01, -1.3078e-01, -1.9858e-02, -2.3557e-01,\n",
      "        -1.7973e-01, -1.2167e-01, -2.1598e-01, -1.8215e-01, -1.1481e-01,\n",
      "        -1.0821e-01, -2.3046e-01, -9.5246e-02, -1.3423e-01, -1.7285e-01,\n",
      "        -3.6823e-01, -1.4185e-01, -1.2896e-01,  5.5591e-03, -2.8418e-01,\n",
      "        -2.4874e-01, -2.2842e-01, -2.4048e-01, -1.7485e-01, -1.3571e-01,\n",
      "        -9.3415e-02, -2.7145e-01, -6.4755e-02, -1.0464e-01,  6.4163e-02,\n",
      "        -1.5238e-01, -1.0846e-01, -2.2835e-01, -2.1487e-01, -1.7003e-01,\n",
      "        -1.3865e-01,  7.2606e-03, -2.1672e-01, -2.1676e-02, -2.9345e-01,\n",
      "        -1.2880e-01, -1.5460e-01, -1.4811e-01, -8.8599e-02, -1.5846e-01,\n",
      "        -9.6354e-02, -1.7506e-01, -1.7213e-01,  1.2259e+00,  8.6872e-01,\n",
      "         8.7365e-01,  1.0983e+00,  8.7516e-01,  9.5860e-01,  8.7790e-01,\n",
      "         8.6063e-01,  1.1170e+00,  8.3750e-01,  1.1713e+00,  8.1022e-01,\n",
      "         1.1551e+00,  1.1704e+00,  7.5639e-01,  8.4147e-01,  1.0966e+00,\n",
      "         9.0409e-01,  9.2131e-01,  8.7307e-01,  7.5169e-01,  8.5225e-01,\n",
      "         1.0204e+00,  1.1006e+00,  9.4455e-01,  9.6721e-01,  8.9342e-01,\n",
      "         7.8555e-01,  9.5005e-01,  8.3536e-01,  8.1481e-01,  1.0733e+00,\n",
      "         8.8772e-01,  8.1078e-01,  8.3964e-01,  9.0241e-01,  8.7489e-01,\n",
      "         8.3509e-01,  1.0710e+00,  7.4858e-01,  1.1428e+00,  7.6223e-01,\n",
      "         1.1585e+00,  8.6464e-01,  8.9032e-01,  8.3007e-01,  8.2285e-01,\n",
      "         9.2732e-01,  9.7679e-01,  9.9068e-01,  8.5312e-01,  7.2817e-01,\n",
      "         7.8881e-01,  1.2650e+00,  9.1782e-01,  8.9912e-01,  1.0696e+00,\n",
      "         1.2933e+00,  1.1469e+00,  7.9201e-01,  7.4443e-01,  1.2235e+00,\n",
      "         9.3859e-01,  1.2391e+00,  1.0722e+00,  1.0818e+00,  7.2817e-01,\n",
      "         8.7090e-01,  9.1306e-01,  8.9971e-01,  1.0185e+00,  1.0058e+00,\n",
      "         9.1361e-01,  1.0362e+00,  7.8937e-01,  9.5448e-01,  1.0123e+00,\n",
      "         1.0510e+00,  8.1006e-01,  8.0694e-01,  8.0811e-01,  1.0387e+00,\n",
      "         8.5145e-01,  8.2202e-01,  8.6605e-01,  9.2519e-01,  7.6133e-01,\n",
      "         9.9302e-01,  7.6289e-01,  7.5260e-01,  9.0982e-01,  8.8633e-01,\n",
      "         8.5336e-01,  8.1978e-01,  8.3643e-01,  8.7252e-01,  8.8456e-01,\n",
      "         6.6695e-01,  9.3047e-01,  8.2171e-01,  6.2774e-01,  9.2351e-01,\n",
      "         7.8654e-01,  8.6029e-01,  8.9677e-01,  9.9557e-01,  8.0343e-01,\n",
      "         1.0861e+00,  9.1513e-01,  9.7221e-01,  8.5641e-01,  1.1813e+00,\n",
      "         7.9516e-01,  1.2202e+00,  8.6395e-01,  8.7579e-01,  1.0394e+00,\n",
      "         8.1255e-01,  1.1811e+00,  1.0914e+00,  9.0257e-01,  7.1634e-01,\n",
      "         8.2340e-01,  9.1424e-01,  8.4564e-01,  9.8729e-01,  9.4590e-01,\n",
      "         1.2217e+00, -7.1641e-02, -3.4044e-03,  1.1018e-01, -1.5542e-02,\n",
      "        -2.8840e-02,  6.2272e-02,  4.9574e-02, -4.1769e-02, -9.4388e-02,\n",
      "        -6.3108e-02, -1.6485e-02, -1.4578e-02, -5.2249e-04, -1.8142e-02,\n",
      "        -4.0550e-02, -8.3113e-02,  7.8052e-02,  5.7460e-02, -1.0550e-01,\n",
      "         4.2196e-02,  8.6914e-03,  7.4367e-02,  3.9399e-02, -3.0022e-02,\n",
      "        -1.1657e-04, -4.9952e-03,  4.7743e-02, -1.1866e-02, -6.2616e-02,\n",
      "        -3.1819e-02, -2.6725e-02,  8.3507e-02,  3.0468e-02,  7.6056e-02,\n",
      "        -1.3513e-01,  9.4240e-03,  1.2233e-02, -4.7065e-02, -4.1845e-03,\n",
      "        -2.0901e-02,  7.1538e-02, -1.1730e-01,  3.3687e-02,  6.0242e-02,\n",
      "         5.0409e-02, -6.7334e-02, -7.1842e-02, -7.8734e-02, -1.2554e-02,\n",
      "        -3.7550e-02, -3.5105e-02,  1.1945e-02, -5.9007e-02,  8.6506e-02,\n",
      "         8.8213e-02,  3.3888e-02, -4.5356e-03,  4.1854e-02,  3.6998e-03,\n",
      "         6.2512e-03, -8.5746e-02, -5.1942e-02, -1.3068e-02,  4.5790e-02,\n",
      "         1.4276e-02, -3.2488e-02,  6.2249e-02, -8.0496e-02,  8.6311e-02,\n",
      "        -9.5075e-02,  1.7184e-02,  1.7046e-02,  7.9405e-02,  2.5371e-03,\n",
      "         1.2962e-02, -1.0745e-02, -1.0220e-01,  1.1335e-01,  3.1582e-02,\n",
      "        -2.9309e-03,  8.3130e-02, -6.5508e-03,  6.9417e-02,  6.0673e-02,\n",
      "        -5.0971e-02,  1.7481e-02, -7.4655e-03,  3.4951e-02, -1.3421e-02,\n",
      "         1.3608e-02,  4.7094e-02,  1.0794e-02,  7.8791e-02, -5.7541e-02,\n",
      "        -3.9177e-02,  5.4583e-02, -3.5982e-02,  5.1368e-02,  8.5814e-02,\n",
      "        -9.5923e-02, -4.7504e-02, -2.2657e-02,  1.4430e-02, -4.4008e-03,\n",
      "        -1.1413e-02,  2.5479e-02, -1.5094e-02,  2.4675e-02, -5.8582e-02,\n",
      "         9.8098e-02, -7.5598e-03, -7.8777e-03, -6.1616e-02, -5.5749e-02,\n",
      "         3.1458e-02,  1.9515e-03,  6.0178e-02,  8.1278e-02, -6.0119e-02,\n",
      "        -9.6536e-02, -2.9930e-02,  1.6005e-02,  5.6060e-02, -6.0936e-02,\n",
      "         7.6064e-02, -3.5598e-02,  9.1639e-03, -4.5128e-02, -5.4270e-02,\n",
      "        -1.6013e-01, -1.5079e-01, -1.6372e-01, -1.6992e-01, -2.2550e-01,\n",
      "        -2.0630e-01, -1.5898e-01, -4.1932e-01, -9.2180e-02, -2.2364e-01,\n",
      "        -2.0321e-01, -2.4220e-01, -1.1255e-01, -2.1209e-01, -1.5791e-01,\n",
      "        -1.9063e-01, -1.5383e-02, -2.4097e-01, -1.9999e-01, -2.7020e-01,\n",
      "        -2.0398e-01, -8.5186e-02, -3.1893e-01, -2.0688e-01, -2.2182e-01,\n",
      "        -8.4042e-02, -1.8165e-01, -2.4828e-01, -2.0968e-01, -1.0988e-01,\n",
      "        -2.2952e-01, -8.1507e-02, -2.0758e-01, -2.0775e-01, -1.2281e-01,\n",
      "        -5.5817e-02, -2.3465e-01, -2.6195e-01, -1.5604e-01, -2.1571e-01,\n",
      "        -1.4522e-01, -1.3733e-01, -1.6004e-01, -1.7747e-01, -2.1488e-01,\n",
      "        -1.6371e-01, -1.6030e-01, -2.4948e-01, -2.1718e-01, -1.9786e-01,\n",
      "        -2.1175e-01, -9.0023e-02, -6.0040e-02, -2.0449e-01, -2.0075e-01,\n",
      "        -1.6409e-01, -7.7259e-02, -2.9532e-01, -7.5731e-02, -6.5056e-02,\n",
      "         7.5978e-03, -1.9832e-01, -2.1607e-01, -2.1301e-01, -6.2120e-02,\n",
      "        -2.2714e-01, -1.5947e-01, -2.6279e-01, -1.1807e-01, -1.9690e-01,\n",
      "        -1.5496e-01, -1.2285e-01, -3.1263e-01, -1.8269e-01, -1.4257e-01,\n",
      "        -2.5302e-01, -2.3705e-01, -2.0309e-01, -2.2195e-01, -9.2276e-02,\n",
      "        -1.1653e-01, -1.6040e-01, -1.8953e-01, -6.8446e-02, -2.0809e-01,\n",
      "        -2.4437e-01, -2.2254e-01, -1.4950e-01, -1.5778e-01, -1.3016e-01,\n",
      "        -7.7422e-02, -6.1831e-02, -4.8562e-02, -1.6860e-01, -2.8356e-01,\n",
      "        -1.9654e-01, -1.9626e-01, -1.2115e-01, -1.2149e-01, -1.1051e-01,\n",
      "        -2.5896e-01, -2.6988e-01, -1.7246e-01, -1.7086e-01, -2.3336e-01,\n",
      "        -2.7351e-01, -3.4381e-01, -1.2471e-01, -2.5117e-01, -1.5541e-01,\n",
      "        -1.6429e-01, -2.1946e-01, -2.2087e-01, -1.8965e-01, -2.3465e-01,\n",
      "        -2.8677e-01, -2.0221e-01, -2.3702e-01, -2.7086e-01, -3.5039e-01,\n",
      "        -2.0238e-01, -2.2300e-01, -1.4579e-01, -1.0458e-01, -1.5048e-01,\n",
      "        -1.9541e-01, -1.2190e-01], device='cuda:0', requires_grad=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('encoder.pool.attn.vw', Parameter containing:\n",
      "tensor([[ 4.3445e-02],\n",
      "        [-3.3858e-02],\n",
      "        [-9.4259e-03],\n",
      "        [-2.2902e-02],\n",
      "        [-5.8698e-04],\n",
      "        [ 3.8060e-02],\n",
      "        [ 1.1113e-01],\n",
      "        [-3.1461e-02],\n",
      "        [-4.3245e-02],\n",
      "        [ 9.0865e-03],\n",
      "        [-2.4958e-03],\n",
      "        [ 2.9520e-02],\n",
      "        [ 4.1044e-02],\n",
      "        [-8.8958e-03],\n",
      "        [ 1.3872e-02],\n",
      "        [-1.8705e-02],\n",
      "        [ 1.0977e-02],\n",
      "        [ 1.1191e-02],\n",
      "        [-2.5137e-02],\n",
      "        [ 2.8345e-02],\n",
      "        [-6.8535e-02],\n",
      "        [-3.9301e-02],\n",
      "        [-1.0473e-01],\n",
      "        [-2.7028e-02],\n",
      "        [ 3.3108e-03],\n",
      "        [ 9.9822e-02],\n",
      "        [ 9.4455e-02],\n",
      "        [-1.2557e-01],\n",
      "        [ 6.5129e-02],\n",
      "        [ 8.3451e-02],\n",
      "        [-2.7889e-02],\n",
      "        [ 4.4682e-02],\n",
      "        [ 4.0399e-02],\n",
      "        [ 4.5862e-02],\n",
      "        [-1.3375e-02],\n",
      "        [ 1.3372e-02],\n",
      "        [ 7.2681e-02],\n",
      "        [ 1.7861e-02],\n",
      "        [ 7.2030e-03],\n",
      "        [-1.4992e-02],\n",
      "        [ 1.7782e-03],\n",
      "        [-4.3476e-03],\n",
      "        [-8.8386e-03],\n",
      "        [ 2.8121e-02],\n",
      "        [ 9.3427e-02],\n",
      "        [-4.2629e-04],\n",
      "        [ 2.3500e-02],\n",
      "        [ 5.2400e-02],\n",
      "        [-6.6227e-02],\n",
      "        [-7.9203e-03],\n",
      "        [-8.1651e-02],\n",
      "        [-1.5228e-02],\n",
      "        [-1.1085e-01],\n",
      "        [-5.2280e-02],\n",
      "        [ 8.1529e-02],\n",
      "        [-6.4972e-03],\n",
      "        [-4.4930e-02],\n",
      "        [-3.8849e-02],\n",
      "        [-5.0896e-02],\n",
      "        [-3.9120e-02],\n",
      "        [-6.4177e-02],\n",
      "        [-2.8938e-03],\n",
      "        [ 1.5433e-01],\n",
      "        [ 1.5116e-01],\n",
      "        [-1.0390e-01],\n",
      "        [ 2.2516e-03],\n",
      "        [-2.4620e-02],\n",
      "        [-9.7306e-02],\n",
      "        [ 3.2661e-02],\n",
      "        [-3.0974e-02],\n",
      "        [-2.5719e-03],\n",
      "        [ 3.9126e-02],\n",
      "        [ 1.3135e-02],\n",
      "        [-8.1182e-02],\n",
      "        [ 6.9794e-02],\n",
      "        [-4.2252e-03],\n",
      "        [ 5.7489e-03],\n",
      "        [ 2.4120e-02],\n",
      "        [-1.0614e-01],\n",
      "        [-2.8079e-02],\n",
      "        [ 1.4847e-02],\n",
      "        [-4.5018e-02],\n",
      "        [-1.6547e-02],\n",
      "        [-2.0564e-02],\n",
      "        [ 8.8192e-02],\n",
      "        [ 3.1030e-02],\n",
      "        [ 3.5097e-02],\n",
      "        [-9.6235e-02],\n",
      "        [-2.5217e-02],\n",
      "        [ 2.7450e-02],\n",
      "        [-3.5529e-02],\n",
      "        [ 1.5146e-02],\n",
      "        [-3.7517e-03],\n",
      "        [-6.0352e-02],\n",
      "        [ 1.9164e-02],\n",
      "        [-5.2327e-02],\n",
      "        [-1.6760e-02],\n",
      "        [ 1.1112e-01],\n",
      "        [ 3.8887e-02],\n",
      "        [-1.1594e-02],\n",
      "        [-6.2534e-02],\n",
      "        [ 8.5255e-02],\n",
      "        [-1.3192e-02],\n",
      "        [ 9.5367e-03],\n",
      "        [ 2.1261e-02],\n",
      "        [-1.1960e-01],\n",
      "        [-3.9119e-03],\n",
      "        [-8.6763e-02],\n",
      "        [-2.0807e-02],\n",
      "        [ 3.9688e-02],\n",
      "        [ 1.5080e-01],\n",
      "        [-1.1830e-03],\n",
      "        [-1.3188e-02],\n",
      "        [-1.9537e-02],\n",
      "        [ 2.1616e-02],\n",
      "        [-5.4538e-02],\n",
      "        [ 4.2669e-02],\n",
      "        [-9.4073e-03],\n",
      "        [-4.3934e-03],\n",
      "        [ 3.8178e-02],\n",
      "        [ 3.4744e-02],\n",
      "        [-3.4357e-02],\n",
      "        [-4.2332e-03],\n",
      "        [ 1.2286e-02],\n",
      "        [-3.7368e-02],\n",
      "        [ 3.8164e-02],\n",
      "        [-8.3429e-02],\n",
      "        [ 1.3025e-01],\n",
      "        [ 5.7674e-03],\n",
      "        [-1.0567e-01],\n",
      "        [ 1.2267e-02],\n",
      "        [-4.7271e-02],\n",
      "        [-7.1423e-02],\n",
      "        [ 2.4015e-03],\n",
      "        [ 5.1798e-02],\n",
      "        [-1.2326e-02],\n",
      "        [-8.3491e-02],\n",
      "        [-8.4686e-02],\n",
      "        [ 3.7822e-02],\n",
      "        [-5.8609e-02],\n",
      "        [ 8.2994e-03],\n",
      "        [-3.6676e-02],\n",
      "        [-2.0261e-02],\n",
      "        [ 6.1258e-02],\n",
      "        [ 1.6703e-02],\n",
      "        [-9.2086e-02],\n",
      "        [ 8.9344e-02],\n",
      "        [ 4.9004e-03],\n",
      "        [-1.2935e-01],\n",
      "        [ 7.5619e-02],\n",
      "        [ 1.8017e-02],\n",
      "        [ 7.0697e-02],\n",
      "        [-1.6710e-02],\n",
      "        [-7.8004e-03],\n",
      "        [-4.3698e-02],\n",
      "        [ 1.2813e-02],\n",
      "        [ 1.1983e-02],\n",
      "        [-7.9546e-02],\n",
      "        [ 2.5642e-02],\n",
      "        [ 2.9953e-02],\n",
      "        [-4.5963e-02],\n",
      "        [ 7.1383e-03],\n",
      "        [-5.5087e-02],\n",
      "        [ 8.0619e-02],\n",
      "        [-5.0081e-02],\n",
      "        [-4.6999e-02],\n",
      "        [ 3.0566e-02],\n",
      "        [ 5.8521e-03],\n",
      "        [-1.7340e-02],\n",
      "        [ 5.2617e-02],\n",
      "        [-1.4870e-02],\n",
      "        [-1.7369e-03],\n",
      "        [-1.0351e-02],\n",
      "        [ 1.9011e-03],\n",
      "        [ 1.6885e-02],\n",
      "        [ 8.9244e-02],\n",
      "        [ 9.5767e-02],\n",
      "        [ 1.2093e-01],\n",
      "        [ 1.5448e-04],\n",
      "        [-5.3644e-03],\n",
      "        [ 9.6342e-02],\n",
      "        [ 1.0792e-02],\n",
      "        [-3.9690e-03],\n",
      "        [ 4.4720e-02],\n",
      "        [ 1.8317e-02],\n",
      "        [ 3.1132e-02],\n",
      "        [-3.6379e-03],\n",
      "        [ 1.2177e-02],\n",
      "        [ 5.8790e-02],\n",
      "        [-1.8283e-02],\n",
      "        [-3.5716e-03],\n",
      "        [-6.5952e-02],\n",
      "        [ 9.0266e-02],\n",
      "        [-1.9792e-02],\n",
      "        [-7.2166e-02],\n",
      "        [-2.4071e-03],\n",
      "        [ 5.8571e-02],\n",
      "        [-7.5728e-02],\n",
      "        [-9.1131e-02],\n",
      "        [-9.8078e-02],\n",
      "        [-1.1927e-03],\n",
      "        [-6.6813e-02],\n",
      "        [ 2.3488e-02],\n",
      "        [ 3.6065e-02],\n",
      "        [ 1.4697e-04],\n",
      "        [-4.5049e-02],\n",
      "        [-1.7018e-02],\n",
      "        [ 4.6002e-02],\n",
      "        [-3.4125e-03],\n",
      "        [-2.1071e-02],\n",
      "        [-1.7208e-02],\n",
      "        [ 5.0398e-03],\n",
      "        [-6.8864e-02],\n",
      "        [-3.0491e-02],\n",
      "        [-8.5945e-02],\n",
      "        [-8.6883e-02],\n",
      "        [ 1.3582e-02],\n",
      "        [ 9.8166e-02],\n",
      "        [ 1.4443e-01],\n",
      "        [-5.3319e-02],\n",
      "        [-1.2505e-02],\n",
      "        [ 3.4436e-02],\n",
      "        [-3.8292e-02],\n",
      "        [-4.8791e-03],\n",
      "        [ 2.3990e-02],\n",
      "        [ 5.4202e-02],\n",
      "        [ 4.5219e-02],\n",
      "        [ 4.2130e-02],\n",
      "        [-1.4954e-01],\n",
      "        [-7.5918e-03],\n",
      "        [ 3.6050e-02],\n",
      "        [-6.3466e-02],\n",
      "        [ 3.5173e-02],\n",
      "        [ 1.1764e-03],\n",
      "        [-4.5213e-02],\n",
      "        [ 4.4726e-02],\n",
      "        [-1.0960e-03],\n",
      "        [-7.8353e-02],\n",
      "        [-5.5409e-03],\n",
      "        [ 9.8773e-02],\n",
      "        [-4.3989e-02],\n",
      "        [-6.8927e-02],\n",
      "        [-1.1740e-01],\n",
      "        [ 2.5367e-02],\n",
      "        [ 7.4177e-03],\n",
      "        [ 9.9297e-03],\n",
      "        [-4.0732e-02],\n",
      "        [-1.4264e-02],\n",
      "        [ 2.4900e-02],\n",
      "        [-5.2396e-03],\n",
      "        [ 8.8953e-03],\n",
      "        [-3.6565e-02],\n",
      "        [-1.5686e-02],\n",
      "        [-1.0962e-01],\n",
      "        [ 1.5600e-02],\n",
      "        [ 2.5775e-02],\n",
      "        [-6.5972e-03],\n",
      "        [-4.3226e-02],\n",
      "        [ 1.0518e-02],\n",
      "        [ 3.2459e-03],\n",
      "        [ 1.4286e-03],\n",
      "        [-3.8101e-03],\n",
      "        [ 7.9691e-02],\n",
      "        [ 4.1458e-02],\n",
      "        [-7.2161e-02],\n",
      "        [-1.6424e-03],\n",
      "        [-3.5867e-02],\n",
      "        [-3.6230e-04],\n",
      "        [ 3.5268e-03],\n",
      "        [-1.7034e-02],\n",
      "        [ 4.5538e-03],\n",
      "        [ 3.0355e-03],\n",
      "        [ 1.7873e-02],\n",
      "        [-6.6756e-02],\n",
      "        [ 2.9832e-03],\n",
      "        [ 1.2885e-01],\n",
      "        [-9.1156e-02],\n",
      "        [-8.1151e-02],\n",
      "        [-2.7193e-02],\n",
      "        [ 1.8809e-02],\n",
      "        [ 1.3166e-01],\n",
      "        [ 1.6828e-02],\n",
      "        [ 5.6280e-03],\n",
      "        [ 3.4950e-02],\n",
      "        [-2.2714e-02],\n",
      "        [-1.6207e-03],\n",
      "        [ 7.8674e-02],\n",
      "        [ 6.0327e-02],\n",
      "        [ 1.1808e-01],\n",
      "        [ 3.7524e-03],\n",
      "        [-2.1255e-02],\n",
      "        [-1.4207e-02],\n",
      "        [ 1.3323e-02],\n",
      "        [ 2.7745e-02],\n",
      "        [ 2.6987e-02],\n",
      "        [-1.0069e-03],\n",
      "        [ 1.5444e-01],\n",
      "        [ 1.1719e-01],\n",
      "        [ 5.2038e-03],\n",
      "        [ 4.3959e-02],\n",
      "        [-3.1395e-03],\n",
      "        [ 2.6446e-03],\n",
      "        [ 2.2946e-02],\n",
      "        [ 7.5978e-02],\n",
      "        [ 2.6373e-02],\n",
      "        [ 3.4581e-02],\n",
      "        [ 1.3802e-02],\n",
      "        [ 3.5659e-02],\n",
      "        [-4.5581e-02],\n",
      "        [-1.3736e-02],\n",
      "        [-1.3984e-02],\n",
      "        [ 6.5596e-03],\n",
      "        [-3.2999e-03],\n",
      "        [ 5.4341e-03],\n",
      "        [ 3.2848e-02],\n",
      "        [-4.4094e-02],\n",
      "        [ 9.0099e-03],\n",
      "        [-8.0594e-02],\n",
      "        [ 1.1224e-01],\n",
      "        [-1.3998e-02],\n",
      "        [-2.1768e-01],\n",
      "        [ 4.8865e-02],\n",
      "        [ 4.3922e-02],\n",
      "        [-6.7802e-02],\n",
      "        [-6.0286e-02],\n",
      "        [-4.4934e-03],\n",
      "        [ 9.3432e-03],\n",
      "        [ 9.2013e-03],\n",
      "        [-3.7903e-03],\n",
      "        [ 4.9344e-02],\n",
      "        [ 3.0752e-02],\n",
      "        [-4.2015e-02],\n",
      "        [ 7.9838e-02],\n",
      "        [-6.0834e-02],\n",
      "        [ 3.8318e-03],\n",
      "        [-2.3790e-02],\n",
      "        [ 1.2656e-01],\n",
      "        [ 1.0082e-01],\n",
      "        [-1.0187e-01],\n",
      "        [ 4.1337e-02],\n",
      "        [-2.4050e-02],\n",
      "        [-4.7618e-03],\n",
      "        [-8.5370e-03],\n",
      "        [-3.2045e-02],\n",
      "        [ 4.3920e-02],\n",
      "        [ 2.8212e-02],\n",
      "        [-1.4040e-02],\n",
      "        [-8.2734e-02],\n",
      "        [ 1.0107e-02],\n",
      "        [-6.8338e-03],\n",
      "        [ 8.7358e-03],\n",
      "        [-1.1634e-02],\n",
      "        [-8.8783e-02],\n",
      "        [-3.7546e-02],\n",
      "        [-3.4116e-02],\n",
      "        [-2.6657e-02],\n",
      "        [-8.2619e-02],\n",
      "        [ 1.9833e-02],\n",
      "        [-1.2049e-01],\n",
      "        [ 8.1300e-02],\n",
      "        [-2.3113e-02],\n",
      "        [ 2.5598e-03],\n",
      "        [ 4.7620e-02],\n",
      "        [ 1.5092e-03],\n",
      "        [-1.7339e-02],\n",
      "        [ 2.1676e-02],\n",
      "        [ 2.1949e-03],\n",
      "        [ 8.5631e-03],\n",
      "        [ 7.7566e-02],\n",
      "        [-3.6611e-03],\n",
      "        [ 1.9261e-02],\n",
      "        [-1.0836e-01],\n",
      "        [-6.2662e-02],\n",
      "        [-3.3112e-02],\n",
      "        [-1.4813e-03],\n",
      "        [ 1.8648e-02],\n",
      "        [-5.8412e-02],\n",
      "        [-1.2850e-02],\n",
      "        [-5.7761e-02],\n",
      "        [-6.2469e-02],\n",
      "        [-7.7859e-02],\n",
      "        [ 8.4010e-02],\n",
      "        [ 2.6901e-02],\n",
      "        [ 6.0934e-02],\n",
      "        [ 4.9641e-02],\n",
      "        [ 8.9622e-03],\n",
      "        [ 5.3568e-02],\n",
      "        [ 5.9680e-02],\n",
      "        [-3.2348e-02],\n",
      "        [ 9.6070e-02],\n",
      "        [-8.0828e-02],\n",
      "        [ 3.3341e-02],\n",
      "        [-3.6090e-03],\n",
      "        [ 2.0281e-02],\n",
      "        [-4.4851e-03],\n",
      "        [-1.9090e-02],\n",
      "        [-2.0742e-02],\n",
      "        [ 2.1459e-02],\n",
      "        [ 1.3423e-02],\n",
      "        [ 5.3088e-02],\n",
      "        [ 8.1415e-02],\n",
      "        [-3.3477e-02],\n",
      "        [-2.2587e-02],\n",
      "        [-8.9509e-02],\n",
      "        [-1.2518e-02],\n",
      "        [-5.8404e-02],\n",
      "        [ 6.6989e-02],\n",
      "        [-4.6041e-02],\n",
      "        [-2.3349e-02],\n",
      "        [ 6.6418e-02],\n",
      "        [-3.5502e-02],\n",
      "        [ 1.4238e-01],\n",
      "        [-6.4418e-02],\n",
      "        [-9.6107e-03],\n",
      "        [-6.6485e-03],\n",
      "        [-1.7572e-02],\n",
      "        [ 4.6269e-02],\n",
      "        [ 9.6375e-04],\n",
      "        [ 1.8476e-03],\n",
      "        [ 3.0869e-02],\n",
      "        [ 4.7440e-04],\n",
      "        [-5.5758e-03],\n",
      "        [-1.3667e-02],\n",
      "        [-6.1620e-03],\n",
      "        [ 3.5491e-02],\n",
      "        [-2.2099e-02],\n",
      "        [ 2.4400e-02],\n",
      "        [-9.2591e-02],\n",
      "        [-4.3395e-02],\n",
      "        [ 9.8815e-03],\n",
      "        [ 4.9634e-02],\n",
      "        [-4.5821e-02],\n",
      "        [-4.3978e-03],\n",
      "        [ 4.1165e-02],\n",
      "        [-6.4773e-03],\n",
      "        [ 3.8762e-04],\n",
      "        [ 1.4697e-01],\n",
      "        [ 7.4957e-02],\n",
      "        [ 7.7432e-04],\n",
      "        [-2.4897e-02],\n",
      "        [ 3.5463e-02],\n",
      "        [ 3.6448e-02],\n",
      "        [ 5.3023e-02],\n",
      "        [-1.5447e-02],\n",
      "        [ 3.7221e-02],\n",
      "        [-4.1884e-04],\n",
      "        [-2.1718e-02],\n",
      "        [ 1.9377e-02],\n",
      "        [ 1.2453e-01],\n",
      "        [ 6.6845e-02],\n",
      "        [-4.4548e-02],\n",
      "        [ 2.8570e-02],\n",
      "        [ 9.1379e-02],\n",
      "        [ 6.4465e-02],\n",
      "        [ 1.0470e-02],\n",
      "        [-3.4506e-02],\n",
      "        [ 1.3379e-01],\n",
      "        [ 1.3499e-02],\n",
      "        [-7.0759e-02],\n",
      "        [ 2.1324e-02],\n",
      "        [ 1.1600e-01],\n",
      "        [-4.4461e-02],\n",
      "        [-4.5783e-02],\n",
      "        [-1.2774e-02],\n",
      "        [ 5.7249e-02],\n",
      "        [-3.0121e-03],\n",
      "        [-1.0616e-02],\n",
      "        [ 6.2571e-02],\n",
      "        [-2.0879e-02],\n",
      "        [ 3.8836e-02],\n",
      "        [-2.0071e-02],\n",
      "        [-2.5543e-03],\n",
      "        [ 7.5107e-02],\n",
      "        [ 1.3765e-01],\n",
      "        [ 4.5508e-02],\n",
      "        [-8.6490e-03],\n",
      "        [-1.5844e-02],\n",
      "        [-8.9644e-03],\n",
      "        [-3.2381e-02],\n",
      "        [ 1.0134e-02],\n",
      "        [-7.0518e-03],\n",
      "        [-4.9137e-02],\n",
      "        [ 5.6002e-03],\n",
      "        [-4.6341e-02],\n",
      "        [ 5.4111e-02],\n",
      "        [-4.0371e-02],\n",
      "        [-1.1585e-01],\n",
      "        [ 7.5895e-02],\n",
      "        [-4.6679e-02],\n",
      "        [ 7.0619e-02],\n",
      "        [ 5.7990e-02],\n",
      "        [ 1.1317e-02],\n",
      "        [ 4.9420e-03],\n",
      "        [ 3.2791e-02],\n",
      "        [ 7.5402e-02],\n",
      "        [ 6.2660e-02],\n",
      "        [-1.3171e-01],\n",
      "        [ 3.4776e-02],\n",
      "        [-7.1269e-02],\n",
      "        [ 2.1283e-02],\n",
      "        [ 2.8785e-02],\n",
      "        [-1.3474e-02],\n",
      "        [ 3.3294e-02],\n",
      "        [ 8.5170e-02],\n",
      "        [ 1.0560e-01],\n",
      "        [-7.7289e-02],\n",
      "        [ 1.9308e-02],\n",
      "        [ 5.7705e-03],\n",
      "        [-1.9192e-02],\n",
      "        [-1.1535e-02],\n",
      "        [ 8.2818e-03],\n",
      "        [-1.3839e-01]], device='cuda:0', requires_grad=True))\n",
      "('encoder.pool.attn.b', Parameter containing:\n",
      "tensor([1.1741], device='cuda:0', requires_grad=True))\n",
      "('encoder.pool.attn.l1.weight', Parameter containing:\n",
      "tensor([[-0.0165,  0.0104, -0.1482,  ...,  0.1180,  0.1075,  0.0241],\n",
      "        [ 0.0174, -0.0419,  0.0764,  ...,  0.0012,  0.0456, -0.0217],\n",
      "        [-0.0753, -0.0282, -0.2567,  ...,  0.0417, -0.0164, -0.0421],\n",
      "        ...,\n",
      "        [-0.0407,  0.0107,  0.2273,  ..., -0.0584,  0.0993,  0.0478],\n",
      "        [-0.0763,  0.0084, -0.0692,  ..., -0.0107,  0.0005,  0.0263],\n",
      "        [ 0.0738,  0.0070,  0.1012,  ...,  0.0034, -0.0167, -0.0895]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.pool.attn.l1.bias', Parameter containing:\n",
      "tensor([ 4.4820e-01,  1.5100e-01,  1.3962e-01,  8.8463e-02, -1.3658e-01,\n",
      "         1.3787e-01, -1.0082e-01,  5.2023e-02,  1.1330e-01,  1.9295e-01,\n",
      "        -1.4016e-01,  5.1312e-02, -2.9420e-03, -1.0922e-01,  1.3831e-01,\n",
      "         1.6843e-01,  2.8080e-02, -1.7494e-02,  1.8619e-01, -4.7209e-02,\n",
      "         2.1740e-01,  9.2641e-02,  1.5069e-01,  1.0171e-01,  9.2133e-02,\n",
      "        -2.3566e-01, -2.6653e-02,  2.1244e-01, -2.0635e-01, -1.9863e-01,\n",
      "         7.0005e-02, -5.7624e-02, -2.2987e-01, -2.0422e-01, -2.3160e-02,\n",
      "        -4.8023e-02, -1.3898e-01,  6.2756e-02,  1.6173e-01,  3.4384e-02,\n",
      "        -2.4670e-01, -1.3362e-01, -4.1076e-02, -2.7162e-02, -6.2522e-02,\n",
      "         8.0340e-02,  1.4081e-02, -1.3265e-01,  1.5694e-01, -1.5334e-01,\n",
      "         1.0437e-01, -1.6101e-01,  1.9873e-01, -8.3626e-02, -1.2385e-01,\n",
      "         3.0796e-01,  2.5398e-01,  1.8122e-01,  2.5193e-01,  2.1584e-01,\n",
      "         2.7714e-02, -4.7948e-02, -8.3008e-02, -6.2382e-02,  2.1826e-01,\n",
      "         2.4546e-01, -3.5232e-02,  6.6038e-02, -2.2573e-01,  1.0182e-01,\n",
      "        -2.8279e-02, -4.5909e-01,  6.6034e-02,  2.5387e-01, -1.2347e-01,\n",
      "        -4.3278e-01,  1.9079e-01, -1.5295e-01,  2.7019e-01,  1.7037e-01,\n",
      "        -2.5946e-02,  1.4662e-01,  7.2585e-02, -7.2545e-03, -2.5919e-01,\n",
      "         4.7757e-02, -1.0684e-01, -2.9402e-01,  1.5692e-01,  1.9096e-02,\n",
      "         1.0567e-01, -1.3176e-01, -1.6772e-01,  1.0042e-01, -1.9083e-02,\n",
      "         1.5788e-01, -1.5692e-01, -2.9709e-01, -9.4948e-02, -2.0985e-01,\n",
      "        -3.6066e-02, -1.0328e-01,  1.5206e-01, -6.2192e-02, -1.1274e-01,\n",
      "         1.7135e-01, -1.1631e-01,  1.3393e-01,  9.0106e-02, -6.0797e-02,\n",
      "        -1.3578e-01, -2.9962e-01, -1.8183e-02, -1.0479e-01, -1.3375e-01,\n",
      "         6.7159e-02, -1.3114e-01,  6.7981e-02,  1.1594e-01, -7.1871e-02,\n",
      "         3.5429e-01,  7.8121e-02, -1.4654e-01, -8.7678e-03,  9.2237e-02,\n",
      "        -5.0676e-02,  4.2772e-01, -1.6697e-01,  2.2650e-01,  1.0954e-01,\n",
      "        -1.1722e-02,  1.5230e-01,  1.9489e-01,  2.8750e-01, -1.4107e-01,\n",
      "        -2.3843e-01,  1.3029e-01,  2.2204e-01, -2.6106e-01,  2.4602e-01,\n",
      "         8.6280e-02,  1.0856e-01, -4.2869e-02, -1.1934e-01,  3.7075e-01,\n",
      "         2.5280e-01, -1.9957e-01,  2.3631e-01,  2.4543e-01, -2.3775e-01,\n",
      "        -1.3836e-01, -2.4221e-02, -1.4131e-01, -2.0120e-01,  2.4110e-01,\n",
      "        -2.5051e-02, -2.1218e-02,  9.8576e-02,  2.2480e-02,  1.6781e-01,\n",
      "         3.9687e-02,  2.2636e-01,  2.3089e-01, -1.9049e-01,  5.6333e-02,\n",
      "         1.3517e-01, -7.5695e-02,  2.3700e-01,  7.2901e-03, -1.7475e-01,\n",
      "        -1.0592e-01,  1.1223e-01,  1.1113e-02, -6.3176e-02, -9.0403e-02,\n",
      "        -1.1233e-01, -1.1033e-01, -2.4616e-01, -2.2973e-01, -3.2190e-02,\n",
      "        -9.1174e-02,  2.4639e-02,  2.0205e-01,  3.0464e-01, -9.4849e-04,\n",
      "        -1.0205e-01, -5.6022e-02,  1.3431e-01,  1.2711e-01,  1.5757e-02,\n",
      "        -1.9860e-01,  1.6093e-01, -1.6328e-01, -2.8511e-01,  2.6826e-01,\n",
      "         1.4024e-01, -6.3769e-02,  5.5827e-02,  2.7603e-01,  1.4295e-02,\n",
      "         4.5955e-01,  1.6304e-01, -3.5937e-02, -1.5588e-01, -6.5206e-02,\n",
      "        -4.1554e-01, -3.3190e-02, -6.0875e-03, -5.2081e-02, -1.1325e-01,\n",
      "         4.0036e-02,  1.5939e-01, -2.2632e-01,  1.1272e-01,  1.5697e-01,\n",
      "         6.2394e-02,  2.3517e-01, -1.5038e-01, -1.2191e-01,  2.6040e-01,\n",
      "        -4.6269e-03, -1.9958e-01,  9.7399e-02,  1.8924e-02, -2.4135e-03,\n",
      "        -2.6366e-01,  2.2224e-01, -1.3653e-01,  1.1275e-01,  4.7087e-02,\n",
      "        -2.1923e-01,  2.4567e-01,  4.5994e-01,  7.3791e-02,  7.6986e-02,\n",
      "        -1.7683e-01,  8.5937e-02, -1.8256e-01,  2.2681e-02, -2.0469e-01,\n",
      "         1.4201e-01,  1.0305e-01,  2.1644e-01,  4.4925e-01, -7.5122e-02,\n",
      "         4.6490e-03,  2.8990e-01,  8.0069e-02,  2.5602e-01, -1.7432e-01,\n",
      "         1.2471e-01,  1.7177e-01,  2.0991e-01,  2.2948e-01, -1.1643e-01,\n",
      "        -2.5545e-02, -1.5100e-01,  2.8819e-01,  6.6970e-02,  1.7893e-01,\n",
      "         3.4369e-01, -7.3327e-02, -2.8977e-01, -9.3918e-02,  1.2653e-01,\n",
      "        -6.0848e-02,  8.6465e-02,  2.1212e-01, -6.2710e-03, -3.0264e-01,\n",
      "         7.5163e-02, -4.7998e-02, -6.3163e-02,  1.9127e-01,  8.8242e-02,\n",
      "        -1.9052e-01,  1.9704e-01, -1.7823e-02, -1.5789e-01, -1.2855e-01,\n",
      "        -2.3805e-01,  3.0087e-01,  1.2534e-02, -1.1200e-01, -2.9014e-02,\n",
      "        -1.2442e-01, -2.1440e-01, -1.5340e-01, -1.8811e-01,  6.4016e-02,\n",
      "         7.6039e-02,  2.3276e-01,  3.4310e-02, -8.5518e-02,  3.6667e-05,\n",
      "        -2.5747e-01, -8.9811e-02, -1.5865e-01, -9.4940e-02, -4.8022e-02,\n",
      "         1.2764e-01,  5.7563e-02,  1.9329e-01,  1.2366e-01,  1.6046e-01,\n",
      "        -1.7540e-01,  1.8785e-02, -8.5021e-02,  8.8343e-02, -1.7186e-01,\n",
      "         6.6172e-02,  2.9366e-01, -7.5586e-02, -2.1809e-01, -1.5787e-01,\n",
      "         1.6439e-01,  1.8864e-01,  1.0066e-01,  1.4062e-02,  3.6703e-02,\n",
      "         2.2949e-01, -5.2326e-02, -2.0438e-01,  2.1063e-01,  1.8156e-01,\n",
      "        -1.0393e-01, -9.8886e-03,  2.2228e-01, -8.2950e-02,  2.6287e-02,\n",
      "         2.4088e-02,  1.0220e-01, -1.6441e-01,  1.6383e-01, -1.2533e-01,\n",
      "        -9.4818e-02, -1.3778e-01, -2.6478e-01,  1.7245e-01, -1.4764e-01,\n",
      "        -1.7203e-01, -4.7775e-02, -3.2335e-01,  1.7390e-01, -1.1568e-01,\n",
      "        -1.9866e-01, -3.7303e-01,  2.3675e-01,  2.7877e-01, -1.0651e-01,\n",
      "         1.7820e-01, -1.0925e-02, -4.9001e-02,  1.3754e-01,  1.1448e-01,\n",
      "        -2.2818e-02,  1.4768e-01,  1.3896e-01, -1.0851e-02, -1.2169e-01,\n",
      "        -2.7153e-02,  8.6575e-03, -5.5056e-02,  1.3224e-01,  9.9676e-02,\n",
      "        -1.4459e-01,  8.2602e-02, -7.2508e-02,  2.8433e-02,  2.1444e-01,\n",
      "        -1.0315e-01,  1.3732e-01,  7.3901e-02,  2.6964e-02, -1.9808e-01,\n",
      "         1.6142e-02,  1.3385e-01, -5.8879e-04,  1.8012e-01,  8.2639e-02,\n",
      "         1.7779e-01, -2.6677e-02,  2.7521e-01, -5.0441e-02, -2.2269e-02,\n",
      "         3.1115e-01, -1.1294e-01, -7.0675e-02,  7.2192e-02, -2.0145e-01,\n",
      "        -2.1210e-01,  3.3419e-02, -1.3560e-01,  5.3130e-02, -8.0773e-02,\n",
      "         1.3966e-01,  6.8503e-02, -2.6972e-01,  3.0312e-01, -8.8129e-02,\n",
      "        -2.1444e-01, -3.2715e-02,  5.0655e-02,  1.0221e-01,  1.4924e-01,\n",
      "         1.5769e-01, -1.1148e-01, -7.8599e-04, -1.3953e-02, -8.5853e-02,\n",
      "         1.8930e-01, -6.0853e-02, -1.5151e-02,  3.2247e-01,  2.8687e-01,\n",
      "        -5.7569e-02, -1.2278e-01,  3.1268e-02, -1.4075e-01, -9.2864e-02,\n",
      "        -8.9734e-03, -8.4740e-02, -2.4785e-01,  1.0111e-01, -2.1432e-01,\n",
      "         4.6800e-02, -1.5728e-01,  1.2697e-01, -8.6267e-02,  2.5371e-02,\n",
      "         9.0475e-03,  1.6486e-01,  3.5702e-02, -1.4724e-01, -1.5343e-01,\n",
      "         1.5223e-01, -1.8863e-01, -2.1966e-01,  3.8516e-01,  9.3445e-02,\n",
      "         4.2497e-01,  3.9045e-02, -2.8217e-01,  1.1073e-01, -1.1922e-02,\n",
      "         1.7221e-01,  8.1863e-02, -1.1461e-01, -1.6178e-01,  9.1582e-02,\n",
      "         3.7592e-02, -1.0864e-01, -9.2470e-02,  1.8042e-03, -8.1931e-02,\n",
      "         2.4695e-01, -1.1202e-01,  3.4020e-02,  1.6776e-01, -1.0385e-01,\n",
      "        -2.4746e-01,  2.8965e-01,  2.3183e-01, -5.9880e-02, -1.6765e-01,\n",
      "         7.1734e-02, -5.8550e-02, -4.1387e-03,  6.4163e-02,  1.3540e-02,\n",
      "        -6.1502e-03, -3.1709e-01, -1.3665e-01, -2.5076e-01, -4.5497e-02,\n",
      "        -1.7344e-01, -1.3337e-01, -4.6399e-01,  2.1921e-01,  1.6683e-01,\n",
      "         1.0652e-01, -2.1642e-01,  1.5606e-01,  2.6556e-02, -2.8046e-01,\n",
      "         1.0329e-01,  1.4808e-01, -1.2324e-01, -1.7180e-02, -2.8388e-01,\n",
      "        -1.2094e-01,  1.2498e-01, -5.4315e-02, -2.0736e-01, -3.3714e-02,\n",
      "        -1.3588e-01,  2.4620e-01, -1.0870e-01,  1.0618e-01,  4.2379e-02,\n",
      "        -7.9024e-02,  1.7413e-01,  1.2298e-01, -9.8422e-02, -9.4907e-02,\n",
      "         1.1172e-01, -1.7667e-01,  1.1306e-01, -3.0793e-02, -6.1713e-02,\n",
      "        -1.3233e-02,  1.7092e-01], device='cuda:0', requires_grad=True))\n",
      "('projection.0.weight', Parameter containing:\n",
      "tensor([[ 0.0360, -0.0235,  0.0234,  ..., -0.1064,  0.0324,  0.0349],\n",
      "        [ 0.0878, -0.1646, -0.0335,  ...,  0.1612, -0.0712, -0.0037],\n",
      "        [-0.0350,  0.0290, -0.0723,  ...,  0.0462, -0.0430,  0.0484],\n",
      "        ...,\n",
      "        [-0.0247,  0.1150, -0.1480,  ..., -0.0648,  0.1063, -0.0479],\n",
      "        [-0.0523,  0.1430, -0.1239,  ..., -0.0088, -0.0306,  0.0050],\n",
      "        [-0.0408,  0.1458, -0.1523,  ..., -0.0584,  0.0387,  0.0569]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('projection.0.bias', Parameter containing:\n",
      "tensor([ 0.0581,  0.1584,  0.1442,  0.1050,  0.0976,  0.0395, -0.0333, -0.1761,\n",
      "        -0.0118,  0.1065,  0.0677,  0.2184,  0.0553, -0.0766, -0.0770,  0.1697,\n",
      "        -0.0142, -0.0149, -0.0520,  0.0717,  0.0989,  0.1103, -0.0068, -0.0711,\n",
      "         0.1863,  0.0551,  0.1457,  0.0628, -0.1083,  0.2182,  0.0428,  0.0734,\n",
      "         0.2189,  0.1006,  0.1050,  0.0189,  0.1612, -0.0193, -0.0024, -0.0972,\n",
      "        -0.0287,  0.0170,  0.0665, -0.0245,  0.1096, -0.0554, -0.0719,  0.0935,\n",
      "        -0.0705, -0.1254], device='cuda:0', requires_grad=True))\n",
      "('projection.3.weight', Parameter containing:\n",
      "tensor([[-1.4351e-01,  3.8197e-02,  3.2546e-02, -1.4141e-01, -1.1273e-01,\n",
      "         -1.5747e-01,  1.8207e-01,  1.4453e-02, -1.9118e-01, -7.3658e-02,\n",
      "          1.5009e-02,  4.2385e-02, -1.4782e-01, -1.8899e-03,  1.8770e-01,\n",
      "         -1.0611e-02,  1.3417e-01,  5.9213e-02,  1.8801e-01, -7.7973e-02,\n",
      "         -2.1863e-01, -6.2674e-02,  2.5527e-02, -8.3184e-02, -8.4801e-02,\n",
      "         -1.0077e-01, -8.2154e-02, -1.8160e-01, -4.4753e-02, -9.2602e-03,\n",
      "         -1.9031e-01, -1.6069e-01,  1.0570e-02, -6.7106e-02, -1.6643e-01,\n",
      "          4.2025e-02, -2.1200e-02,  1.7391e-01, -2.9508e-02,  1.5337e-01,\n",
      "          1.2347e-01,  1.1051e-01, -1.7984e-01,  6.5631e-02,  7.1871e-02,\n",
      "          3.7131e-02,  1.2907e-02,  5.5086e-02,  1.6313e-01,  2.8073e-02],\n",
      "        [-1.3449e-02, -1.5186e-01, -1.2410e-01, -1.2895e-01, -1.2997e-01,\n",
      "          1.8692e-02,  3.4200e-02, -1.2311e-02,  8.1447e-02, -1.3798e-01,\n",
      "         -1.0218e-02,  4.9603e-02, -7.5598e-02,  1.5357e-02,  4.2588e-02,\n",
      "         -3.5395e-01,  3.2552e-02,  6.6722e-03,  2.4100e-02, -9.5176e-02,\n",
      "          2.3596e-02, -1.6188e-01, -4.8919e-02,  6.6791e-02,  8.9089e-02,\n",
      "         -9.5901e-02, -1.2595e-01,  8.6103e-02,  6.3628e-02, -1.3970e-01,\n",
      "          2.7707e-02,  3.1062e-03, -2.8051e-01, -1.0024e-01, -9.2290e-03,\n",
      "          4.1834e-02, -2.1007e-01,  2.9359e-02,  3.7194e-02,  3.9104e-02,\n",
      "          6.3986e-02,  3.5356e-02,  7.3006e-02,  5.6293e-02, -1.1506e-01,\n",
      "          3.5591e-02,  1.2352e-02, -3.8728e-02,  1.5992e-02,  8.6011e-02],\n",
      "        [-1.1468e-01, -3.6956e-02, -2.1979e-01, -6.2744e-02, -1.4859e-01,\n",
      "         -8.1664e-02,  9.9857e-02,  4.8493e-02, -1.0819e-01, -9.5630e-02,\n",
      "          8.4707e-02, -1.7193e-01, -1.4233e-01,  7.9666e-02,  1.0771e-01,\n",
      "          3.6511e-02,  9.4789e-02, -3.0872e-02,  1.0918e-01, -7.5842e-02,\n",
      "         -1.0280e-01, -1.4597e-01, -2.0767e-01, -9.1485e-03, -1.5397e-01,\n",
      "         -8.2547e-02, -6.6374e-02, -4.0151e-02,  2.9060e-02, -1.9072e-01,\n",
      "         -5.5304e-02, -7.9869e-02, -6.1598e-02, -1.1181e-01, -8.3288e-02,\n",
      "         -1.4981e-01, -1.4136e-01,  5.1513e-02,  3.1047e-02,  1.0627e-01,\n",
      "          4.3538e-02,  1.0449e-01, -1.8724e-02,  9.5107e-03, -5.2811e-02,\n",
      "         -9.9907e-03,  1.1583e-01, -7.1624e-02,  8.7621e-02,  9.3584e-02],\n",
      "        [-5.4739e-02, -1.9998e-01,  2.5440e-02, -1.4143e-01, -5.5398e-02,\n",
      "         -1.3174e-01,  4.5551e-02, -9.2710e-03,  4.8647e-02, -2.0091e-01,\n",
      "         -2.2926e-01,  2.3577e-01, -7.2968e-02, -3.4099e-01,  3.3192e-02,\n",
      "         -2.5285e-01, -1.0069e-01,  1.1597e-01,  3.5576e-02, -8.1391e-02,\n",
      "         -1.1768e-02, -1.4654e-01, -9.5214e-02, -2.7620e-01, -2.2396e-01,\n",
      "          2.1255e-02, -1.5619e-01, -4.3840e-02,  1.4581e-02,  1.6744e-02,\n",
      "          6.7060e-02,  3.7623e-02, -1.6382e-01, -1.7134e-01, -5.3957e-02,\n",
      "          1.6118e-01, -1.1624e-01,  9.6027e-02, -2.7942e-01,  1.9225e-02,\n",
      "          2.9643e-02,  3.2089e-02,  2.0215e-02,  5.1698e-02, -4.6549e-01,\n",
      "          9.6343e-02, -2.2130e-01, -4.5664e-02,  1.1264e-02,  9.8053e-03],\n",
      "        [-4.8816e-02,  7.1554e-02, -4.2414e-02, -1.0457e-01, -1.1701e-01,\n",
      "         -4.0846e-02,  3.0115e-02,  1.1483e-01, -8.1916e-02, -4.8884e-02,\n",
      "         -2.0478e-01, -2.2875e-01, -8.7833e-02,  3.8713e-02,  5.9941e-02,\n",
      "         -3.5165e-03,  4.2422e-02,  1.1332e-01,  3.4594e-02, -1.2537e-01,\n",
      "         -9.4587e-02, -1.7212e-01,  6.3413e-02,  1.5382e-02, -5.7284e-02,\n",
      "         -1.3897e-01, -1.5960e-01, -1.4037e-01,  3.6430e-02, -9.3504e-02,\n",
      "         -2.0275e-02, -4.8199e-02, -1.6133e-01, -1.2052e-01, -6.0896e-02,\n",
      "          1.4094e-02, -8.8203e-02,  4.4805e-02,  5.7470e-02,  7.8863e-02,\n",
      "          5.4376e-02, -1.2316e-02, -3.4106e-02,  7.0535e-02,  5.0123e-02,\n",
      "          6.3021e-02, -1.0586e-01, -1.0076e-01,  7.4586e-02,  5.5378e-02],\n",
      "        [ 2.6678e-02, -3.3115e-01, -3.2480e-03, -1.3977e-01, -1.0990e-01,\n",
      "         -1.4232e-02,  2.6474e-02,  1.1852e-01, -4.4930e-04, -1.3917e-01,\n",
      "         -2.5953e-01, -1.6994e-01, -1.0647e-01, -9.4010e-02, -1.1965e-02,\n",
      "         -2.2297e-01, -4.9078e-02, -2.8354e-01,  3.5280e-02, -4.9841e-02,\n",
      "         -1.3851e-01, -8.5846e-02,  2.4902e-03,  7.7001e-02, -1.0427e-01,\n",
      "         -4.6882e-02, -1.4392e-01, -7.5684e-02,  6.8669e-02, -4.0885e-03,\n",
      "         -7.5308e-02,  8.7259e-05, -2.2155e-01,  8.4005e-03, -1.3224e-01,\n",
      "          1.0397e-02, -1.3669e-01, -2.2125e-02, -1.7738e-01,  4.8762e-02,\n",
      "          4.5465e-02, -1.0672e-01,  5.1803e-03,  9.6284e-02,  9.2778e-02,\n",
      "         -1.8447e-02, -1.1763e-01,  1.2771e-01,  2.9863e-02,  1.0322e-01]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('projection.3.bias', Parameter containing:\n",
      "tensor([-1.9964, -4.6998, -2.8844, -5.7746, -2.9490, -4.7510], device='cuda:0',\n",
      "       requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for p in model.named_parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from collections import defaultdict\n",
    "from allennlp.common.tqdm import Tqdm\n",
    "\n",
    "def dict_append(d: Dict[str, List], upd: Dict[str, Any]) -> Dict[str, List]:\n",
    "    for k, v in upd.items(): d[k].append(v)\n",
    "\n",
    "def tonp(tsr): return tsr.detach().cpu().numpy()\n",
    "        \n",
    "class Predictor:\n",
    "    def __init__(self, model: Model, iterator: DataIterator,\n",
    "                 cuda_device: int=-1) -> None:\n",
    "        self.model = model\n",
    "        self.iterator = iterator\n",
    "        self.cuda_device = cuda_device\n",
    "        \n",
    "    def _extract_data(self, batch) -> Dict[str, np.ndarray]:\n",
    "        out_dict = self.model(**batch)\n",
    "        lens = tonp(get_text_field_mask(batch[\"tokens\"]).sum(1))\n",
    "        \n",
    "        if self.model.loss._get_name()==\"MBernLossWithLogits\":\n",
    "            preds = tonp(out_dict[\"marginal_probs\"])\n",
    "        else:\n",
    "            preds = expit(tonp(out_dict[\"class_logits\"]))\n",
    "        \n",
    "        return {\n",
    "                \"preds\": preds,\n",
    "                \"oov_ratio\": tonp((batch[\"tokens\"][\"tokens\"] == 1).sum(1)) / lens,\n",
    "                \"lens\": lens,\n",
    "               }\n",
    "        \n",
    "    def _postprocess(self, predictions: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        return {k: np.concatenate(v, axis=0) for k, v in predictions.items()}\n",
    "    \n",
    "    @gpu_mem_restore\n",
    "    def predict(self, ds: Iterable[Instance]) -> np.ndarray:\n",
    "        pred_generator = self.iterator(ds, num_epochs=1, shuffle=False)\n",
    "        self.model.eval()\n",
    "        pred_generator_tqdm = Tqdm.tqdm(pred_generator,\n",
    "                                        total=self.iterator.get_num_batches(ds))\n",
    "        preds = defaultdict(list)\n",
    "        with torch.no_grad():\n",
    "            for batch in pred_generator_tqdm:\n",
    "                batch = nn_util.move_to_device(batch, self.cuda_device)\n",
    "                dict_append(preds, self._extract_data(batch))\n",
    "        return self._postprocess(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BasicIterator\n",
    "seq_iterator = BasicIterator(batch_size=64)\n",
    "seq_iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horrible solution to the shuffling problem with BasicIterator\n",
    "# TODO: Solve more elegantly?\n",
    "if not config.bucket:\n",
    "    del train_ds; import gc; gc.collect()\n",
    "    if config.val_ratio > 0.0:\n",
    "        train_ds = reader.read(DATA_ROOT / \"train_wo_val.csv\")\n",
    "    else:\n",
    "        train_ds = reader.read(DATA_ROOT / \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2494/2494 [04:30<00:00,  9.36it/s]\n",
      "100%|██████████| 1000/1000 [01:59<00:00,  6.61it/s]\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, seq_iterator, cuda_device=0 if USE_GPU else -1)\n",
    "train_meta = predictor.predict(train_ds) \n",
    "train_preds = train_meta.pop(\"preds\")\n",
    "test_meta = predictor.predict(test_ds)\n",
    "test_preds = test_meta.pop(\"preds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_df = pd.read_csv(DATA_ROOT / \"test_proced.csv\")\n",
    "test_labels = tst_df[label_cols].values\n",
    "test_texts = tst_df[\"comment_text\"].values\n",
    "if config.testing:\n",
    "    test_labels = test_labels[:len(test_ds), :]\n",
    "    test_texts = test_texts[:len(test_ds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, thres=0.5):\n",
    "        if isinstance(thres, float):\n",
    "            self.thres = np.ones(len(label_cols)) * thres\n",
    "        else:\n",
    "            self.thres = thres\n",
    "    \n",
    "    def _to_metric_dict(self, t: np.ndarray, y: np.ndarray, thres: float) -> Dict:\n",
    "        tn, fp, fn, tp = confusion_matrix(t, y >= thres).ravel()\n",
    "        return {\"auc\": roc_auc_score(t, y),\n",
    "                \"f1\": f1_score(t, y >= thres),\n",
    "                \"acc\": accuracy_score(t, y >= thres),\n",
    "                \"tnr\": tn / len(t), \"fpr\": fp / len(t),\n",
    "                \"fnr\": fn / len(t), \"tpr\": tp / len(t),\n",
    "                \"precision\": tp / (tp + fp), \"recall\": tp / (tp + fn),\n",
    "        }\n",
    "\n",
    "    def _stats_per_quadrant(self, tgt, preds, \n",
    "                            metadata: Dict[str, np.ndarray],\n",
    "                            texts: np.ndarray=None):\n",
    "        out_data = {}\n",
    "        for i, lbl in enumerate(label_cols):\n",
    "            # get indicies of each quadrant`\n",
    "            preds_bin = preds[:, i] >= self.thres[i]\n",
    "            quads = {\n",
    "                \"tp\": np.where((tgt[:, i] == 1) & preds_bin)[0],\n",
    "                \"fp\": np.where((tgt[:, i] == 0) & preds_bin)[0],\n",
    "                \"tn\": np.where((tgt[:, i] == 0) & ~preds_bin)[0],\n",
    "                \"fn\": np.where((tgt[:, i] == 1) & ~preds_bin)[0],\n",
    "            }\n",
    "            \n",
    "            # get stats for metadata\n",
    "            out_data[lbl] = {}\n",
    "            for quad, qidxs in quads.items():\n",
    "                quad_data = {}\n",
    "                for k, full_data in metadata.items():\n",
    "                    data = full_data[qidxs]\n",
    "                    for metric in [\"mean\", \"std\", \"min\", \"max\"]:\n",
    "                        if len(data) > 0:\n",
    "                            quad_data[f\"{k}_{metric}\"] = getattr(data, metric)()\n",
    "                        else:\n",
    "                            quad_data[f\"{k}_{metric}\"] = np.nan\n",
    "\n",
    "                out_data[lbl][quad] = quad_data\n",
    "            \n",
    "            # do error analysis\n",
    "            if texts is not None:\n",
    "                for quad, qidxs in quads.items():\n",
    "                    quad_preds = preds[qidxs, i]\n",
    "                    if len(quad_preds) == 0: continue\n",
    "                    if quad in [\"tp\", \"fp\"]:\n",
    "                        out_data[lbl][quad][\"most_confident\"] = texts[quad_preds.argmax()]\n",
    "                        out_data[lbl][quad][\"most_confident_prob\"] = quad_preds.max()\n",
    "                        out_data[lbl][quad][\"least_confident\"] = texts[quad_preds.argmin()]\n",
    "                        out_data[lbl][quad][\"least_confident_prob\"] = quad_preds.min()\n",
    "                    else:\n",
    "                        out_data[lbl][quad][\"most_confident\"] = texts[quad_preds.argmin()]\n",
    "                        out_data[lbl][quad][\"most_confident_prob\"] = quad_preds.min()\n",
    "                        out_data[lbl][quad][\"least_confident\"] = texts[quad_preds.argmax()]\n",
    "                        out_data[lbl][quad][\"least_confident_prob\"] = quad_preds.max()\n",
    "        return out_data        \n",
    "    \n",
    "    @gpu_mem_restore\n",
    "    def evaluate(self, tgt: np.ndarray, preds: np.ndarray,\n",
    "                 trn_tgt: np.ndarray, trn_preds: np.ndarray,\n",
    "                 metadata: Dict[str, np.ndarray]={}, \n",
    "                 texts: np.ndarray=None) -> Dict:\n",
    "        \"\"\"\n",
    "        Metadata: Data about the inputs (e.g. length, OOV ratio)\n",
    "        \"\"\"\n",
    "        train_label_metrics = {}\n",
    "        label_metrics = {}\n",
    "                \n",
    "        # get per-label stats\n",
    "        for i, lbl in enumerate(label_cols):\n",
    "            train_label_metrics[lbl] = self._to_metric_dict(trn_tgt[:, i],\n",
    "                                                            trn_preds[:, i],\n",
    "                                                            self.thres[i])\n",
    "            label_metrics[lbl] = self._to_metric_dict(tgt[:, i], preds[:, i],\n",
    "                                                      self.thres[i])\n",
    "            print(f\"========{lbl}=========\")\n",
    "            print(label_metrics[lbl])\n",
    "        \n",
    "        # get global stats\n",
    "        for mtrc in label_metrics[\"toxic\"].keys():\n",
    "            label_metrics[f\"global_{mtrc}\"] = \\\n",
    "                np.mean([label_metrics[col][mtrc] for col in label_cols])\n",
    "            \n",
    "        # get per-label-quadrant stats\n",
    "        quad_stats = self._stats_per_quadrant(tgt, preds, metadata=metadata, texts=texts)\n",
    "        if len(quad_stats) > 0:\n",
    "            for c in label_cols:\n",
    "                label_metrics[c][\"quad_stats\"] = quad_stats[c]\n",
    "\n",
    "        label_metrics[\"train\"] = train_label_metrics,\n",
    "        return label_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anna/neuralnlp/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Compute best threshold based on training data\n",
    "if config.compute_thres_on_test:\n",
    "    lbls, pds = test_labels, test_preds\n",
    "else:\n",
    "    lbls, pds = train_labels, train_preds\n",
    "    \n",
    "thres = np.zeros(len(label_cols))\n",
    "best_scores = np.zeros(len(label_cols))\n",
    "for i, col in enumerate(label_cols):\n",
    "    best_score = -1\n",
    "    best_thres = -1\n",
    "    for x in np.linspace(0, 1.0, num=999):\n",
    "        scr = f1_score(lbls[:, i], pds[:, i] >= x)\n",
    "        if scr > best_score:\n",
    "            best_thres = x\n",
    "            best_score = scr\n",
    "    thres[i] = best_thres\n",
    "    best_scores[i] = best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.31863727, 0.21342685, 0.37074148, 0.27354709, 0.34569138,\n",
       "       0.23346693])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========toxic=========\n",
      "{'auc': 0.9733541359182014, 'f1': 0.6668260609683204, 'acc': 0.9128763012285473, 'tnr': 0.8256900809653318, 'fpr': 0.07912094782581512, 'fnr': 0.008002750945637562, 'tpr': 0.08718622026321549, 'precision': 0.5242481203007519, 'recall': 0.915927750410509}\n",
      "========severe_toxic=========\n",
      "{'auc': 0.9903388001586789, 'f1': 0.41945773524720886, 'acc': 0.9886210884991716, 'tnr': 0.9845103004157679, 'fpr': 0.00975335271499578, 'fnr': 0.00162555878583263, 'tpr': 0.00411078808340367, 'precision': 0.2965050732807215, 'recall': 0.7166212534059946}\n",
      "========obscene=========\n",
      "{'auc': 0.9819203584019629, 'f1': 0.6903379993171731, 'acc': 0.9574697552283598, 'tnr': 0.9100628340992216, 'fpr': 0.03224545937666073, 'fnr': 0.010284785394979524, 'tpr': 0.04740692112913814, 'precision': 0.5951726844583988, 'recall': 0.8217285288539691}\n",
      "========threat=========\n",
      "{'auc': 0.9946744059404063, 'f1': 0.5338645418326693, 'acc': 0.9963424927318766, 'tnr': 0.994248022757823, 'fpr': 0.002453968551689643, 'fnr': 0.0012035387164337742, 'tpr': 0.002094469974053581, 'precision': 0.46048109965635736, 'recall': 0.6350710900473934}\n",
      "========insult=========\n",
      "{'auc': 0.980067077035197, 'f1': 0.6871038675462423, 'acc': 0.9621901278564506, 'tnr': 0.9206758573259558, 'fpr': 0.025758854606270906, 'fnr': 0.01205101753727844, 'tpr': 0.041514270530494855, 'precision': 0.6171003717472119, 'recall': 0.7750218850306391}\n",
      "========identity_hate=========\n",
      "{'auc': 0.990039558319306, 'f1': 0.6067159931701765, 'acc': 0.9891994122979775, 'tnr': 0.9808684235205852, 'fpr': 0.008002750945637562, 'fnr': 0.0027978367563850073, 'tpr': 0.008330988777392228, 'precision': 0.5100478468899522, 'recall': 0.7485955056179775}\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(thres=thres)\n",
    "label_metrics = evaluator.evaluate(\n",
    "    test_labels, test_preds,\n",
    "    train_labels, train_preds,\n",
    "    metadata=test_meta, texts=test_texts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9850657226289589"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_metrics[\"global_auc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic': {'auc': 0.9733541359182014,\n",
       "  'f1': 0.6668260609683204,\n",
       "  'acc': 0.9128763012285473,\n",
       "  'tnr': 0.8256900809653318,\n",
       "  'fpr': 0.07912094782581512,\n",
       "  'fnr': 0.008002750945637562,\n",
       "  'tpr': 0.08718622026321549,\n",
       "  'precision': 0.5242481203007519,\n",
       "  'recall': 0.915927750410509,\n",
       "  'quad_stats': {'tp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 51.85550376479025,\n",
       "    'lens_std': 138.96824621374864,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 4950,\n",
       "    'most_confident': '\" \\n ::As far as I know, linguistic/dialectal studies aren\\'t affected by migrations and subsequent Dialect Levelling ; the latter only creates new \"\"koines\"\" without affecting old dialects.  \\n ::As I said before, EALL/Versteegh should be considered as a highly valuable secondary source instead of making WP:SYNTH using different sources   \"',\n",
       "    'most_confident_prob': 0.9999964,\n",
       "    'least_confident': 'Please stop. If you continue to vandalize Wikipedia, as you did to Homosexuality, you will be blocked from editing.',\n",
       "    'least_confident_prob': 0.31865752},\n",
       "   'fp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 63.30462267878309,\n",
       "    'lens_std': 131.18146855023957,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 3307,\n",
       "    'most_confident': 'The anti vandal tool is great.  Your popups.js tool is also great.',\n",
       "    'most_confident_prob': 0.9998815,\n",
       "    'least_confident': \"By the way- your usr page opinions are ludicrous!  Everyone on this planet has suddenly gone crazy except me.  That's why I'm proclaiming myself president of the world.\",\n",
       "    'least_confident_prob': 0.31866205},\n",
       "   'tn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 85.93741718093364,\n",
       "    'lens_std': 122.29637286086809,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 1640,\n",
       "    'most_confident': \"How old are you?  12?  Please don't vandalize WP like you did here.\",\n",
       "    'most_confident_prob': 9.125983e-07,\n",
       "    'least_confident': '== what are you talking about == \\n\\n I did not make this edit you cretin. I have no idea who Shaun Mckay is, this has nothing to do with me. Get your facts straight before you go about making accusations you mug.',\n",
       "    'least_confident_prob': 0.31862262},\n",
       "   'fn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 63.896484375,\n",
       "    'lens_std': 96.21565452222617,\n",
       "    'lens_min': 3,\n",
       "    'lens_max': 1505,\n",
       "    'most_confident': 'FJTFBYT6RHDFS6HF NBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHJDDDDDNNXCXDHDHV H   FTHCVYD',\n",
       "    'most_confident_prob': 0.00056022464,\n",
       "    'least_confident': '\"* Introduction and Getting started \\n * Contributing to Wikipedia \\n * The five pillars of Wikipedia \\n * How to edit a page and How to develop articles \\n * How to create your first article  \\n * Simplified Manual of Style \\n\\n You may also want to take the Wikipedia Adventure, an interactive tour that will help you learn the basics of editing Wikipedia. \\n\\n Please remember to sign your messages on talk pages by typing four tildes (~~~~); this will automatically insert your username and the date. If you need help, check out Wikipedia:Questions, ask me on , or  to ask for help on your talk page, and a volunteer should respond shortly. Again, welcome!  —\\xa0\\xa0  \\n\\n  \\n \"',\n",
       "    'least_confident_prob': 0.31561834}}},\n",
       " 'severe_toxic': {'auc': 0.9903388001586789,\n",
       "  'f1': 0.41945773524720886,\n",
       "  'acc': 0.9886210884991716,\n",
       "  'tnr': 0.9845103004157679,\n",
       "  'fpr': 0.00975335271499578,\n",
       "  'fnr': 0.00162555878583263,\n",
       "  'tpr': 0.00411078808340367,\n",
       "  'precision': 0.2965050732807215,\n",
       "  'recall': 0.7166212534059946,\n",
       "  'quad_stats': {'tp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 164.57794676806083,\n",
       "    'lens_std': 292.03822964552336,\n",
       "    'lens_min': 3,\n",
       "    'lens_max': 1345,\n",
       "    'most_confident': \"* I'll be there.   ,\",\n",
       "    'most_confident_prob': 0.97217494,\n",
       "    'least_confident': '== Reason for banning throwing == \\n\\n This article needs a section on /why/ throwing is banned. At the moment, to a non-cricket fan, it seems kind of arbitrary.',\n",
       "    'least_confident_prob': 0.21838482},\n",
       "   'fp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 82.22916666666667,\n",
       "    'lens_std': 213.23259175784338,\n",
       "    'lens_min': 2,\n",
       "    'lens_max': 2321,\n",
       "    'most_confident': '==Stop doubting me!== \\n Please stop saying that user:Gabeyc is my puppet!',\n",
       "    'most_confident_prob': 0.99165404,\n",
       "    'least_confident': \".  One man's bemuddlement need not be another man's beffudlement.\",\n",
       "    'least_confident_prob': 0.21354105},\n",
       "   'tn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 80.59056630733326,\n",
       "    'lens_std': 122.12412395331467,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 4950,\n",
       "    'most_confident': 'Are you unable to pee with a half filled bladder???',\n",
       "    'most_confident_prob': 1.28889575e-08,\n",
       "    'least_confident': 'Thanks for the compliments.  Will be busy for the next couple days, but will review the page as requested.  Best to you.',\n",
       "    'least_confident_prob': 0.21333516},\n",
       "   'fn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 109.51923076923077,\n",
       "    'lens_std': 225.2711605688213,\n",
       "    'lens_min': 4,\n",
       "    'lens_max': 1032,\n",
       "    'most_confident': ':::If Ollie or others think that one list of the oldest people we know about is too long, the easy answer is to raise the cutoff age. 110 is purely a round number and a full 12 years shorter then the record. We can make it the top 1000 or top 500 or everyone above 115 - tell us what the maximum list size is and we can set a threshold.',\n",
       "    'most_confident_prob': 0.0010129319,\n",
       "    'least_confident': 'But this is not the article about government position but about the reaction. Add positions to 2008 Kosovo declaration of independence or Foreign relations of Kosovo.',\n",
       "    'least_confident_prob': 0.21332274}}},\n",
       " 'obscene': {'auc': 0.9819203584019629,\n",
       "  'f1': 0.6903379993171731,\n",
       "  'acc': 0.9574697552283598,\n",
       "  'tnr': 0.9100628340992216,\n",
       "  'fpr': 0.03224545937666073,\n",
       "  'fnr': 0.010284785394979524,\n",
       "  'tpr': 0.04740692112913814,\n",
       "  'precision': 0.5951726844583988,\n",
       "  'recall': 0.8217285288539691,\n",
       "  'quad_stats': {'tp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 59.832509066930434,\n",
       "    'lens_std': 171.00319861983567,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 4950,\n",
       "    'most_confident': '== Incorrect information about Bury Lawn School == \\n\\n  \\n Bury Lawn School was not founded by Rennee Soskin in 1970 as stated in your History section. \\n\\n Bury Lawn School was founded in the Spring of 1940 by Miss Iris Aylward Michell and Miss Katharine Myrtle Michell as in depth research and Webber Independent School will verify. The School was sold in 1970 and continued to be run from the same address Bury Lawn, Union Street, Newport Pagnell until 1987 when it was moved to Milton Keynes where in 2010/2011 the name was changed to Webber Independent School. \\n\\n A K Michell.',\n",
       "    'most_confident_prob': 0.9998313,\n",
       "    'least_confident': '\" \\n\\n == JustBeenPaid == \\n\\n Some editors have been adding information about JustBeenPaid (JBP), an \"\"indefinitely sustainable\"\" program, to this article as an example of a legitimate HYIP. The only thing that makes this program sustainable is that its proprietor can \"\"restart\"\" it by switching from a Ponzi scheme (2% per day payout) to a pyramid scheme (matrixes where $80 comes in on the bottom level and $60 is paid out on the top level). Both of these schemes operate by taking money from people within the program to pay people within the program, although the pyramid scheme part actually pays out less by funneling money to the operators. JBP meets both the definition of an HYIP by promising unrealistically high returns in a short period of time (as a Ponzi scheme) and the definition of a pyramid scheme by generating income purely from within the program.   \"',\n",
       "    'least_confident_prob': 0.3713269},\n",
       "   'fp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 69.99369849733398,\n",
       "    'lens_std': 134.92899142921408,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 1990,\n",
       "    'most_confident': 'There are many equivalente wikipedia pages with alike companies in other countries (e.g. Ecotricity, Good Energy, Green Mountain Energy). Another utility company called Dong Energy is also there. \\n\\n I work in the company but I have tried to be neutral. Input for improvements are very welcome.',\n",
       "    'most_confident_prob': 0.9963303,\n",
       "    'least_confident': \"::::Sounds absolutely like a wilderness paradise. So so sad for nature that it happens to be a highly strategic wilderness paradise... we couldn't leave Alaska pristine no matter how badly we wanted to.\",\n",
       "    'least_confident_prob': 0.3708748},\n",
       "   'tn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 82.8527239626271,\n",
       "    'lens_std': 121.74423533251704,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 3307,\n",
       "    'most_confident': '\" \\n ==Planet X crazies== \\n Do you think it might be a good idea to get Planet X semi-protected? The closer we get to 2012, the more that page is going to get savaged by well-meaning freaks. I have to say the whole thing freaks me out, especially when you realise that it all started with a woman who thinks she has an alien implant in her brain.  \"',\n",
       "    'most_confident_prob': 1.5709783e-06,\n",
       "    'least_confident': '==Minor Biblical Figure== \\n Should this article be moved to the Minor Biblical Figures listing (where variant spellings Penuel and Penueul are already shown)?',\n",
       "    'least_confident_prob': 0.37031302},\n",
       "   'fn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 49.02279635258358,\n",
       "    'lens_std': 93.17689697978913,\n",
       "    'lens_min': 3,\n",
       "    'lens_max': 1064,\n",
       "    'most_confident': '. \\n\\n :::What makes you think being old is going to help stop a smoking addiction?  Vonnegut attended self-help classes and actually did quit smoking for a while, but has since then decided to start again',\n",
       "    'most_confident_prob': 0.0016240615,\n",
       "    'least_confident': '\" \\n\\n You wrote: \\n :This kid is a well-known Birmingham athlete so I\\'m struggling to see why the article on him has been deleted. It was a misinformed and disrespectful move on your part. \\n The article as written is simply about a young athlete who was good at basketball in school but now is enrolled in university, just like hundreds of thousands of other people in the world. Please read Wikipedia:What Wikipedia is not, specifically, \"\"Wikipedia is not an indiscriminate collection of information.\"\" You may also wish to read the guidelines for biography articles. If you wish to re-create the article, please do so but bear in mind that it must be verifiable from reliable sources. Regards, e  {} \"',\n",
       "    'least_confident_prob': 0.3704142}}},\n",
       " 'threat': {'auc': 0.9946744059404063,\n",
       "  'f1': 0.5338645418326693,\n",
       "  'acc': 0.9963424927318766,\n",
       "  'tnr': 0.994248022757823,\n",
       "  'fpr': 0.002453968551689643,\n",
       "  'fnr': 0.0012035387164337742,\n",
       "  'tpr': 0.002094469974053581,\n",
       "  'precision': 0.46048109965635736,\n",
       "  'recall': 0.6350710900473934,\n",
       "  'quad_stats': {'tp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 76.45522388059702,\n",
       "    'lens_std': 177.6429260880629,\n",
       "    'lens_min': 2,\n",
       "    'lens_max': 1169,\n",
       "    'most_confident': 'REDIRECT Talk:Ponhook Lake 10',\n",
       "    'most_confident_prob': 0.88139886,\n",
       "    'least_confident': '\" \\n\\n You wrote: \\n\\n You will need to cite reliable sources for the assertions \"\"far too large to be chemical\"\" and \"\"no endothermic storage events\"\". I do not think any source can conclusively estabish either point . . . \\n\\n It is very easy to establish both points, and there are hundreds of papers with proof of them. \\n\\n 1. The limits of chemical storage (electron bonds) are well understood. The most energy dense chemicals, such as gasoline, store 4 to 8 eV per atom. Cold fusion reactions have often produced far more than this, ranging from ~100 to ~100,000 eV per atom of the material in the cell, and in all cases there is no chemical fuel before the reaction, and no chemical ash after it finishes. Cell with a few grams of inert material in them have produced hundreds of megajoules of heat  as much as several kilograms of gasoline. if you were to burn the cell contents, they would produce only a tiny fraction of the heat. In fact, you could burn the cell, the table, and all books in the room and still get that much chemical energy. Therefore, chemistry is ruled out. \\n\\n 2. Any calorimeter can measure an endothermic event as accurately as it measures an exothermic event. For example, all calorimeters used in these studies show the endothermic formation of Pd-D at the beginning of the experiment. Since the calorimeters shows the exothermic excess heat, they would also show the endothermic storage that proceeds it. They have never done this. The heat balance is always zero before the heat bursts, and after. \\n\\n - Jed Rothwell \\n\\n  \\n HERE IS more useful information that someone keeps erasing: \\n\\n Please note that Cal Tech (Lewis et al.) is a false negative, as pointed out by Noninski, Miles and Fleischmann. This is described in several papers, for example this one, p. 20: \\n\\n http://lenr-canr.org/acrobat/MilesMisoperibol.pdf \\n\\n Also, the name is Julian Schwinger (with ch). See: \\n\\n http://lenr-canr.org/acrobat/SchwingerJcoldfusiona.pdf \\n\\n - Jed Rothwell, Librarian, LENR-CANR.org\"',\n",
       "    'least_confident_prob': 0.27401903},\n",
       "   'fp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 81.96815286624204,\n",
       "    'lens_std': 179.07211017299994,\n",
       "    'lens_min': 2,\n",
       "    'lens_max': 1194,\n",
       "    'most_confident': '*Unfortunately, the site you took the text from suffers from really poor grammar and spelling.',\n",
       "    'most_confident_prob': 0.9054733,\n",
       "    'least_confident': ': see Axial tilt',\n",
       "    'least_confident_prob': 0.27576172},\n",
       "   'tn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 81.01402295236598,\n",
       "    'lens_std': 124.50161569563741,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 4950,\n",
       "    'most_confident': \"::: Both Henry Winkler and Donny Most were credited in Co-Starring roles for Season One; their credits appeared first during the closing titles, indicating that they were considered more as recurring characters.  In Season Two, however, their billings changed and they were now included in the opening titles as fourth and fifth leads respectively, and as a result their characters became more central to the show.  It was the Third Season that saw Winkler's billing change again from fourth lead to second lead, after Ron Howard.\",\n",
       "    'most_confident_prob': 6.932598e-09,\n",
       "    'least_confident': 'Thanks - that change can work.',\n",
       "    'least_confident_prob': 0.27207252},\n",
       "   'fn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 74.37662337662337,\n",
       "    'lens_std': 167.2747983254865,\n",
       "    'lens_min': 4,\n",
       "    'lens_max': 946,\n",
       "    'most_confident': \"This is a REDIRECT discussion page. Please do not leave article related discussion here.  Please go to the article's discussion page.\",\n",
       "    'most_confident_prob': 0.0007309919,\n",
       "    'least_confident': '\" \\n\\n :Not sure whether this is notable enough to be mentioned in the article, but you\\'re right – versions later than 6 (2001) can open files created by all previous versions, but save files only in its own format, with a suffix of \"\".msX\"\", where X is the version number (7, 8, 9, 10, 11, 12...). This is intentional, of course. Besides, all versions but 7 save files in a compressed format. ☭共产主义万岁★ \"',\n",
       "    'least_confident_prob': 0.27026254}}},\n",
       " 'insult': {'auc': 0.980067077035197,\n",
       "  'f1': 0.6871038675462423,\n",
       "  'acc': 0.9621901278564506,\n",
       "  'tnr': 0.9206758573259558,\n",
       "  'fpr': 0.025758854606270906,\n",
       "  'fnr': 0.01205101753727844,\n",
       "  'tpr': 0.041514270530494855,\n",
       "  'precision': 0.6171003717472119,\n",
       "  'recall': 0.7750218850306391,\n",
       "  'quad_stats': {'tp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 57.32003012048193,\n",
       "    'lens_std': 145.95078542943497,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 2321,\n",
       "    'most_confident': 'also WP:NOT, WP:NOPRICES -)',\n",
       "    'most_confident_prob': 0.9904987,\n",
       "    'least_confident': 'P.S. I just read you talk page. I can see you are for America and have strong opinions regarding America and those who would have us up go down the drain and up in flames, while they are holding the match, rather quickly. I stand with you on some things, but this is not one. Sometimes a duck is just a duck and a crazy is just a crazy.',\n",
       "    'least_confident_prob': 0.3463199},\n",
       "   'fp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 60.67050970873787,\n",
       "    'lens_std': 182.21566506846258,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 4950,\n",
       "    'most_confident': '\" \\n\\n :::Well, I was responding primarily to Shykee.  If you look up above, I started by agreeing with something Shirhadassah made - that is my main point.  I do not in any way dispute your claims about reform, I didn\\'t think anyone else was.  The conversation, it seemed to me, was about Orthodoxy not Reform Judaism.  As to Orthodoxy as a movement that was founded, well, the question is, is Orthodoxy a movement?  If so, it was indeed founded by people, all movements are.  If you are saying that there was a large ppopulation of Jews during this time that did not belong to any \"\"movement,\"\" well, sure, who could dispute that?  I do not doubt that between 1600 and 1800 there were many Jews in Eastern Europe you lived according to the Shulchan Aruch, more or less.  And I agree that such a group does not have a \"\"founder.\"\"  But what could it possibly mean to lable them as \"\"Orthodox?\"\"  Did they identify themselves as Orthodox?  Or, are you identifying these people with the Orthodox movement that emerged in the 1800s?  Are you making the same claims Shykee seemed to be making (I may have misread him) about the Orthodox, then the same points the Conservatives make about the orthodox movement - that it reflects a particular moment in jewish history and does not reflect an unbroken continuity with the past, let alone rabbinic Judaism, well, then those points still apply.    |   \"',\n",
       "    'most_confident_prob': 0.9692239,\n",
       "    'least_confident': \"== Note: 1 & 2 Copied from Talk:Airbus G-EUPC == \\n\\n ==1. Comments== \\n New page created, feel free to add or edit.   \\n :wHILST I would dearly love to see this page blossom, I cannot seem to be able to justify it. I feel that the subject should have no more mention than a parqagraph in an article on the 2012 Games as a whole. Having said that I amhoping that I spark a discussion that will enable the coorect path to be chosen. P.S. my ex-wife is going to be one of the torch-bearers!!   \\n ::Actually I kinda agree. Let's leave here for a few days and then see what LOCOG use the plane for next! If it disappears back into the BA schedules perhaps I should move the whole page to WikiNews??   \\n :::On further reflection I have moved most of the non-aviation material to a new section of 2012 Summer Olympics torch relay.   \\n\\n == 2. Deletion request == \\n\\n  wrote concern = Not notable outside of the 2012 Summer Olympics torch relay, failing WP:GNG and WP:ONEEVENT.  Any useful information can be merged there (which most of it is already).  No real need for a redirect as it is an unlikely search term. \\n :However she/he did not put any discussion on this page to discuss this deletion first.   reveals s/he has a tendency to delete articles without a full discussion on appropriate pages and wikiprojects eg Wikipedia:AVIA. Her/his reasons for deletion may be valid but please let's have an informed discussion first.   \\n ::Wikipedia has several modern aircraft used for specific purposes, eg VC-137C SAM 26000 in the USA. These specially painted Airbus aircraft will have several purposes during the Summer Olympics, not just the arrival of the Olympic Torch. Can we wait a bit before Speedy Deletion? Perhaps widen scope of article to include all 9 Airbus aircraft with livery designed by Pascal Anson.   \\n :::Lidos, as I (just) mentioned on your talk page, the WP:PROD process does not require discussion (and is completely different from the Speed Deletion process.  There was nothing wrong my actions, nor was any offensive intended towards you nor an attempt to circumvent any discussion.  lease review what PROD means before making further allegations.  If you think an article deserves a deletion discussion (aka an WP:AFD process), whether or not you believe the article should (or would/will) be deleted it is perfectly acceptable to remove the PROD tag without comment (though preferably a reason should be given.). Finally, I do not agree that the article (even if expanded in scope) will ever meet WP:GNG and intend to bring the article to WP:AFD.\",\n",
       "    'least_confident_prob': 0.3458449},\n",
       "   'tn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 83.04938627913688,\n",
       "    'lens_std': 122.05640573353928,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 3307,\n",
       "    'most_confident': '\"As someone with mixed family ethnicity, I\\'m going to step in for a second: the article in question needs to be re-read in order to stop the complaints pronto.  When one reads it, it is necessary to understand the history of US-USSR relations at the time.  In the US, communism was bad - indeed, communists were the enemy - \"\"pinkos\"\".  Everybody in the US hated the commies.  Picture this imaginary conversation between am American (A) and someone from the USSR (U): \\n * A: You communists are despicable! \\n * U: No, we\\'re not, it\\'s simply a different economic concept... \\n * A: But you make people work, take their wages, and redistribute them! That\\'s sick! \\n * U: To each according to their needs, from each according to their ability... \\n * A: Pinko bastards, you\\'re war-mongering freaks! \\n * U: We\\'re freaks? This coming from a country where they still find it OK to lynch people of colour?!??!? \\n Very much a \"\"don\\'t point your finger at someone else, unless your own hands are clean\"\" kind of message that yes, indeed, is propaganda intended to deflect American concerns about the communist state. Truly, the message in and of itself by mocking the American belief that lynching was OK, actually says that lynching was bad.  Lynching is therefore not a joke, but America (who turned a blind eye to lynching their own residents) were the joke. \\xa0Bwilkins / BMW\\xa0  \\n\\n \"',\n",
       "    'most_confident_prob': 9.495699e-08,\n",
       "    'least_confident': \"carlos mencia is really funny have you seen his retard impressions. hehe they're great \\n\\n he's really funny because he's like \\n\\n i am edgy and push boundaries also i am MEXICAN. mexicans are lazy there is some self-deprecating humor here \\n\\n he is almost as great as dane cook\",\n",
       "    'least_confident_prob': 0.3456197},\n",
       "   'fn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 49.36186770428016,\n",
       "    'lens_std': 83.78862942795044,\n",
       "    'lens_min': 3,\n",
       "    'lens_max': 1539,\n",
       "    'most_confident': \"*Oppose, it doesn't hurt to mention Bach, some searching for just French Suites will find the same,\",\n",
       "    'most_confident_prob': 0.0009765932,\n",
       "    'least_confident': '\" \\n\\n == Illusions about \"\"standard Croatian\"\" == \\n\\n The debate on this page is purely political and as disconnected from scientific discussion as are apples and watermelons. So much ignorance and nonsense flies around that it is next to impossible to start anywhere to debunk this ignorance so strongly displayed by almost everyone. \\n\\n Like this \"\"pearl\"\" from a certified \"\"Iknoweverything\"\": \\n\\n There is no such thing as \"\"Slavic origins\"\". Anyone making a claim that tens of thousands of year old genetic markers have anything to do a decades- or centuries-old identities, that have been constructed in the historical period as can be abundantly attested in the respective literature, is a brain-dead moron. The spread of Slavic identity was primarily through linguistic assimilation (the word Slavs originally meant \"\"those who speak like us\"\"). Croats from northern Croatia are much more genetically close to neighboring Hungarians or Austrians than they are to remote Russians. The same is valid for any other imaginary collective that we call nation-state or civilizational milieu. Ivan Štambuk (talk)  \\n\\n For your information Slaven, not Slav is the original word. And the word \"\"Slaven\"\" means in a number of germanic languages (consult german or english dictionaries for more info) a \"\"dirty boy\"\". Think of a prepubescent or a teenager\\'s face and clothes after a game of football and you may get the picture. Only in your twisted and ignorant head the word \"\"Slav\"\" means something. ANd of all things you claim that it means \"\"those who speak like us\"\"... \\n\\n In which language? Do tell.  \\n\\n Ignorant.  \\n\\n How about tis other \"\"pearl\"\" from local \"\"authority\"\" on everything: \\n\\n \"\"Croats from northern Croatia are much more genetically close to neighboring Hungarians or Austrians than they are to remote Russians. The same is valid for any other imaginary collective that we call nation-state or civilizational milieu.\"\" \\n\\n A number of genetical studies have been conducted since the war in Jugoslavia and every single one has confirmed that 51% of Croats, especially those near Austria and Hungary are genetically Slavens (Slavs, if you prefer) and as such are identical to Russians, not to Austrians or Hungarians. Many of these studies have been conducted by Croatians too, so above \"\"pearl\"\" is only indicative of the size of Stambuk\\'s ignorance, already verified too many times to mention here. \\n\\n Another genius, among many, tty29a quotes here and then misses the point completely: \\n\\n \"\"    Another cold fact is that Ča- Kaj- and I-kavski are not even dialects. Rather, they are deflections of adjectives (in Ikavski, ijekavski, ekavski) and/or pronouns (in sto-sta- kaj- ca- kavski). Fundamental grammatical rules are identical and do not change beyond what could only be described as dialectal differences between Ča- Kaj Sto- Sta- I- kavski. \\n     Let me see if I got this right. They\\'re not dialects but they have dialectal differences? You seem to be in disagreement with your self. Deflections of adjectives and pronouns? Really? Also, if the grammatical systems are identical, why do they have different case and tense systems? I honestly hope you\\'re just a troll. tty29a:talk \"\" \\n\\n Have a look at this from the quote by tty29a: \"\"Fundamental grammatical rules are identical and do NOT change beyond what could only be described as dialectal differences between Ča- Kaj Sto- Sta- I- kavski.\"\" \\n\\n And tty29a misses the NOT completely to come up with this nonsense: \"\"They\\'re not dialects but they have dialectal differences? You seem to be in disagreement with your self.\"\" \\n\\n The only one in disagreement with himself is tty29a. The other person who wrote the quoted text made it CLEAR that they are NOT dialects. \\n\\n And the story goes on and on, forever and ever. Few and far inbetween with some understanding and knowledge, but for the most part just a bunch of ignorants debating things which they do not have the capacity, intellectual one, to grasp and process before they grab the microphone and start making fools of themselves. \\n\\n As a linguist with 27 years of experience, I am apalled with the level of ignorance that reigns here. You people have no idea what you are talking about and yet you have take to yourselves to be the arbitrators who say what goes and what doesn\\'t.  \\n\\n You are products of your consumeristic and instant gratification society. Online anarchists who believe in some higher purpose that the rest of us must absorb and abide by your decisions in order to make it happen. Linguistic cannibals. \\n\\n And, indeed, that is all you poor wretches are. Linguistic cannibals.\"',\n",
       "    'least_confident_prob': 0.3454079}}},\n",
       " 'identity_hate': {'auc': 0.990039558319306,\n",
       "  'f1': 0.6067159931701765,\n",
       "  'acc': 0.9891994122979775,\n",
       "  'tnr': 0.9808684235205852,\n",
       "  'fpr': 0.008002750945637562,\n",
       "  'fnr': 0.0027978367563850073,\n",
       "  'tpr': 0.008330988777392228,\n",
       "  'precision': 0.5100478468899522,\n",
       "  'recall': 0.7485955056179775,\n",
       "  'quad_stats': {'tp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 68.80675422138837,\n",
       "    'lens_std': 162.59991932043167,\n",
       "    'lens_min': 2,\n",
       "    'lens_max': 1215,\n",
       "    'most_confident': 'Please stop your disruptive editing. If you continue to vandalize Wikipedia, as you did at Warwick School, you will be blocked from editing.',\n",
       "    'most_confident_prob': 0.966466,\n",
       "    'least_confident': '\" \\n\\n ==Two short stories== \\n *The list of works contains something called \"\"The Mysterious Portrait\"\". Is this the same as the short story \"\"The Portrait\"\"?  \"',\n",
       "    'least_confident_prob': 0.23362644},\n",
       "   'fp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 72.0859375,\n",
       "    'lens_std': 171.46836111873844,\n",
       "    'lens_min': 3,\n",
       "    'lens_max': 1373,\n",
       "    'most_confident': '\" \\n\\n ::::This is not exactly helpful advice. If you are unable to articulate a coherent criticism, then your suggestion is arbitrary, and hints at WP:OWN. There is no time limit on this, as WP:WIP says. I would be inclined to say, once again, that other sections of the article need more attention than the lede; we can always come back to it. \\n ::::I would like to propose, , that you and I (at least, others are welcome of course) choose another section to work on together. I would propose the one on Justice, which has had a \"\"requires expansion\"\" tag on it since late 2010. I dont think this talk page is the most appropriate forum for the initial bigger edits, but perhaps your or my sandbox could be used to that end. After some editing and expansion, we will post sections of it here for additional comment. \\n ::::How about it?    \"',\n",
       "    'most_confident_prob': 0.8871105,\n",
       "    'least_confident': ':Agreed. Moreover, the recent additions place an undue emphasis on arguments in court briefs, much of which will become far less salient after the Supreme Court actually decides the case in a few months. –  ·',\n",
       "    'least_confident_prob': 0.23406751},\n",
       "   'tn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 81.2760302132135,\n",
       "    'lens_std': 124.14790483784333,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 4950,\n",
       "    'most_confident': '== New Jersey Wine == \\n I merged New Jersey Wine into Wineries, Breweries and Distilleries of New Jersey.  I my initial merge I added an extra comma to the page name.',\n",
       "    'most_confident_prob': 8.753936e-09,\n",
       "    'least_confident': 'yeahh BANNNNGG BANNNNNGG SKEEETT SKEEETTT MUTHHA FUCKKKAA!',\n",
       "    'least_confident_prob': 0.23253052},\n",
       "   'fn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 45.614525139664806,\n",
       "    'lens_std': 63.31328517399102,\n",
       "    'lens_min': 4,\n",
       "    'lens_max': 480,\n",
       "    'most_confident': \"Avg, plenty of Greeks love the King. Stop acting like all Greeks hate him. You just parrot what the government tells us via their 'media.' We Greeks asked George I to be our King. We supported them. They reclaimed much of our lands. We voted to keep them numerous times. In the 1930's by 97% vote! \\n Also, he tried to overthrow the dictators. Read a book. The Greek government is the biggest liars. The US didn't want us to have our King and our claim to Constantinople/Asia Minor. He is a direct descendant of five Greek imperial (Byzantine) dynasties (Monomachos, Comnenos, Laskaris, Angelos, and Paleologos).\",\n",
       "    'most_confident_prob': 0.0006062536,\n",
       "    'least_confident': '\" \\n\\n :Not sure whether this is notable enough to be mentioned in the article, but you\\'re right – versions later than 6 (2001) can open files created by all previous versions, but save files only in its own format, with a suffix of \"\".msX\"\", where X is the version number (7, 8, 9, 10, 11, 12...). This is intentional, of course. Besides, all versions but 7 save files in a compressed format. ☭共产主义万岁★ \"',\n",
       "    'least_confident_prob': 0.23248461}}},\n",
       " 'global_auc': 0.9850657226289589,\n",
       " 'global_f1': 0.6007176996802984,\n",
       " 'global_acc': 0.9677831963070639,\n",
       " 'global_tnr': 0.9360092531807808,\n",
       " 'global_fpr': 0.026222555670178293,\n",
       " 'global_fnr': 0.005994248022757823,\n",
       " 'global_tpr': 0.03177394312628299,\n",
       " 'global_precision': 0.5005925327222323,\n",
       " 'global_recall': 0.7688276688944137,\n",
       " 'train': ({'toxic': {'auc': 0.9903736473209769,\n",
       "    'f1': 0.8550757259075854,\n",
       "    'acc': 0.9717555194866235,\n",
       "    'tnr': 0.8884321085911601,\n",
       "    'fpr': 0.015723408388742314,\n",
       "    'fnr': 0.012521072124634176,\n",
       "    'tpr': 0.08332341089546347,\n",
       "    'precision': 0.8412527681113572,\n",
       "    'recall': 0.8693605335425657},\n",
       "   'severe_toxic': {'auc': 0.9925930259157654,\n",
       "    'f1': 0.5577812018489985,\n",
       "    'acc': 0.9892085654661561,\n",
       "    'tnr': 0.9824028175545682,\n",
       "    'fpr': 0.007601631875466093,\n",
       "    'fnr': 0.0031898026583777753,\n",
       "    'tpr': 0.006805747911587945,\n",
       "    'precision': 0.47237929534580253,\n",
       "    'recall': 0.6808777429467084},\n",
       "   'obscene': {'auc': 0.9959964667271411,\n",
       "    'f1': 0.8651983394833949,\n",
       "    'acc': 0.9853482149012038,\n",
       "    'tnr': 0.9383283930037413,\n",
       "    'fpr': 0.00872338958833372,\n",
       "    'fnr': 0.005928395510462427,\n",
       "    'tpr': 0.04701982189746257,\n",
       "    'precision': 0.8435075885328837,\n",
       "    'recall': 0.8880340868741863},\n",
       "   'threat': {'auc': 0.9954429038860906,\n",
       "    'f1': 0.6099009900990099,\n",
       "    'acc': 0.9975308796711182,\n",
       "    'tnr': 0.995600704388642,\n",
       "    'fpr': 0.0014037638418008284,\n",
       "    'fnr': 0.001065356487080986,\n",
       "    'tpr': 0.0019301752824761392,\n",
       "    'precision': 0.5789473684210527,\n",
       "    'recall': 0.6443514644351465},\n",
       "   'insult': {'auc': 0.9920026890292976,\n",
       "    'f1': 0.7948482060717572,\n",
       "    'acc': 0.9790375444159654,\n",
       "    'tnr': 0.9384286618495842,\n",
       "    'fpr': 0.012207731981375062,\n",
       "    'fnr': 0.008754723602659631,\n",
       "    'tpr': 0.04060888256638111,\n",
       "    'precision': 0.7688656858092074,\n",
       "    'recall': 0.8226482163260125},\n",
       "   'identity_hate': {'auc': 0.9942411196165225,\n",
       "    'f1': 0.6471150781000956,\n",
       "    'acc': 0.9930626492282433,\n",
       "    'tnr': 0.9867018443200832,\n",
       "    'fpr': 0.0044932976543356875,\n",
       "    'fnr': 0.0024440531174210853,\n",
       "    'tpr': 0.006360804908160004,\n",
       "    'precision': 0.586027713625866,\n",
       "    'recall': 0.7224199288256228}},)}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'toxic': {'auc': 0.9903736473209769,\n",
       "   'f1': 0.8550757259075854,\n",
       "   'acc': 0.9717555194866235,\n",
       "   'tnr': 0.8884321085911601,\n",
       "   'fpr': 0.015723408388742314,\n",
       "   'fnr': 0.012521072124634176,\n",
       "   'tpr': 0.08332341089546347,\n",
       "   'precision': 0.8412527681113572,\n",
       "   'recall': 0.8693605335425657},\n",
       "  'severe_toxic': {'auc': 0.9925930259157654,\n",
       "   'f1': 0.5577812018489985,\n",
       "   'acc': 0.9892085654661561,\n",
       "   'tnr': 0.9824028175545682,\n",
       "   'fpr': 0.007601631875466093,\n",
       "   'fnr': 0.0031898026583777753,\n",
       "   'tpr': 0.006805747911587945,\n",
       "   'precision': 0.47237929534580253,\n",
       "   'recall': 0.6808777429467084},\n",
       "  'obscene': {'auc': 0.9959964667271411,\n",
       "   'f1': 0.8651983394833949,\n",
       "   'acc': 0.9853482149012038,\n",
       "   'tnr': 0.9383283930037413,\n",
       "   'fpr': 0.00872338958833372,\n",
       "   'fnr': 0.005928395510462427,\n",
       "   'tpr': 0.04701982189746257,\n",
       "   'precision': 0.8435075885328837,\n",
       "   'recall': 0.8880340868741863},\n",
       "  'threat': {'auc': 0.9954429038860906,\n",
       "   'f1': 0.6099009900990099,\n",
       "   'acc': 0.9975308796711182,\n",
       "   'tnr': 0.995600704388642,\n",
       "   'fpr': 0.0014037638418008284,\n",
       "   'fnr': 0.001065356487080986,\n",
       "   'tpr': 0.0019301752824761392,\n",
       "   'precision': 0.5789473684210527,\n",
       "   'recall': 0.6443514644351465},\n",
       "  'insult': {'auc': 0.9920026890292976,\n",
       "   'f1': 0.7948482060717572,\n",
       "   'acc': 0.9790375444159654,\n",
       "   'tnr': 0.9384286618495842,\n",
       "   'fpr': 0.012207731981375062,\n",
       "   'fnr': 0.008754723602659631,\n",
       "   'tpr': 0.04060888256638111,\n",
       "   'precision': 0.7688656858092074,\n",
       "   'recall': 0.8226482163260125},\n",
       "  'identity_hate': {'auc': 0.9942411196165225,\n",
       "   'f1': 0.6471150781000956,\n",
       "   'acc': 0.9930626492282433,\n",
       "   'tnr': 0.9867018443200832,\n",
       "   'fpr': 0.0044932976543356875,\n",
       "   'fnr': 0.0024440531174210853,\n",
       "   'tpr': 0.006360804908160004,\n",
       "   'precision': 0.586027713625866,\n",
       "   'recall': 0.7224199288256228}},)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_metrics[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record results and save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ[\"IS_COLAB\"] != \"True\":\n",
    "    import sys\n",
    "    sys.path.append(\"../lib\")\n",
    "    from record_experiments import record\n",
    "else:\n",
    "    PASSWORD = \"mongo11747\" # FILL IN IF COLAB\n",
    "\n",
    "    from typing import *\n",
    "    import pymongo\n",
    "    from bson.objectid import ObjectId\n",
    "    import os\n",
    "    import logging\n",
    "\n",
    "    # Logger\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('[%(levelname)s] %(asctime)s - %(name)s %(message)s')\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    conn_str = f\"mongodb+srv://root:{PASSWORD}@cluster0-ptgoc.mongodb.net/test?retryWrites=true\"\n",
    "\n",
    "    client = pymongo.MongoClient(conn_str)\n",
    "    db = client.experiments\n",
    "    collection = db.logs\n",
    "\n",
    "    def _cln(v: Any) -> Any:\n",
    "        \"\"\"Ensure variables are serializable\"\"\"\n",
    "        if isinstance(v, (np.float, np.float16, np.float32, np.float64, np.float128)):\n",
    "            return float(v)\n",
    "        elif isinstance(v, (np.int, np.int0, np.int8, np.int16, np.int32, np.int64)):\n",
    "            return int(v)\n",
    "        elif isinstance(v, dict):\n",
    "            return {k: _cln(v_) for k, v_ in v.items()}\n",
    "        else:\n",
    "            return v\n",
    "\n",
    "    def record(log: dict):\n",
    "        res = collection.insert_one({str(k): _cln(v) for k, v in log.items()})\n",
    "        logger.info(f\"Inserted results at id {res.inserted_id}\")\n",
    "        return res\n",
    "\n",
    "    def find(id_: Optional[str]=None, query: Optional[dict]=None):\n",
    "        if query is None: query = {\"_id\": ObjectId(id_)}\n",
    "        res = collection.find_one(query)\n",
    "        return res\n",
    "\n",
    "    def delete(id_: Optional[str]=None, query: Optional[dict]=None):\n",
    "        if query is None: query = {\"_id\": ObjectId(id_)}\n",
    "        res = collection.delete_many(query)\n",
    "        logger.info(f\"Deleted {res.deleted_count} entries\")\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-04-19 10:07:46,900 - record_experiments Inserted results at id 5cb9d632145ce536ca6b251f\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "if not config.testing:\n",
    "    experiment_log = dict(config)\n",
    "    tz = timezone('EST')\n",
    "    experiment_log[\"execution_date\"] = datetime.now(tz).strftime(\"%Y-%m-%d %H:%M %Z\")\n",
    "    experiment_log.update(metrics)\n",
    "    experiment_log.update(label_metrics)\n",
    "    record(experiment_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
