{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import *\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, SequenceLabelField, LabelField\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs:\n",
    "\n",
    "- Make compatible with GPU\n",
    "- Try replicating SST results\n",
    "- Write results to MongoDB Atlas\n",
    "- Store weights in s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# for papermill\n",
    "testing = True\n",
    "seed = 1\n",
    "computational_batch_size = 4\n",
    "batch_size = 16\n",
    "lr = 5e-5\n",
    "epochs = 3\n",
    "embed_dim = 256\n",
    "hidden_sz = 768\n",
    "dataset = \"sst-2\"\n",
    "n_classes = 2\n",
    "max_seq_len = 512\n",
    "bert_model = \"bert-base-cased\"\n",
    "run_id = \"replicate_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Can we make this play better with papermill?\n",
    "config = Config(\n",
    "    testing=testing,\n",
    "    seed=seed,\n",
    "    computational_batch_size=computational_batch_size,\n",
    "    batch_size=batch_size, # This is probably too large: need to handle effective v.s. machine batch size\n",
    "    lr=lr,\n",
    "    epochs=epochs,\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_sz=hidden_sz,\n",
    "    dataset=dataset,\n",
    "    n_classes=n_classes,\n",
    "    max_seq_len=max_seq_len, # necessary to limit memory usage\n",
    "#     bert_model=None,\n",
    "    bert_model=bert_model,\n",
    "    run_id=run_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common.checks import ConfigurationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.computational_batch_size * config.max_seq_len > 32 * 128:\n",
    "    raise ConfigurationError(f\"Batch size {config.computational_batch_size} too large for seq len {config.max_seq_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.batch_size % config.computational_batch_size != 0:\n",
    "    raise ConfigurationError(f\"Computational batch size {config.computational_batch_size} \"\n",
    "                     + f\"must be mutiple of batch size {config.batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.batch_size < config.computational_batch_size:\n",
    "    raise ConfigurationError(f\"Computational batch size {config.computational_batch_size} \"\n",
    "                     + f\"must be smaller than batch size {config.batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "RUN_ID = config.run_id if config.run_id is not None else now.strftime(\"%m_%d_%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"../data\") / config.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed manually to replicate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x113f1fe50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.dataset_readers import DatasetReader, StanfordSentimentTreeBankDatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_registry = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register(name: str):\n",
    "    def dec(x: Callable):\n",
    "        reader_registry[name] = x\n",
    "        return x\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register(\"jigsaw\")\n",
    "class JigsawDatasetReader(DatasetReader):\n",
    "    def __init__(self, tokenizer: Callable[[str], List[str]]=lambda x: x.split(),\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None, # TODO: Handle mapping from BERT\n",
    "                 max_seq_len: Optional[int]=config.max_seq_len) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self, tokens: List[Token], label: str = None) -> Instance:\n",
    "        # TODO: Reimplement\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"tokens\": sentence_field}\n",
    "\n",
    "        label_field = LabelField(label=label, skip_indexing=True)\n",
    "        fields[\"label\"] = label_field\n",
    "\n",
    "        return Instance(fields)\n",
    "    \n",
    "    @overrides\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if config.testing: df = df.head(10000)\n",
    "        for i, row in df.iterrows():\n",
    "            yield self.text_to_instance(\n",
    "                [Token(x) for x in self.tokenizer(row[\"comment_text\"])],\n",
    "                row[\"toxic\"]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register(\"imdb\")\n",
    "class IMDBDatasetReader(DatasetReader):\n",
    "    def __init__(self, tokenizer=None, \n",
    "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                 max_seq_len=None) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    @overrides\n",
    "    def text_to_instance(self, tokens: List[Token], label: str = None) -> Instance:\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"tokens\": sentence_field}\n",
    "        \n",
    "        # TODO: Add statistical features?\n",
    "\n",
    "        label_field = LabelField(label=label)\n",
    "        fields[\"label\"] = label_field\n",
    "\n",
    "        return Instance(fields)\n",
    "\n",
    "    @overrides\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        # TODO: Implement\n",
    "        for label in [\"pos\", \"neg\"]:\n",
    "            for file in (Path(file_path) / label).glob(\"*.txt\"):\n",
    "                text = file.open(\"rt\", encoding=\"utf-8\").read()\n",
    "                yield self.text_to_instance([Token(word) for word in self.tokenizer(text)], \n",
    "                                            label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register(\"sst\")\n",
    "class SSTDatasetReader(StanfordSentimentTreeBankDatasetReader):\n",
    "    def __init__(self, *args, tokenizer=None, **kwargs):\n",
    "        super().__init__(*args, granularity=f\"{config.n_classes}-class\", **kwargs)\n",
    "        self._tokenizer = tokenizer\n",
    "        \n",
    "    @overrides\n",
    "    def text_to_instance(self, tokens: List[str], sentiment: str=None) -> Instance:\n",
    "        \"\"\"\n",
    "        Forcibly re-tokenize the input to be wordpiece tokenized\n",
    "        \"\"\"\n",
    "        tokens = self._tokenizer(\" \".join(tokens))\n",
    "        return super().text_to_instance(tokens, sentiment=sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register(\"sst-2\") # different from SST!!\n",
    "class SST2DatasetReader(DatasetReader):\n",
    "    def __init__(self, tokenizer: Callable[[str], List[str]]=lambda x: x.split(),\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None, # TODO: Handle mapping from BERT\n",
    "                 max_seq_len: Optional[int]=config.max_seq_len) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self, tokens: List[Token], label: str = None) -> Instance:\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"tokens\": sentence_field}\n",
    "\n",
    "        label_field = LabelField(label=label, skip_indexing=True)\n",
    "        fields[\"label\"] = label_field\n",
    "\n",
    "        return Instance(fields)\n",
    "    \n",
    "    @overrides\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "        for i, row in df.iterrows():\n",
    "            yield self.text_to_instance(\n",
    "                [Token(x) for x in self.tokenizer(row[\"sentence\"])],\n",
    "                row[\"label\"] if \"label\" in row else None\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare token handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.token_indexers import WordpieceIndexer, SingleIdTokenIndexer\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "class BertIndexerCustom(WordpieceIndexer):\n",
    "    \"\"\"\n",
    "    Virtually the same as PretrainedWordIndexer, except exposes more options.\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_model: str,\n",
    "                 use_starting_offsets: bool = False,\n",
    "                 do_lowercase: bool = True,\n",
    "                 never_lowercase: List[str] = None,\n",
    "                 max_pieces: int = 512,\n",
    "                 start_tokens=[\"[CLS]\"],\n",
    "                 end_tokens=[\"[SEP]\"]) -> None:\n",
    "        assert not (pretrained_model.endswith(\"-cased\") and do_lowercase)\n",
    "        assert not (pretrained_model.endswith(\"-uncased\") and not do_lowercase)\n",
    "        bert_tokenizer = BertTokenizer.from_pretrained(pretrained_model,\n",
    "                                                       do_lower_case=do_lowercase)\n",
    "        super().__init__(vocab=bert_tokenizer.vocab,\n",
    "                         wordpiece_tokenizer=bert_tokenizer.wordpiece_tokenizer.tokenize,\n",
    "                         namespace=\"bert\",\n",
    "                         use_starting_offsets=use_starting_offsets,\n",
    "                         max_pieces=max_pieces,\n",
    "                         do_lowercase=do_lowercase,\n",
    "                         never_lowercase=never_lowercase,\n",
    "                         start_tokens=start_tokens,\n",
    "                         end_tokens=end_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/19/2019 08:51:22 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /Users/keitakurita/.pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    }
   ],
   "source": [
    "if config.bert_model is not None:\n",
    "    token_indexer = BertIndexerCustom(\n",
    "        pretrained_model=config.bert_model,\n",
    "        max_pieces=config.max_seq_len,\n",
    "        do_lowercase=\"uncased\" in config.bert_model,\n",
    "     )\n",
    "    # apparently we need to truncate the sequence here, which is a stupid design decision\n",
    "    def tokenizer(s: str):\n",
    "        return token_indexer.wordpiece_tokenizer(s)[:config.max_seq_len - 2]\n",
    "else:\n",
    "    token_indexer = SingleIdTokenIndexer(\n",
    "        lowercase_tokens=False,  # don't lowercase by default\n",
    "    )\n",
    "    tokenizer = lambda x: x.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_cls = reader_registry[config.dataset]\n",
    "reader = reader_cls(tokenizer=tokenizer,\n",
    "                    token_indexers={\"tokens\": token_indexer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "6it [00:00, 59.96it/s]\u001b[A\n",
      "467it [00:00, 85.18it/s]\u001b[A\n",
      "67349it [00:13, 4985.29it/s]\n",
      "872it [00:00, 4555.28it/s]\n"
     ]
    }
   ],
   "source": [
    "if config.dataset == \"imdb\":\n",
    "    data_dir = DATA_ROOT / \"imdb\" / \"aclImdb\"\n",
    "    train_ds, test_ds = (reader.read(data_dir / fname) for fname in [\"train\", \"test\"])\n",
    "    val_ds = None\n",
    "elif config.dataset == \"sst\":\n",
    "    data_dir = DATA_ROOT / \"trees\"\n",
    "    train_ds, val_ds, test_ds = (reader.read(data_dir / fname) for fname in [\"train.txt\", \"dev.txt\", \"test.txt\"])\n",
    "elif config.dataset == \"jigsaw\":\n",
    "    train_ds, test_ds = (reader.read(DATA_ROOT / fname) for fname in [\"train.csv\", \"test_proced.csv\"])\n",
    "elif config.dataset == \"sst-2\":\n",
    "    train_ds, val_ds = (reader.read(DATA_ROOT / fname) for fname in [\"train.tsv\", \"dev.tsv\"])\n",
    "    test_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67349"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/19/2019 08:51:36 - INFO - allennlp.data.vocabulary -   Fitting token dictionary from dataset.\n",
      "100%|██████████| 67349/67349 [00:00<00:00, 252190.56it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(train_ds)\n",
    "if config.bert_model is not None: \n",
    "    token_indexer._add_encoding_to_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.data.iterators import BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Allow for customization\n",
    "iterator = BucketIterator(batch_size=config.batch_size, \n",
    "                          biggest_batch_first=True,\n",
    "                          sorting_keys=[(\"tokens\", \"num_tokens\")],\n",
    "                         )\n",
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    batch = next(iter(iterator(train_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': {'tokens': tensor([[  101,  1272,  1157, 22593,  6639,  2953, 12788,  1158,  3981,  1116,\n",
       "             117,  1489, 28137,  4980,  1813, 28137, 11015, 11580,  3740, 23639,\n",
       "            1605, 11266,  1320,   117,   127, 28137,  4980,  1813, 28137, 11015,\n",
       "            3583,  2927,  1616,  4982,  1105,  1275, 28137,  4980,  1813, 28137,\n",
       "           11015,  1119,  1179,  1616, 24438,  7903,  1116,   117,  7627,  1366,\n",
       "            1104,  1103,  3796,  1104,  1103, 10228,   117,   192,  3708,  3540,\n",
       "           11972,  1121,   170,  1677,  7138,  5015,   102,     0,     0,     0],\n",
       "          [  101,   112,  1116,  1662,  1106,  5403,  2256,  7204,  1106,  8991,\n",
       "             170,  2523,  1136,  1178,  1121, 22572, 26464, 10734,  4703,  2851,\n",
       "             179,  9899,   176,  7777,  7836,  2328,  1348,  1133,  1145,  1121,\n",
       "            8587,   184, 26996,  1197,  5222, 28117,  9995, 21718, 13141,  1320,\n",
       "             117,  6693,  1394, 16358,  3101,  1399,  1105, 16358,  6071, 11151,\n",
       "             117,  1870, 25551,  8468,  7836,   185,  4165,  3186,  1186,  7561,\n",
       "            1228,  1103,  8809,  1114,   170,  1643,  2858, 12913,   119,   102],\n",
       "          [  101,  1232,   188, 15633,  1181,  1114,  8594,   113,   112,   178,\n",
       "            2936, 21785,  3596,  5886,   117,   112, 19961,  1116, 10552, 13200,\n",
       "            1643,  1170,   170,  1897,   117, 14044,   117,   171, 10354,  4999,\n",
       "            3670,  1114,  1126,  8143,  5579,  9332,   114,  1105, 20787,  2340,\n",
       "            1146, 23562,  1116,   113,  2878,   112,  1116, 14247, 10595,  1144,\n",
       "            1151,  2125,  1114,   182,  1766,  7880,   117,   170, 10509,  8143,\n",
       "            6093,  1150, 27180,  1116,  2490,  1105,  1917,  1213,   114,   102],\n",
       "          [  101,  1156,  1129,  1126, 25731,  3276, 27984, 10761,  2541,  1111,\n",
       "            1142, 18737, 28137,  1643, 12316,  1394, 12562,  1207, 26063,  4661,\n",
       "            1200,  1191,  1744, 28137,  4980,  1813, 28137, 11015,  1231,  6420,\n",
       "            1114,  1468, 24877,  1127,  1136,  1113,  1289,  1106,  1107, 16811,\n",
       "            1123,  5805,  7369,  1959,   117,  1143, 21690,  1162,  1610,  7257,\n",
       "            2328,  1883,   117,  1114,   170,  4672,  1107, 17149,  1104,  1385,\n",
       "           28137,  8057,  5933, 23138, 16358,  6071,  2615,  3974,   119,   102],\n",
       "          [  101,  5540,  2881,   112,  1116,   184,  2007,  1106, 27629, 27102,\n",
       "            1297,   112,  1116,  4608,  1880,  1110,   170, 26084,  6647,  1105,\n",
       "            1107,  2528, 12807,  2227, 23487,  1186,  1164,  1103, 14673,  1757,\n",
       "            1104,  8366,  1348, 10116,  1232,  1107,  9010,  1104,  1567,  2606,\n",
       "             170, 11925, 10771,  1361,  1105,  3073,  5208,  3121,  2285,  1322,\n",
       "           26687,  1115,   112,  1116,  8362, 14467,  6697,  1174,  1105, 21359,\n",
       "           24348,  1193,  4252,  2225,  3365,  3798,   119,   102,     0,     0]]),\n",
       "  'tokens-offsets': tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "           19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "           37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "           55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65,  0,  0,  0],\n",
       "          [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "           19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "           37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "           55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68],\n",
       "          [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "           19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "           37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "           55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68],\n",
       "          [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "           19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "           37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "           55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68],\n",
       "          [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "           19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "           37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "           55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66,  0,  0]]),\n",
       "  'mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])},\n",
       " 'label': tensor([1, 1, 1, 1, 0])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1272,  1157, 22593,  6639,  2953, 12788,  1158,  3981,  1116,\n",
       "           117,  1489, 28137,  4980,  1813, 28137, 11015, 11580,  3740, 23639,\n",
       "          1605, 11266,  1320,   117,   127, 28137,  4980,  1813, 28137, 11015,\n",
       "          3583,  2927,  1616,  4982,  1105,  1275, 28137,  4980,  1813, 28137,\n",
       "         11015,  1119,  1179,  1616, 24438,  7903,  1116,   117,  7627,  1366,\n",
       "          1104,  1103,  3796,  1104,  1103, 10228,   117,   192,  3708,  3540,\n",
       "         11972,  1121,   170,  1677,  7138,  5015,   102,     0,     0,     0],\n",
       "        [  101,   112,  1116,  1662,  1106,  5403,  2256,  7204,  1106,  8991,\n",
       "           170,  2523,  1136,  1178,  1121, 22572, 26464, 10734,  4703,  2851,\n",
       "           179,  9899,   176,  7777,  7836,  2328,  1348,  1133,  1145,  1121,\n",
       "          8587,   184, 26996,  1197,  5222, 28117,  9995, 21718, 13141,  1320,\n",
       "           117,  6693,  1394, 16358,  3101,  1399,  1105, 16358,  6071, 11151,\n",
       "           117,  1870, 25551,  8468,  7836,   185,  4165,  3186,  1186,  7561,\n",
       "          1228,  1103,  8809,  1114,   170,  1643,  2858, 12913,   119,   102],\n",
       "        [  101,  1232,   188, 15633,  1181,  1114,  8594,   113,   112,   178,\n",
       "          2936, 21785,  3596,  5886,   117,   112, 19961,  1116, 10552, 13200,\n",
       "          1643,  1170,   170,  1897,   117, 14044,   117,   171, 10354,  4999,\n",
       "          3670,  1114,  1126,  8143,  5579,  9332,   114,  1105, 20787,  2340,\n",
       "          1146, 23562,  1116,   113,  2878,   112,  1116, 14247, 10595,  1144,\n",
       "          1151,  2125,  1114,   182,  1766,  7880,   117,   170, 10509,  8143,\n",
       "          6093,  1150, 27180,  1116,  2490,  1105,  1917,  1213,   114,   102],\n",
       "        [  101,  1156,  1129,  1126, 25731,  3276, 27984, 10761,  2541,  1111,\n",
       "          1142, 18737, 28137,  1643, 12316,  1394, 12562,  1207, 26063,  4661,\n",
       "          1200,  1191,  1744, 28137,  4980,  1813, 28137, 11015,  1231,  6420,\n",
       "          1114,  1468, 24877,  1127,  1136,  1113,  1289,  1106,  1107, 16811,\n",
       "          1123,  5805,  7369,  1959,   117,  1143, 21690,  1162,  1610,  7257,\n",
       "          2328,  1883,   117,  1114,   170,  4672,  1107, 17149,  1104,  1385,\n",
       "         28137,  8057,  5933, 23138, 16358,  6071,  2615,  3974,   119,   102],\n",
       "        [  101,  5540,  2881,   112,  1116,   184,  2007,  1106, 27629, 27102,\n",
       "          1297,   112,  1116,  4608,  1880,  1110,   170, 26084,  6647,  1105,\n",
       "          1107,  2528, 12807,  2227, 23487,  1186,  1164,  1103, 14673,  1757,\n",
       "          1104,  8366,  1348, 10116,  1232,  1107,  9010,  1104,  1567,  2606,\n",
       "           170, 11925, 10771,  1361,  1105,  3073,  5208,  3121,  2285,  1322,\n",
       "         26687,  1115,   112,  1116,  8362, 14467,  6697,  1174,  1105, 21359,\n",
       "         24348,  1193,  4252,  2225,  3365,  3798,   119,   102,     0,     0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 70])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.token_embedders.bert_token_embedder import BertEmbedder, PretrainedBertEmbedder\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.stacked_bidirectional_lstm import StackedBidirectionalLstm\n",
    "from allennlp.nn.util import get_text_field_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmEncoder(nn.Module):\n",
    "    def __init__(self, lstm):\n",
    "        super().__init__()\n",
    "        self.lstm = lstm\n",
    "        \n",
    "    def forward(self, x, mask): # TODO: replace with allennlp built in modules\n",
    "        _, (state, _) = self.lstm(x)\n",
    "        state = torch.cat([state[0, :, :], state[1, :, :]], dim=1)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    \"\"\"Source code copied\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(768, 768)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states, mask):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModel(Model):\n",
    "    def __init__(self, word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: StackedBidirectionalLstm,\n",
    "                 out_sz=config.n_classes):\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "#         self.projection = nn.Linear(encoder.get_output_dim(), out_sz)\n",
    "        self.projection = nn.Linear(config.hidden_sz, out_sz)\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        \n",
    "    def forward(self,\n",
    "                tokens: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor = None) -> torch.Tensor:\n",
    "        mask = get_text_field_mask(tokens)\n",
    "        embeddings = self.word_embeddings(tokens[\"tokens\"])\n",
    "        state = self.encoder(embeddings, mask)\n",
    "\n",
    "        class_logits = self.projection(state)\n",
    "        \n",
    "        output = {\"class_logits\": class_logits}\n",
    "        if label is not None:\n",
    "            output[\"accuracy\"] = self.accuracy(class_logits, label, None)\n",
    "            output[\"loss\"] = nn.CrossEntropyLoss()(class_logits, label)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/19/2019 08:51:43 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz from cache at /Users/keitakurita/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c\n",
      "01/19/2019 08:51:43 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /Users/keitakurita/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c to temp dir /var/folders/hy/1czs1y5j2d58zgkqx6w_wnpw0000gn/T/tmpp8fwfrl1\n",
      "01/19/2019 08:51:47 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if config.bert_model is None:\n",
    "    token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                                embedding_dim=config.embed_dim)\n",
    "    word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "    # encoder = PytorchSeq2VecWrapper(nn.LSTM(config.embed_dim, config.hidden_sz, batch_first=True,\n",
    "    #                                         bidirectional=True))\n",
    "    encoder = LstmEncoder(\n",
    "        nn.LSTM(config.embed_dim, config.hidden_sz, batch_first=True, bidirectional=True)\n",
    "    )\n",
    "\n",
    "else:\n",
    "    word_embeddings = PretrainedBertEmbedder(\n",
    "        pretrained_model=config.bert_model,\n",
    "        top_layer_only=True, # conserve memory\n",
    "    )\n",
    "    encoder = BertPooler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentAnalysisModel(\n",
    "    word_embeddings, \n",
    "    encoder, \n",
    "    out_sz=config.n_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU: model.cuda()\n",
    "else: model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(list(model.word_embeddings.parameters())[0].detach().numpy()).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.isnan(x.detach().numpy()).any() for x in list(model.encoder.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.isinf(x.detach().numpy()).any() for x in list(model.encoder.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2596,  0.4273, -0.2406,  ..., -0.4589, -0.2846, -0.0843],\n",
       "        [-0.3921,  0.5188, -0.4173,  ..., -0.7065, -0.3623, -0.0750],\n",
       "        [ 0.0432,  0.2216, -0.0860,  ..., -0.5040, -0.2837,  0.0297],\n",
       "        [-0.2084,  0.3396, -0.2487,  ..., -0.6313, -0.2363, -0.0369],\n",
       "        [-0.0938,  0.4056, -0.3456,  ..., -0.6304, -0.2349,  0.2175]],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = batch[\"tokens\"]\n",
    "encoder(model.word_embeddings(tokens[\"tokens\"]), get_text_field_mask(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(**batch)[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6632, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.CrossEntropyLoss()(model(**batch)[\"class_logits\"][:10, :], batch[\"label\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6479, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-4.8972e-03, -8.8047e-04,  2.4230e-04,  ...,  3.4703e-03,\n",
       "          -1.1962e-03, -4.8932e-04],\n",
       "         [-7.3226e-04, -1.2156e-04,  3.5136e-05,  ...,  5.1501e-04,\n",
       "          -1.7994e-04, -7.3413e-05],\n",
       "         [ 8.6812e-03,  1.7256e-03, -3.5987e-04,  ..., -6.2586e-03,\n",
       "           2.1314e-03,  1.0785e-03],\n",
       "         ...,\n",
       "         [ 3.4397e-03,  6.3277e-04, -1.5470e-04,  ..., -2.4107e-03,\n",
       "           8.4264e-04,  2.1589e-04],\n",
       "         [ 9.8634e-04,  1.9345e-04, -5.7689e-05,  ..., -6.8664e-04,\n",
       "           2.5014e-04,  8.4279e-05],\n",
       "         [ 8.0626e-03,  1.5895e-03, -3.9801e-04,  ..., -5.7370e-03,\n",
       "           1.9845e-03,  8.0873e-04]]),\n",
       " tensor([-7.6779e-03, -1.1523e-03,  1.3969e-02, -2.6948e-03,  1.2352e-02,\n",
       "          5.0405e-03,  1.0513e-02, -6.8732e-03, -1.1279e-02, -7.0509e-03,\n",
       "          2.4282e-03, -1.3056e-03, -5.9292e-03, -6.9618e-03,  9.3582e-03,\n",
       "         -6.5148e-03, -8.8608e-04, -5.4255e-03,  4.4703e-04, -1.0966e-03,\n",
       "          1.5481e-02, -7.8649e-04, -8.0571e-03, -8.9866e-04,  1.0882e-04,\n",
       "          2.3196e-03, -1.1380e-02,  2.2790e-03, -6.5691e-03, -7.7481e-03,\n",
       "         -1.8363e-03,  3.5299e-04, -6.6431e-03, -5.1103e-03, -1.0091e-03,\n",
       "          8.8208e-04,  6.0441e-03,  4.6113e-03,  6.1661e-03, -2.4519e-04,\n",
       "          3.5303e-03, -1.7857e-03, -2.2096e-03, -1.8904e-04, -3.1679e-03,\n",
       "          8.0785e-04, -1.0329e-02,  2.2891e-03,  9.3568e-04,  2.8673e-03,\n",
       "         -6.5338e-03, -8.9569e-04, -1.5565e-02, -6.3908e-04,  5.5718e-03,\n",
       "          5.9752e-03, -5.0400e-03, -1.1871e-03,  1.1263e-02,  2.8218e-03,\n",
       "          6.2794e-03,  1.3630e-03,  3.2338e-04,  1.0222e-02,  3.1971e-03,\n",
       "          4.3790e-03,  1.8406e-03, -7.0752e-04,  1.1112e-03, -2.9827e-03,\n",
       "         -8.9034e-03,  9.4556e-03, -1.0510e-03, -6.8198e-03, -1.0911e-02,\n",
       "         -3.3302e-03,  7.2870e-03, -7.6870e-03,  5.4827e-03, -2.0707e-03,\n",
       "          1.7361e-02, -5.9470e-03, -1.1580e-02,  1.2469e-02, -8.6078e-03,\n",
       "          1.2856e-02, -8.4704e-04,  6.0758e-03,  8.7546e-03,  7.6509e-03,\n",
       "          1.1621e-03, -1.1671e-03,  2.1647e-03, -9.0549e-03, -1.3728e-03,\n",
       "         -3.4394e-03, -3.2006e-03, -4.0735e-04, -3.5780e-03, -4.9187e-03,\n",
       "         -8.0336e-03, -1.3043e-03, -7.5720e-03, -1.7894e-03, -1.9257e-03,\n",
       "         -7.8431e-04,  7.8765e-04,  1.0731e-03,  4.7707e-03, -5.1459e-03,\n",
       "         -7.2509e-03,  2.0052e-03, -5.6853e-03,  1.6483e-02,  3.2292e-03,\n",
       "          2.7712e-03,  1.6366e-03,  2.4079e-03,  3.5054e-05,  5.4174e-03,\n",
       "         -1.3492e-03, -9.2931e-04,  9.5672e-03,  2.5162e-03,  2.1845e-04,\n",
       "          1.3939e-02, -3.8757e-04,  1.0706e-02, -1.1892e-02, -1.1629e-03,\n",
       "         -4.8503e-03, -4.1384e-03,  3.5118e-03, -7.8597e-03, -8.3223e-03,\n",
       "          6.0943e-04, -2.0393e-03,  8.7762e-03, -9.5632e-04, -1.4623e-02,\n",
       "          1.7285e-03,  4.1304e-03, -1.2813e-03,  6.1168e-03,  1.2822e-02,\n",
       "          9.0173e-04,  4.6215e-03,  9.4470e-04,  8.1709e-03,  6.9098e-03,\n",
       "          1.5544e-02,  9.1191e-03,  2.7013e-03,  1.5074e-03, -1.0373e-02,\n",
       "          4.6135e-03,  3.5408e-03, -3.1630e-03, -2.5977e-04,  3.2279e-03,\n",
       "         -6.2803e-03, -8.1921e-03,  7.8395e-03, -4.4390e-03, -3.0090e-03,\n",
       "         -3.9615e-03,  6.0700e-03, -3.4273e-03, -3.4919e-03,  1.8223e-03,\n",
       "          1.1488e-02, -8.8086e-03,  1.0214e-03, -6.9081e-03, -1.9870e-03,\n",
       "         -1.4308e-02, -1.2373e-03, -1.1203e-02, -7.4463e-03,  6.6422e-03,\n",
       "         -3.8044e-03, -3.4840e-03, -4.2962e-03, -1.2668e-02, -4.8980e-03,\n",
       "          1.1465e-04, -4.9324e-03, -1.1220e-02,  1.3177e-02,  2.8947e-03,\n",
       "          2.3263e-03,  6.3389e-03,  3.2385e-03, -1.0744e-02,  6.2705e-03,\n",
       "          2.2517e-03,  2.3450e-03,  4.7442e-03,  3.4624e-03,  6.5865e-03,\n",
       "         -8.8582e-03,  9.5522e-03, -1.7184e-03, -1.1900e-02, -6.1942e-03,\n",
       "          1.0962e-02, -7.1781e-04,  1.1095e-02,  8.0188e-03, -4.6123e-03,\n",
       "          8.7468e-03, -1.3878e-02,  4.6994e-04, -2.7262e-03, -6.7428e-03,\n",
       "          1.0987e-02, -7.5811e-03, -4.2066e-03,  4.6374e-03,  1.0235e-02,\n",
       "          5.8948e-03, -1.1775e-03,  1.0278e-03,  9.1534e-03, -5.8685e-03,\n",
       "         -1.1155e-02, -2.0016e-03,  7.9040e-03, -1.1713e-02, -6.7940e-03,\n",
       "          7.7234e-03, -1.2100e-02, -1.0455e-03,  1.1916e-02,  5.8707e-04,\n",
       "         -1.1383e-02,  3.5318e-03,  8.1139e-03,  4.9658e-04, -6.0021e-03,\n",
       "         -6.8825e-04, -1.0497e-03,  1.0742e-04,  1.3624e-02, -6.3867e-03,\n",
       "          5.7114e-03, -1.6559e-02,  2.2176e-03, -6.2598e-03,  4.3459e-04,\n",
       "          1.3478e-02, -8.9610e-03, -1.2376e-02,  1.0995e-02, -1.5767e-04,\n",
       "         -4.5312e-03, -6.8042e-03, -4.6515e-03, -1.7148e-03, -9.3459e-03,\n",
       "         -9.4624e-03, -4.9825e-03, -3.3445e-03,  1.4934e-03,  6.5406e-03,\n",
       "          3.3564e-03,  5.8088e-03,  7.1028e-03,  2.9430e-03, -1.4253e-02,\n",
       "         -2.3099e-03, -4.8532e-03, -7.8470e-03, -5.2982e-03, -9.1346e-03,\n",
       "          1.2662e-03,  3.4648e-03,  6.8496e-03, -2.1906e-03, -7.2378e-03,\n",
       "         -1.4495e-03, -1.6347e-03, -3.7850e-03, -8.4111e-03,  2.7678e-03,\n",
       "          7.4466e-03, -8.7829e-03,  9.1976e-03, -5.1380e-03,  4.3861e-03,\n",
       "         -5.4201e-04,  2.0358e-03,  3.3879e-03,  4.7710e-03,  4.0313e-03,\n",
       "          3.2352e-04,  1.1266e-03, -1.0217e-02, -5.5602e-03,  9.1039e-04,\n",
       "          2.2585e-03, -2.5327e-03, -8.0668e-03, -1.6842e-02, -1.3808e-02,\n",
       "          7.1863e-03,  4.4926e-03, -9.9119e-03, -9.3115e-03,  1.5726e-02,\n",
       "         -1.2485e-04, -5.7293e-04,  9.2538e-03,  1.0152e-02,  6.2959e-03,\n",
       "          9.5527e-03, -6.0146e-03,  3.1713e-03, -1.1411e-02, -1.1352e-02,\n",
       "         -5.0472e-03, -1.7103e-03,  7.8328e-03, -7.0622e-03,  3.2750e-03,\n",
       "          1.0229e-02, -3.1986e-03,  8.4689e-03,  6.3421e-03,  5.4680e-03,\n",
       "         -1.7979e-03, -1.2807e-02, -2.3680e-03,  1.3172e-02,  1.2702e-02,\n",
       "         -1.2114e-02, -9.9713e-03,  2.9341e-03, -1.0230e-02,  5.6833e-03,\n",
       "         -1.3488e-03,  1.9559e-03, -1.1593e-03,  6.2627e-03,  5.3282e-03,\n",
       "          1.1333e-02,  2.4043e-03,  1.7683e-02,  1.4431e-03,  1.2898e-02,\n",
       "         -2.5935e-03,  1.5673e-02, -1.2839e-03,  1.3942e-03,  7.6469e-03,\n",
       "         -1.3366e-02, -1.1102e-02, -3.6001e-03,  3.3409e-03,  8.8018e-03,\n",
       "          7.9026e-03, -1.0938e-03,  9.2671e-03,  8.7648e-03,  8.3146e-03,\n",
       "          6.2088e-03,  3.1127e-04,  1.7300e-03,  1.0924e-03,  4.6034e-03,\n",
       "         -3.4447e-03, -2.6277e-04, -1.4054e-02,  5.2636e-03, -4.6328e-03,\n",
       "          5.0703e-03,  1.0586e-02, -2.8216e-03, -5.7225e-03, -9.5669e-04,\n",
       "          5.7402e-03, -6.1662e-03, -2.9221e-03,  1.4261e-02, -3.0560e-03,\n",
       "          1.4234e-03, -1.0629e-02, -1.0979e-02,  9.8295e-03,  1.1294e-03,\n",
       "          6.2815e-03,  3.4281e-03, -4.1251e-03, -1.4584e-02, -1.0935e-02,\n",
       "          1.3168e-02,  8.5560e-03, -1.4162e-02, -1.2625e-03,  5.1192e-03,\n",
       "          1.6049e-03,  2.3950e-03, -6.6559e-03, -3.0956e-03, -5.5143e-03,\n",
       "         -1.1070e-02, -1.4870e-03,  8.4004e-03, -8.0903e-03, -5.1407e-03,\n",
       "         -1.5641e-02,  6.4482e-03, -1.3845e-02,  5.7543e-03,  7.3223e-03,\n",
       "         -1.7370e-02, -7.1431e-03,  5.3965e-03,  2.3976e-03,  1.0439e-02,\n",
       "         -4.4036e-03,  5.2338e-03, -6.5736e-03, -8.6803e-03, -5.5488e-04,\n",
       "         -5.9111e-03, -9.5417e-04, -8.4844e-03, -1.2163e-02, -5.4569e-03,\n",
       "         -1.1542e-02, -9.9394e-03,  7.4912e-05,  4.6034e-03, -1.7849e-03,\n",
       "          1.4663e-03, -5.7054e-03, -6.3927e-03, -3.9837e-04, -1.0850e-02,\n",
       "         -2.1707e-03,  2.2633e-03, -4.5950e-03,  9.2101e-03, -1.2766e-02,\n",
       "         -4.9794e-03, -2.8973e-03,  7.4284e-05, -8.4037e-03, -3.2561e-04,\n",
       "          8.0778e-03, -1.3149e-03, -9.1981e-03, -3.4748e-03,  2.3190e-03,\n",
       "         -2.3077e-03, -3.9280e-03,  1.0891e-02,  4.8027e-03,  1.3871e-03,\n",
       "          3.3717e-03,  1.8278e-04, -1.1492e-02,  3.6797e-03,  2.0656e-03,\n",
       "         -9.8721e-03,  1.7075e-03,  8.4154e-04, -1.2697e-03, -4.0178e-03,\n",
       "          9.9173e-03, -7.7897e-03, -1.0582e-02, -4.2372e-03, -1.3213e-02,\n",
       "          5.0092e-03, -1.1616e-02,  8.7602e-03,  8.1619e-03,  1.3585e-02,\n",
       "         -7.5031e-03, -5.7597e-03, -1.0387e-02,  4.1617e-03,  1.3899e-02,\n",
       "          2.6044e-03, -2.4281e-03,  3.9766e-03, -1.5722e-04,  7.6690e-04,\n",
       "         -1.3643e-02, -7.9764e-03,  2.3005e-04,  2.5802e-04,  5.9264e-04,\n",
       "          3.0110e-03,  3.9155e-03, -1.5814e-02, -7.3516e-03, -9.8310e-03,\n",
       "         -4.7903e-03,  6.1835e-03, -4.3052e-03, -1.7467e-02,  8.7675e-03,\n",
       "          2.4487e-03,  1.1955e-02,  4.8049e-03,  6.4089e-03,  1.4847e-02,\n",
       "          1.3239e-03, -8.2486e-04,  7.3455e-03, -1.3205e-02,  1.5597e-02,\n",
       "         -9.3011e-03,  8.2682e-03,  1.3595e-02,  2.2502e-03,  8.6232e-03,\n",
       "          1.1461e-04,  6.4344e-03,  7.9561e-04,  4.7458e-03, -8.2647e-03,\n",
       "          6.1044e-03, -6.8861e-03, -7.0190e-03,  7.3256e-03,  1.8972e-03,\n",
       "          1.1732e-02, -7.5302e-03,  4.1031e-04, -4.1613e-03,  6.6375e-03,\n",
       "         -6.3810e-04, -3.3293e-03,  1.7876e-03,  4.4264e-03,  2.0078e-03,\n",
       "          6.0064e-04,  1.9441e-03,  1.4079e-02, -8.3004e-03, -9.4164e-04,\n",
       "          2.6129e-03,  4.2071e-03, -2.9208e-03,  2.7028e-03,  4.7644e-03,\n",
       "         -1.7545e-03,  2.3384e-03,  3.4423e-03, -6.4048e-03, -7.6226e-03,\n",
       "          8.9045e-03, -9.3697e-03, -1.5308e-03,  5.5812e-03, -7.3377e-03,\n",
       "         -2.3090e-03, -5.2023e-03,  1.0539e-03, -1.3943e-03,  3.6877e-03,\n",
       "         -1.8878e-03,  2.4377e-03, -2.9577e-03,  1.3879e-02, -4.9496e-03,\n",
       "          1.5537e-02,  6.3929e-03,  1.2597e-03, -8.3719e-04, -1.5527e-03,\n",
       "          6.2972e-04, -4.2628e-03, -3.3851e-03, -3.7377e-03,  1.3069e-02,\n",
       "          5.4957e-04,  1.3011e-02, -6.5788e-03,  6.5116e-04,  3.7455e-03,\n",
       "         -3.8046e-03, -2.4540e-03,  1.0479e-02, -6.3404e-03, -5.6050e-03,\n",
       "          7.6872e-03, -4.5414e-03,  1.1577e-03,  4.5758e-04,  7.4899e-03,\n",
       "          9.9980e-03,  9.1674e-04, -5.1247e-03, -6.3870e-03, -5.9330e-04,\n",
       "          4.4773e-03, -1.9077e-03, -9.0153e-04, -3.1969e-03,  7.0632e-03,\n",
       "         -7.1864e-03,  3.6566e-03, -1.3264e-02, -1.0183e-03,  3.6254e-03,\n",
       "         -8.2039e-03, -5.8406e-03,  7.8202e-03, -1.3761e-02, -1.0222e-02,\n",
       "          9.8408e-03, -1.3215e-02,  1.5125e-03,  7.5279e-03, -5.6841e-04,\n",
       "          9.0498e-03,  6.4131e-03, -2.9960e-03, -7.9170e-03,  5.1155e-03,\n",
       "         -2.8672e-03, -1.6485e-02,  1.3684e-03,  3.2232e-03,  1.1005e-02,\n",
       "         -2.5076e-03, -8.4268e-03, -2.3509e-03, -2.1993e-03,  9.9841e-03,\n",
       "          7.1127e-03,  5.4824e-03,  2.5001e-03, -2.6053e-03,  4.6898e-03,\n",
       "         -9.4221e-03, -2.1828e-03, -5.2197e-03, -3.0109e-03, -7.6159e-03,\n",
       "          3.2851e-03, -8.9588e-04, -6.7359e-03, -4.6850e-03,  1.0111e-02,\n",
       "          1.0401e-02, -3.1286e-03, -5.1936e-03, -6.4427e-03,  9.1643e-03,\n",
       "         -6.9942e-04,  1.1162e-02,  7.9851e-04,  1.1981e-02,  1.6884e-03,\n",
       "          1.4832e-03, -1.3269e-02, -2.9395e-03,  5.3190e-03,  1.0432e-04,\n",
       "          6.1907e-03, -1.4744e-02,  6.6215e-03, -8.7954e-05, -7.1418e-03,\n",
       "         -7.3412e-03,  6.4563e-03, -3.4352e-03, -3.8883e-03, -1.4270e-03,\n",
       "         -2.4938e-03, -1.3287e-02, -9.9559e-05, -1.9084e-03, -3.4132e-03,\n",
       "          5.7588e-03, -3.6287e-03,  1.8402e-03,  4.2982e-03,  4.3262e-03,\n",
       "         -9.7593e-03,  3.0806e-03,  3.5251e-03,  6.3216e-03,  1.2123e-02,\n",
       "          9.4473e-03, -7.1478e-03,  2.4457e-03,  9.4594e-04,  5.1639e-03,\n",
       "          6.2455e-03,  4.4818e-04, -1.0301e-02, -1.2867e-02, -6.7927e-03,\n",
       "         -7.9435e-03, -3.8038e-04, -1.9296e-03, -3.4742e-03,  2.0037e-03,\n",
       "         -5.8003e-03,  8.9982e-04, -1.4271e-02, -8.0265e-03, -3.3550e-03,\n",
       "         -4.1238e-03, -9.9489e-03, -8.9530e-04, -6.2817e-03, -5.6956e-03,\n",
       "          2.8925e-03,  3.9991e-03, -5.8869e-03, -5.8837e-03, -1.1186e-02,\n",
       "          9.4634e-04,  2.4098e-03,  9.2219e-03,  5.8059e-03,  2.9166e-03,\n",
       "          1.2203e-02,  3.8559e-03, -1.0659e-02,  4.2435e-03,  8.5719e-03,\n",
       "         -1.3846e-03,  7.2558e-03, -1.4578e-02, -1.4930e-02, -6.3715e-03,\n",
       "         -9.7169e-03, -4.2257e-03, -3.8614e-04, -1.0458e-03,  1.0791e-03,\n",
       "         -1.0249e-02,  1.4133e-02, -5.8815e-04, -9.2481e-05,  5.4528e-04,\n",
       "         -3.4662e-03, -1.4819e-02,  4.3623e-03, -7.1918e-03, -1.1691e-02,\n",
       "         -1.0228e-02, -6.0515e-03, -5.7545e-03, -5.4257e-03,  2.9529e-03,\n",
       "         -8.7946e-03, -2.9658e-03,  1.8088e-03,  3.6220e-03, -1.8342e-03,\n",
       "         -6.9305e-03, -1.0663e-02,  7.0683e-03,  1.4240e-03,  1.0485e-02,\n",
       "          5.4784e-03,  1.5917e-03,  1.2828e-02])]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.grad for x in list(model.encoder.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training import trainer as _trainer\n",
    "from allennlp.training.trainer import *\n",
    "import math\n",
    "logger = _trainer.logger\n",
    "\n",
    "N_BATCHES_PER_UPDATE = config.batch_size // config.computational_batch_size\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Trains one epoch and returns metrics. Copied from source\n",
    "        \"\"\"\n",
    "        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n",
    "        peak_cpu_usage = peak_memory_mb()\n",
    "        logger.info(f\"Peak CPU memory usage MB: {peak_cpu_usage}\")\n",
    "        gpu_usage = []\n",
    "        for gpu, memory in gpu_memory_mb().items():\n",
    "            gpu_usage.append((gpu, memory))\n",
    "            logger.info(f\"GPU {gpu} memory usage MB: {memory}\")\n",
    "\n",
    "        train_loss = 0.0\n",
    "        # Set the model to \"train\" mode.\n",
    "        self.model.train()\n",
    "\n",
    "        # Get tqdm for the training batches\n",
    "        train_generator = self.iterator(self.train_data,\n",
    "                                        num_epochs=1,\n",
    "                                        shuffle=self.shuffle)\n",
    "        num_training_batches = self.iterator.get_num_batches(self.train_data)\n",
    "        self._last_log = time.time()\n",
    "        last_save_time = time.time()\n",
    "\n",
    "        batches_this_epoch = 0\n",
    "        if self._batch_num_total is None:\n",
    "            self._batch_num_total = 0\n",
    "\n",
    "        if self._histogram_interval is not None:\n",
    "            histogram_parameters = set(self.model.get_parameters_for_histogram_tensorboard_logging())\n",
    "\n",
    "        logger.info(\"Training\")\n",
    "        train_generator_tqdm = Tqdm.tqdm(train_generator,\n",
    "                                         total=num_training_batches)\n",
    "        cumulative_batch_size = 0\n",
    "        for batch in train_generator_tqdm:\n",
    "            batches_this_epoch += 1\n",
    "            self._batch_num_total += 1\n",
    "            batch_num_total = self._batch_num_total\n",
    "\n",
    "            self._log_histograms_this_batch = self._histogram_interval is not None and (\n",
    "                    batch_num_total % self._histogram_interval == 0)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            ###########\n",
    "            # Custom  #\n",
    "            ###########\n",
    "            loss = self.batch_loss(batch, for_training=True)\n",
    "            if torch.isnan(loss):\n",
    "                raise ValueError(\"nan loss encountered\")\n",
    "            train_loss += loss.item()\n",
    "            # wait to update\n",
    "            if (batches_this_epoch % N_BATCHES_PER_UPDATE) != 0: continue\n",
    "            ###############\n",
    "            # End Custom  #\n",
    "            ###############\n",
    "            \n",
    "            loss.backward()\n",
    "            batch_grad_norm = self.rescale_gradients()\n",
    "\n",
    "            # This does nothing if batch_num_total is None or you are using an\n",
    "            # LRScheduler which doesn't update per batch.\n",
    "            if self._learning_rate_scheduler:\n",
    "                self._learning_rate_scheduler.step_batch(batch_num_total)\n",
    "\n",
    "            if self._log_histograms_this_batch:\n",
    "                # get the magnitude of parameter updates for logging\n",
    "                # We need a copy of current parameters to compute magnitude of updates,\n",
    "                # and copy them to CPU so large models won't go OOM on the GPU.\n",
    "                param_updates = {name: param.detach().cpu().clone()\n",
    "                                 for name, param in self.model.named_parameters()}\n",
    "                self.optimizer.step()\n",
    "                for name, param in self.model.named_parameters():\n",
    "                    param_updates[name].sub_(param.detach().cpu())\n",
    "                    update_norm = torch.norm(param_updates[name].view(-1, ))\n",
    "                    param_norm = torch.norm(param.view(-1, )).cpu()\n",
    "                    self._tensorboard.add_train_scalar(\"gradient_update/\" + name,\n",
    "                                                       update_norm / (param_norm + 1e-7),\n",
    "                                                       batch_num_total)\n",
    "            else:\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Update the description with the latest metrics\n",
    "            metrics = self._get_metrics(train_loss, batches_this_epoch)\n",
    "            description = self._description_from_metrics(metrics)\n",
    "\n",
    "            train_generator_tqdm.set_description(description, refresh=False)\n",
    "\n",
    "            # Log parameter values to Tensorboard\n",
    "            if batch_num_total % self._summary_interval == 0:\n",
    "                if self._should_log_parameter_statistics:\n",
    "                    self._parameter_and_gradient_statistics_to_tensorboard(batch_num_total, batch_grad_norm)\n",
    "                if self._should_log_learning_rate:\n",
    "                    self._learning_rates_to_tensorboard(batch_num_total)\n",
    "                self._tensorboard.add_train_scalar(\"loss/loss_train\", metrics[\"loss\"], batch_num_total)\n",
    "                self._metrics_to_tensorboard(batch_num_total,\n",
    "                                             {\"epoch_metrics/\" + k: v for k, v in metrics.items()})\n",
    "\n",
    "            if self._log_histograms_this_batch:\n",
    "                self._histograms_to_tensorboard(batch_num_total, histogram_parameters)\n",
    "\n",
    "            if self._log_batch_size_period:\n",
    "                cur_batch = self._get_batch_size(batch)\n",
    "                cumulative_batch_size += cur_batch\n",
    "                if (batches_this_epoch - 1) % self._log_batch_size_period == 0:\n",
    "                    average = cumulative_batch_size/batches_this_epoch\n",
    "                    logger.info(f\"current batch size: {cur_batch} mean batch size: {average}\")\n",
    "                    self._tensorboard.add_train_scalar(\"current_batch_size\", cur_batch, batch_num_total)\n",
    "                    self._tensorboard.add_train_scalar(\"mean_batch_size\", average, batch_num_total)\n",
    "\n",
    "            # Save model if needed.\n",
    "            if self._model_save_interval is not None and (\n",
    "                    time.time() - last_save_time > self._model_save_interval\n",
    "            ):\n",
    "                last_save_time = time.time()\n",
    "                self._save_checkpoint(\n",
    "                        '{0}.{1}'.format(epoch, time_to_str(int(last_save_time))), [], is_best=False\n",
    "                )\n",
    "        metrics = self._get_metrics(train_loss, batches_this_epoch, reset=True)\n",
    "        metrics['cpu_memory_MB'] = peak_cpu_usage\n",
    "        for (gpu_num, memory) in gpu_usage:\n",
    "            metrics['gpu_'+str(gpu_num)+'_memory_MB'] = memory\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_options = {\n",
    "    # TODO: Add appropriate learning rate scheduler\n",
    "    \"should_log_parameter_statistics\": True,\n",
    "    \"should_log_learning_rate\": True,\n",
    "    \"num_epochs\": config.epochs,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/19/2019 08:52:28 - WARNING - allennlp.training.trainer -   You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n"
     ]
    }
   ],
   "source": [
    "SER_DIR = DATA_ROOT / \"ckpts\" / RUN_ID\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    iterator=iterator,\n",
    "    train_dataset=train_ds,\n",
    "    validation_dataset=val_ds,\n",
    "    serialization_dir=SER_DIR,\n",
    "    cuda_device=0 if USE_GPU else -1,\n",
    "    **training_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/19/2019 08:52:31 - INFO - allennlp.training.trainer -   Beginning training.\n",
      "01/19/2019 08:52:31 - INFO - allennlp.training.trainer -   Epoch 0/2\n",
      "01/19/2019 08:52:31 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 1479.880704\n",
      "01/19/2019 08:52:31 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.7746, loss: 0.4878 ||: 100%|██████████| 4210/4210 [43:09<00:00,  1.43it/s]  \n",
      "01/19/2019 09:35:40 - INFO - allennlp.training.trainer -   Validating\n",
      "accuracy: 0.8222, loss: 0.3901 ||: 100%|██████████| 55/55 [00:52<00:00,  1.65s/it]\n",
      "01/19/2019 09:36:33 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/19/2019 09:36:33 - INFO - allennlp.training.trainer -   cpu_memory_MB |  1479.881  |       N/A\n",
      "01/19/2019 09:36:33 - INFO - allennlp.training.trainer -   loss          |     0.488  |     0.390\n",
      "01/19/2019 09:36:33 - INFO - allennlp.training.trainer -   accuracy      |     0.775  |     0.822\n",
      "01/19/2019 09:36:34 - INFO - allennlp.training.trainer -   Best validation performance so far. Copying weights to '../data/sst-2/ckpts/replicate_0/best.th'.\n",
      "01/19/2019 09:36:35 - INFO - allennlp.training.trainer -   Epoch duration: 00:44:04\n",
      "01/19/2019 09:36:35 - INFO - allennlp.training.trainer -   Estimated training time remaining: 1:28:08\n",
      "01/19/2019 09:36:35 - INFO - allennlp.training.trainer -   Epoch 1/2\n",
      "01/19/2019 09:36:35 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 1479.880704\n",
      "01/19/2019 09:36:35 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.8104, loss: 0.4152 ||: 100%|██████████| 4210/4210 [1:02:48<00:00,  1.54it/s]\n",
      "01/19/2019 10:39:24 - INFO - allennlp.training.trainer -   Validating\n",
      "accuracy: 0.8222, loss: 0.3865 ||: 100%|██████████| 55/55 [00:57<00:00,  1.62s/it]\n",
      "01/19/2019 10:40:21 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/19/2019 10:40:21 - INFO - allennlp.training.trainer -   cpu_memory_MB |  1479.881  |       N/A\n",
      "01/19/2019 10:40:21 - INFO - allennlp.training.trainer -   loss          |     0.415  |     0.387\n",
      "01/19/2019 10:40:21 - INFO - allennlp.training.trainer -   accuracy      |     0.810  |     0.822\n",
      "01/19/2019 10:40:22 - INFO - allennlp.training.trainer -   Best validation performance so far. Copying weights to '../data/sst-2/ckpts/replicate_0/best.th'.\n",
      "01/19/2019 10:40:23 - INFO - allennlp.training.trainer -   Epoch duration: 01:03:47\n",
      "01/19/2019 10:40:23 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:53:56\n",
      "01/19/2019 10:40:23 - INFO - allennlp.training.trainer -   Epoch 2/2\n",
      "01/19/2019 10:40:23 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 1479.880704\n",
      "01/19/2019 10:40:23 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.8156, loss: 0.4048 ||: 100%|██████████| 4210/4210 [47:56<00:00,  2.16it/s]  \n",
      "01/19/2019 11:28:19 - INFO - allennlp.training.trainer -   Validating\n",
      "accuracy: 0.8131, loss: 0.3985 ||: 100%|██████████| 55/55 [00:59<00:00,  1.87s/it]\n",
      "01/19/2019 11:29:19 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/19/2019 11:29:19 - INFO - allennlp.training.trainer -   cpu_memory_MB |  1479.881  |       N/A\n",
      "01/19/2019 11:29:19 - INFO - allennlp.training.trainer -   loss          |     0.405  |     0.398\n",
      "01/19/2019 11:29:19 - INFO - allennlp.training.trainer -   accuracy      |     0.816  |     0.813\n",
      "01/19/2019 11:29:20 - INFO - allennlp.training.trainer -   Epoch duration: 00:48:56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'peak_cpu_memory_MB': 1479.880704,\n",
       " 'training_duration': '02:36:48',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 2,\n",
       " 'epoch': 2,\n",
       " 'training_accuracy': 0.8155726142927141,\n",
       " 'training_loss': 0.40481197644033934,\n",
       " 'training_cpu_memory_MB': 1479.880704,\n",
       " 'validation_accuracy': 0.8130733944954128,\n",
       " 'validation_loss': 0.39849679117852993,\n",
       " 'best_epoch': 1,\n",
       " 'best_validation_accuracy': 0.8222477064220184,\n",
       " 'best_validation_loss': 0.38654813874851573}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'peak_cpu_memory_MB': 1479.880704,\n",
       " 'training_duration': '02:36:48',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 2,\n",
       " 'epoch': 2,\n",
       " 'training_accuracy': 0.8155726142927141,\n",
       " 'training_loss': 0.40481197644033934,\n",
       " 'training_cpu_memory_MB': 1479.880704,\n",
       " 'validation_accuracy': 0.8130733944954128,\n",
       " 'validation_loss': 0.39849679117852993,\n",
       " 'best_epoch': 1,\n",
       " 'best_validation_accuracy': 0.8222477064220184,\n",
       " 'best_validation_loss': 0.38654813874851573}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record results and save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import record_experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-01-19 12:39:36,084 - record_experiments Inserted results at id 5c4360d772bcea8281b90c83\n",
      "01/19/2019 12:39:36 - INFO - record_experiments -   Inserted results at id 5c4360d772bcea8281b90c83\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x138ee6b08>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_log = dict(config)\n",
    "experiment_log.update(metrics)\n",
    "record_experiments.record(experiment_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output tensorboard outputs and training logs to s3\n",
    "\n",
    "(Remove weights since they take up too much space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm {SER_DIR / \"*.th\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best.th              metrics_epoch_0.json metrics_epoch_2.json\r\n",
      "\u001b[1m\u001b[36mlog\u001b[m\u001b[m                  metrics_epoch_1.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls {SER_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../data/sst-2/ckpts/replicate_0/metrics_epoch_1.json to s3://nnfornlp/ckpts/RUN_ID/metrics_epoch_1.json\n",
      "upload: ../data/sst-2/ckpts/replicate_0/log/validation/events.out.tfevents.1547905948.Keitas-MacBook-Pro.local to s3://nnfornlp/ckpts/RUN_ID/log/validation/events.out.tfevents.1547905948.Keitas-MacBook-Pro.local\n",
      "upload: ../data/sst-2/ckpts/replicate_0/metrics_epoch_0.json to s3://nnfornlp/ckpts/RUN_ID/metrics_epoch_0.json\n",
      "upload: ../data/sst-2/ckpts/replicate_0/metrics_epoch_2.json to s3://nnfornlp/ckpts/RUN_ID/metrics_epoch_2.json\n",
      "upload: ../data/sst-2/ckpts/replicate_0/log/train/events.out.tfevents.1547905948.Keitas-MacBook-Pro.local to s3://nnfornlp/ckpts/RUN_ID/log/train/events.out.tfevents.1547905948.Keitas-MacBook-Pro.local\n",
      "upload: ../data/sst-2/ckpts/replicate_0/best.th to s3://nnfornlp/ckpts/RUN_ID/best.th\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync {SER_DIR} s3://nnfornlp/ckpts/{RUN_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
